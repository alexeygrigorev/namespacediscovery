\section{Mathematical Definition Extraction}
Mathematical expressions are hard to understand without the natural language description, 
so we want to find identifiers in the mathematical expressions and extract their 
definition from the surrounding text

Example:

The precision $P$ is defined as $P = \cfrac{w}{w + y}$ where $w$ is the number of 
correctly labeled pairs and $y$ is the number of incorrectly labeled pairs

We want to extract:

($P$, "precision"), 
($w$, "the number of correctly labeled pairs"),
($y$, "the number of incorrectly labeled pairs")

Another example: "Let $e$ be the base of natural logarithm"

want to extract ($e$, "the base of natural logarithm")


A phrase that defines a mathematical expression consists of three parts \cite{kristianto2012extracting}: 

\begin{itemize}
  \item \emph{definiendum}, a mathematical expression or identifier, is the term to be defined;
  \item \emph{definiens} is the definition itself: it is the word or phrase that defines the definiendum in a definition.
  \item \emph{definitor} is a relator verb that links definiendum and definiens.
\end{itemize}
 
In this work we are interested only in first two parts: definiendum and definiens. 
Thus we define a \emph{relation} as a pair (definiendum, definiens). Since we are 
interested in finding definitions only for identifiers, we restrict ourselves only
to (identifier, definition) relations.


% TODO: 
An identifier is a mathematical ...


\subsection{Formula Representation: MathML}

\cite{mathml}

\subsection{Automatic Definition Extraction}
We have an identifier and want to find what it stands for.

The definitions of mathematical expressions and identifiers can be found from
the natural text description around the expression.

Assumption: the definitions of mathematical expressions are always noun phrases

In general, a noun phrase can be

\begin{itemize}
  \item a simple noun
  \item a compound noun (e.g. adjective + noun)
  \item a compound noun with a clause, prepositional phrase, etc
\end{itemize}



\subsection{Preprocessing}

The typical (see e.g. \cite{kristianto2012extracting, pagael2014mlp} (more?)) pipeline is the following:

\begin{itemize}
  \item Read corpus of documents in some markup language, e.g. Mediawiki or Latex
  \item Translate all formulas in the documents into MathML 
  \item Process MathML formulas
  \item Replace formulas with some placeholder
  \item Annotate text ([[Math-Aware POS Tagging]])
  \item Find relations in the text
\end{itemize}


For example:

%TODO:

% http://habrastorage.org/files/025/1f5/4fa/0251f54fa7a248faa6718839ee060b53.png
% MLP flow


\subsection{Math-aware POS tagging}

NLP is a tool for text processing, but it's also applicable to scientific documents with math expressions. So we can adjust traditional NLP methods to dealing with formulas.


Penn Treebank POS Scheme \cite{santorini1990part}  doesn't have special classes for mathematics.
What we can do is to add other math-related classes:

ID for identifiers (e.g. "... where $E$ stands for energy", $E$ should be tagged as ID)

\verb|MATH| for formulas (e.g. "$E = mc^2$ is the mass-energy equivalence formula", "$E = mc^2$ should be tagged as \verb|MATH|)

Mathematical expressions are usually contained within special tags, e.g. inside
tag \verb|<math>...</math>| for wikipedia, or inside \verb|$...$| for 
latex documents.

We find all such mathematical expressions and replace each with a unique single token \verb|MATH_mathID|

the \verb|mathID| could be a randomly generated string or result of some hash function 
applied to the content of formula. The latter approach is preferred when we want 
to have consistent strings across several runs.

Then we apply traditional [[POS Tagging]] (e.g. via Stanford 
CoreNLP \cite{manning2014stanford})
techniques to the textual data. They typically will annotate such \verb|MATH_mathID|
tokens as nouns

after that we may want to re-annotate all math tokens: if it contains only 
one identifier, we label it as \verb|ID|, if several - as \verb|MATH|. But in some cases 
we want to keep original annotation

after that we can bring the mathematical content back to the document



\subsection{Extraction Methods}
\subsubsection{Nearest Noun Method}
Definition is a combination of adjectives and nouns (also sometimes determinants) in the text before the identifier

\cite{grigore2009towards}
\cite{yokoi2011contextual}


This way it only can be compound nouns without additional phrases.


E.g:
* "In other words, the bijection $\sigma$ normalizes $G$ in ..."
* It will extract relations ($\sigma$, "bijection")



\subsubsection{Pattern Matching Methods}

Use patterns to extract definitions

For example,

\begin{itemize}
  \item DESC IDENTIFIER (this is actually the same as nearest noun method)
  \item let|set IDENTIFIER denote|denotes|be DESC
  \item DESC is|are denoted|defined|given as|by IDENTIFIER
  \item IDENTIFIER denotes|dentore|stand|stands as|by IDENTIFIER
  \item IDENTIFIER is|are DESC
  \item DESC is|are IDENTIFIER
  \item and others
\end{itemize}


Patterns taken from
\cite{trzeciak1995writing} (TODO: mention who exactly did that)
and sentence patterns from Graphs and Combinatorics papers from Springer

Used in \cite{quoc2010mining}

Others (\cite{kristianto2012extracting}, \cite{kristianto2014extracting}, \cite{pagael2014mlp})
usually use this method as the baseline for comparison.


\subsubsection{Machine Learning Based Methods}
Formulates definition extraction as a binary classification problem

\begin{itemize}
  \item find candidate pairs (identifier, candidate definition)
  \item candidates are nouns and noun phrases from the same sentence as the expression
\end{itemize}


Features:
\begin{itemize}
  \item sentence patterns (true if one of the patterns can capture this relation - could be separate feature for each pattern)
  \item if there's a colon/comma between candidate and identifier
  \item if there's another math expression between
  \item if candidate is inside parentheses and identifier is outside
  \item word-distance between candidate and identifier
  \item position of candidate relative to identifier
  \item text and POS tag of one/two/three preceding and following tokens around the candidate
  \item unigram/bigram/trigram of previous features
  \item text of first verb between candidate and identifier
  \item others
\end{itemize}

Classifiers:
[[SVM]] (linear kernel) (\cite{kristianto2014extracting}, \cite{yokoi2011contextual})

[[Conditional Random Fields]] (\cite{kristianto2014extracting})


\subsubsection{Probabilistic Approaches}
Mathematical Language Processing (MLP) Approach \cite{pagael2014mlp}: Statistical definition discovery: rank candidate definitions by probability and design an 
information extraction system that shots the most relevant (i.e. probable) 
definition to the reader to facilitate reading scientific texts with mathematics.


The main idea: the definitions occur very closely to identifiers in sentences.


extract identifiers from MathML


Two assumptions

\begin{itemize}
  \item identifier and its definition occur in the same sentence, so the candidates are 
  taken only from the same sentences (as in the ML approach)
  \item definitions are likely occur earlier in the document when authors introduce 
  the meaning of an identifier, in subsequent uses the definition is typically not repeated
\end{itemize}

These assumptions are used in the ranking formula

each candidate is ranked with the following weighed sum:

$$R(n, \Delta, t, d) = \cfrac{\alpha \, R_{\sigma_d}(\Delta) + \beta \, R_{\sigma_s}(n) + \gamma \, \text{tf}(t, s)}{\alpha + \beta + \gamma}$$

where

$t$ is the candidate term,
$s$ set of sentences in the document,
$\Delta$ is the distance (the amount of word tokens) between identifier and the candidate term $t$,
$n$ what is n??? % TODO

The distances modeled with Gaussians (instead of taking the raw ones)

$$R_{\sigma}(\Delta) = \exp \left( -\cfrac{1}{2} \cdot {\Delta^2 - 1}{\sigma_2} \right)$$

assume that the probability to find a relation at $\Delta = 1$ is maximal

Finding Parameters $\sigma_d$ and $\sigma_s$

$\sigma_d$ - the standard deviation of Gaussian that models the distance to definition candidate
manual evaluation showed that $R_{\sigma_d}(1) \approx R_{\sigma_d}(5)$,
i.e. it's two times more likely to find the real definition at distance $\Delta=1$ 
than at distance $\Delta=5$.
thus $\sigma_d = \sqrt{\cfrac{12}{\ln 2}}$


% TODO: check that
$\sigma_s$ - the standard deviation of the Gaussian that models the distance 
from the beginning of document
% 
$\sigma_s = 2 \sqrt{\cfrac{1}{\ln 2}}$


weights $\alpha, \beta, \gamma$:
$\alpha = \beta = 1$ and 
$\gamma = 0.1$ because some valid definitions may occur more often than other 
valid definitions, e.g. "length" vs "Hamiltonian"


\subsubsection{Other Ideas}
Translation of mathematical formulas to English using machine-translation techniques
\cite{nghiem2012towards}


\subsection{Performance Measures}

\begin{itemize}
  \item Precision: no of math expresssion with correctly extracted definitions / no of extracted definitions
  \item Recall: no of math expresssion with correctly extracted definitions / no of expressions with definitions
  \item F1 = 2PR / (P + R): harmonic mean between P and R
\end{itemize}
