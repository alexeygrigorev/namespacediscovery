\section{Mathematical Definition Extraction} \label{sec:definitionextraction-top}

Mathematical expressions are hard to understand without the natural language description,
therefore we want to extract identifiers from mathematical expressions
and then find their definitions from the surrounding text.

For example, given the sentence ``The relation between energy and mass is
described by  the mass-energy equivalence formula $E = mc^2$,
where $E$ is energy, $m$ is mass and $c$ is the speed of light''
the goal is to extract the following identifier-definition relations:
($E$, ``energy''),
($m$, ``mass'') and
($c$, ``the speed of light'').

Consider another example: ``Let $e$ be the base of natural logarithm''.
We would like to extract ($e$, ``the base of natural logarithm'').

Formally, a phrase that defines a mathematical expression consists of three parts \cite{kristianto2012extracting}:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item \emph{definiendum} is the term to be defined: it is a mathematical expression
      or an identifier;
  \item \emph{definiens} is the definition itself: it is the word or phrase that defines the definiendum in a definition;
  \item \emph{definitor} is a relator verb that links definiendum and definiens.
\end{itemize}


In this work we are interested in the first two parts: \emph{definiendum} and
\emph{definiens}. Thus we define a \emph{relation} as a pair
(definiendum, definiens). For example, ($E$, ``energy'') is a relation where
$E$ is a definiendum, and ``energy'' is a definiens.

In this chapter we will discuss how the relations can be discovered automatically.
It is organized as follows:
then discuss
the Math-Aware POS Tagging procedure in section~\ref{sec:postagging} and
finally review the extraction methods in section~\ref{sec:definition-extraction-methods}
and briefly discuss how the quality of extracted identifiers is evaluated
in section~\ref{sec:definition-evaluation}.



\subsection{Math-aware POS tagging} \label{sec:postagging}
Part-of-Speech Tagging (POS Tagging) is a typical Natural Language Processing
task which assigns a POS Tag to each word in a given text \cite{jurafsky2000speech}.
While the POS Tagging task is mainly a tool for text processing, it can
also be applicable to scientific documents with mathematical expressions,
and can be adjusted to dealing with formulae \cite{schoneberg2014pos}
\cite{pagael2014mlp}.

A \emph{POS tag} is an abbreviation that corresponds to some
part of speech. Penn Treebank POS Scheme \cite{santorini1990part} is
a commonly used POS tagging scheme which defines a set of part-of-speech tags
for annotating English words.
For example, \texttt{JJ} is an adjective (``big''), \texttt{RB} as in adverb,
\texttt{DT} is a determiner (``a'', ``the''), \texttt{NN} is a
noun (``corpus'') and \texttt{SYM} is used for symbols (``$>$'', ``$=$'').


However the Penn Treebank scheme does not have special tags for mathematics,
but it is flexible enough and can be extended to include additional tags.
For example, we can include a math-related tag \texttt{MATH}.
Usually it is done by first applying traditional POS taggers (like Stanford
CoreNLP \cite{manning2014stanford}), and then
refining the results by re-tagging math-related tokens of text as \texttt{MATH}
\cite{schoneberg2014pos}.


For example, consider the following sentence:
``The relation between energy and mass is
described by  the mass-energy equivalence formula $E = mc^2$,
where $E$ is energy, $m$ is mass and $c$ is the speed of light''.
In this case we will assign the \verb|MATH| tag to ``$E = mc^2$'', ``$E$'',
``$m$'' and ``$c$''

However we can note that for finding identifier-definition relations
the \texttt{MATH} tag alone is not sufficient: we need to distinguish
between complex mathematical expressions and stand-alone identifiers -
mathematical expressions that contain only one symbol: the identifier.
For the example above we would like to be able to distinguish the
expression ``$E = mc^2$'' from identifier tokens ``$E$'',
``$m$'' and ``$c$''. Thus we extend the Penn Treebank scheme even more
and introduce an additional tag \texttt{ID} to denote stand-alone identifiers.


Thus, in the example above ``$E = mc^2$'' will be assigned the \texttt{MATH} tag
and ``$E$'', ``$m$'' and ``$c$'' will be annotated with \texttt{ID}.

In the next section we discuss how this can be used to find identifier-definition
relations.


\subsection{Extraction Methods} \label{sec:definition-extraction-methods}
There are several ways of extracting the identifier-definition relations.
Here we will review the following:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item Nearest Noun
  \item Pattern Matching
  \item Machine-Learning based methods
  \item Probabilistic methods
\end{itemize}


\subsubsection{Nearest Noun Method}

\ \\

The Nearest Noun  \cite{grigore2009towards} \cite{yokoi2011contextual}
is the simplest definition extraction method.
It assumes that the definition is a combination of ad
It finds definitions by looking for combinations of adjectives and nouns
(sometimes preceded by determiners) in the text before the identifier.

I.e. if we see a token annotated with \texttt{ID}, and then a sequence
consisting only of adjectives (\texttt{JJ}), nouns (\texttt{NN}, \texttt{NNS})
and determiners (\texttt{DET}), then we say that this sequence is
the definition for the identifer.

For example, given the sentence ``In other words, the bijection $\sigma$ normalizes
$G$ in ...'' we will extract relation ($\sigma$, "bijection").



\subsubsection{Pattern Matching Methods} \label{sec:pattern-matching}

\ \\

The Pattern Matching method \cite{quoc2010mining} is an extension of the
Nearest Noun method: In Nearest Noun, we are looking for one specific patten
where identifier is followed by the definition, but we can define several such
patterns and use them to extract definitions.

For example, we can define the following patterns:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item \texttt{IDE} \texttt{DEF}
  \item \texttt{DEF} \texttt{IDE}
  \item let$|$set \texttt{IDE} denote$|$denotes$|$be \texttt{DEF}
  \item \texttt{DEF} is$|$are denoted$|$\texttt{def}ined$|$given as$|$by \texttt{IDE}
  \item \texttt{IDE} denotes$|$denote$|$stand$|$stands as$|$by \texttt{DEF}
  \item \texttt{IDE} is$|$are \texttt{DEF}
  \item \texttt{DEF} is$|$are \texttt{IDE}
  \item and many others
\end{itemize}


In this method \texttt{IDE} and \texttt{DEF} are placeholders that are
assigned a value when the pattern is matched against some subsequence
of tokens.  \texttt{IDE} and DEF need to
satisfy certain criteria in order to be successfully matched: like in the
Nearest Noun method we assume that \texttt{IDE} is some token annotated with
\texttt{ID} and \texttt{DEF} is a phrase containing adjective (\texttt{JJ}),
nouns (\texttt{NN}) and  determiners (\texttt{DET}). Note that the first patten corresponds
to the Nearest Noun pattern.

The patterns above are combined from two sources: one
is extracted from a guide to writing mathematical papers in English
(\cite{trzeciak1995writing}) by \textbf{TODO}, and another
is extracted from Graphs and Combinatorics papers from Springer
by \textbf{TODO}.


The pattern matching method is often used as the baseline method
for identifier-definition extraction methods \cite{kristianto2012extracting}
\cite{kristianto2014extracting} \cite{pagael2014mlp}.


\subsubsection{Machine Learning Based Methods}

\ \\

The definition extraction problem can be formulated as a binary classification
problem: given a pair (identifier, candidate-definition), does this pair
correspond to real identifier-definition relation?

To do this we find all candidate pairs: identifiers are tokens
annotated with \texttt{ID}, and candidate defections are nouns and
noun phrases from the same sentence as the definition.

Once the candidate pairs are found, we extract the following features \cite{kristianto2014extracting} \cite{yokoi2011contextual}:


\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item boolean features for each of the patterns from
    section~\ref{sec:pattern-matching} indicating if the pattern is matched,
  \item indicator if there's a colon or comma between candidate and identifier,
  \item indicator if there's another math expression between candidate and identifier,
  \item indicator if candidate is inside parentheses and identifier is outside,
  \item distance (in words) between the identifier and the candidate,
  \item the position of candidate relative to identifier,
  \item text and POS tag of one/two/three preceding and following tokens around the candidate,
  \item text of the first verb between candidate and identifier,
  \item many others.
\end{itemize}

Once the features are extracted, a binary classifier can be trained to predict
if an unseen candidate pair is a relation or not.
For this task the popular choices of classifiers are Support Vector Machine
classifier with linear kernel \cite{kristianto2014extracting} \cite{yokoi2011contextual}
and Conditional Random Fields \cite{kristianto2014extracting},
but, in principle, any other binary classifier can be applied
as well.


\subsubsection{Probabilistic Approaches} \label{sec:mlp} \ \\

In the Mathematical Language Processing approach \cite{pagael2014mlp}
a definition for an identifier is extracted by ranking
candidate definitions by the probability of definining
the identifier, and only the most probable candidates are retained.

The main idea of this approach is that the definitions occur very closely
to identifiers in sentences, and the closeness can be used to
model the probability distribution over candidate definitions.

The candidates are ranked by the following formula:
$$R(n, \Delta, t, d) = \cfrac{\alpha \, R_{\sigma_d}(\Delta) + \beta \, R_{\sigma_s}(n) + \gamma \, \text{tf}(t, s)}{\alpha + \beta + \gamma}$$
where $\alpha, \beta, \gamma$ are weighting parameters.


Finally, for weights $\alpha, \beta, \gamma$ the following parameters were chosen in
\cite{pagael2014mlp}: $\alpha = \beta = 1$ and $\gamma = 0.1$.

