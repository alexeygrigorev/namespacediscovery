\section{Namespaces as Document Clusters} \label{sec:namespaces-top}


In this chapter, we discuss how the process of namespace discovery
can be automated.

First, in section~\ref{sec:vsm} we describe the Vector Space Model (VSM): 
the traditional way of representing a collection of documents as vectors,
and also extend it to identifiers and discuss how definitions can
be incorporated in this space. 
Then, we go over common similarity and distance functions that are useful
for document clustering in section~\ref{sec:similarity-distance}. 
Next, we review common techniques for document clustering in section~\ref{sec:doc-clustering}
and the Latent Semantic Analysis method in section~\ref{sec:lsa}.


\subsection{Vector Space Model} \label{sec:vsm}

Vector Space Model is a statistical model for representing documents
in some vector space. It is an Information Retrieval
model \cite{manning2008introduction}, but it is also used for various
Text Mining tasks such as Document Classification \cite{sebastiani2002machine}
and Document Clustering \cite{oikonomakou2005review} \cite{aggarwal2012survey}.

In Vector Space Model we make two assumptions about the data:
(1) \emph{Bag of Words assumption}: the order of words is not important,
only word counts;
(2) \emph{Independence assumption}: we treat all words as independent.
% Bag of Words = unordered list of terms
% good enough for topic similarity
Both assumptions are quite strong, but nonetheless this method often
gives good results.

Let $\mathcal V = \{t_1, t_2, \ ... \ , t_m \}$ be a set of $n$ terms.
Then documents can be represented as $m$-vectors
$\mathbf d_i = (w_1, w_2, \ ... \ , w_m)$, where $w_j$ is the weight
of term $t_j$ in the document $\mathbf d_i$,
and the document collection can be represented by a \emph{term-document matrix}
$D$, where columns of $D$ are document vectors
$\mathbf d_1, \mathbf d_2, \ ... \ , \mathbf d_n$
and rows of $D$ are indexed by terms $t_1, t_2, \ ... \ , t_m$
(see fig.~\ref{fig:document-vsm}).

\begin{figure}[h]
\centering\includegraphics[width=0.5\textwidth]{document-vsm.pdf}
\caption{Documents $\mathbf d_1, \mathbf d_2, \mathbf d_3$
in a document space with dimensions $t_1, t_2, t_3$.}
\label{fig:document-vsm}
\end{figure}


There are the following term weighting schemes \cite{manning2008introduction}:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item binary: 1 if a term is present, 0 otherwise;
  \item term frequency (TF): number of occurrences of the term in a document;
  \item document frequency (DF): number of documents containing the terml
  \item TF-IDF: combination of TF and inverse DF.
\end{itemize}


\textbf{Term Frequency (TF)} weights terms by local frequency in the document.
That is, the term is weighed by how many times it occurs in the document.
Sometimes a term is used too often in a document, and we want to
reduce its influence, and this is typically done by applying some
sublinear transformation to TF, for instance, a square root or a logarithms.

\textbf{Document Frequency (DF)} weights terms by their global frequency
in the collection, which is the number of documents that contain the token.
But more often we are interested in domain specific words than in neutral words,
and these domain specific words tent to occur less frequently and they usually
have more discriminative power: that is, they are better in telling one document apart from another. So we use \textbf{Inverse Document Frequency (IDF)} to give more
weight to rare words rather than to frequent words.

A good weighting system gives the best performance when it assigns
more weights to terms with high TF, but low DF \cite{salton1988term}.
This can be achieved by combining both TF and IDF
schemes. Usually a sublinear TF is used to avoid the dominating effect of
words that occur too frequently. As the result, terms appearing
too rarely or too frequently are ranked low.
The TF and IDF are combined together in \textbf{TF-IDF} weighting scheme:
$$\text{tf-idf}(t, \mathbf d) = (1 + \log \text{tf}(t, \mathbf d)) \cdot \log \cfrac{n}{\text{df}(t)} \, ,$$
where $\text{tf}(t, \mathbf d)$ is term frequency of term $t$ in document
$\mathbf d$ and $\text{df}(t)$ is the document frequency of term $t$ in
the document collection.

\ \\


The Vector Space Model can be adjusted to represent documents by identifers
they contain instead of words. To do that we replace the vocabulary $\mathcal V$
with  a set of identifiers $\mathcal I = \{ i_1, i_2, \ ... \ , i_m \}$,
but documents are still represented as $m$-vectors $\mathbf d_j = (w_1, w_2, \ ... \ , w_m)$,
where $w_k$ is a weight of identifier $i_k$ in the document $\mathbf d_j$.
Likewise, we can define an identifier-document matrix $D$ as a matrix where
columns are document vectors and rows are indexed by the identifiers.

\ \\

Identifiers, as terms, suffer from the problems of synonymy and polysemy,
and we solve this problem by extracting definitions for all the identifiers.
There are several ways of incorporating the extracted definitions into the
model:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item do not include definition information at all, use only identifiers;
  \item use ``weak'' identifier-definition association: include identifiers and
        definitions as separate dimensions;
  \item use ``strong'' association: append definition to identifier.
\end{itemize}

To illustrate how it is done, consider three relations ($E$, ``energy''),
($m$, ``mass'') and ($c$, ``speed of light''), and three documents
$d_1 = \{E, m, c\}, d_2 = \{ m, c\}, d_3 = \{ E \}$. Then

\begin{itemize}\itemsep1pt\parskip0pt\parsep0pt
  \item no definitions: dimensions are ($E$, $m$, $c$) and the identifier-document matrix is
  $$D = \left[
    \begin{array}{c|ccc}
       & d_1 & d_2 & d_3 \\
      \hline
      E & 1 & 0 & 1  \\
      m & 1 & 1 & 0 \\
      c & 1 & 1 & 0 \\
    \end{array}
  \right];$$
  \item ``weak'' association: dimensions are ($E$, $m$, $c$, energy, mass,
  speed of light), and the matrix is $$D = \left[
    \begin{array}{r|ccc}
       & d_1 & d_2 & d_3 \\
      \hline
      E                     & 1 & 0 & 1  \\
      m                     & 1 & 1 & 0 \\
      c                     & 1 & 1 & 0 \\
      \text{energy}         & 1 & 0 & 1  \\
      \text{mass}           & 1 & 1 & 0 \\
      \text{speed of light} & 1 & 1 & 0 \\
    \end{array}
  \right];$$
  \item ``strong'' association: dimensions are ($E$\_energy, $m$\_mass, $c$\_speed of light), and the matrix is $$D = \left[
    \begin{array}{r|ccc}
       & d_1 & d_2 & d_3 \\
      \hline
      E\text{\_energy} & 1 & 0 & 1  \\
      m\text{\_mass} & 1 & 1 & 0 \\
      c\text{\_speed of light} & 1 & 1 & 0 \\
    \end{array}
  \right].$$
\end{itemize}


\subsection{Similarity Measures and Distances} \label{sec:similarity-distance}

Once the documents are represented in some vector space, we need to
define how to compare these documents to each other. There are two
ways of doing this: using a similarity function that tells how similar
two objects are (the higher values, the more similar the objects),
or using a distance function, sometimes called ``dissimilarity function'',
which is the opposite of similarity (the higher the values, the less similar
the objects).

We consider Euclidean distance, inner product, cosine similarity and
Jaccard coefficient.


\subsubsection{Euclidean Distance} \ \\

The Euclidean distance function is the most commonly used distance
function in vector spaces. Euclidean distance corresponds to
the geometric distance between two data points in the vector space.
For example, if we have two points $\mathbf x$ and
$\mathbf z$, then the Euclidean distance is the
length of the line that connects these two points.
The square of the Euclidean distance is defined as
$$\| \mathbf x - \mathbf z \|^2 = (\mathbf x - \mathbf z)^T (\mathbf x - \mathbf z)  =\sum_i (x_i - z_i)^2.$$

This distance is useful for low-dimensional data, but it does not always work
well in high dimensions, especially with sparse vector such as
document vectors \cite{ertoz2003finding}.


\subsubsection{Inner product} \ \\

The inner product between two vectors can be used as a similarity function:
the more similar two vectors are, the larger is their inner product.
Geometrically the inner product between two vectors $\mathbf x$ and $\mathbf z$
is defined as
$\mathbf x^T \mathbf z = \|\mathbf x \| \, \| \mathbf z \| \, \cos \theta$
where $\theta$ is the angle between vectors $\mathbf x$ and $\mathbf z$.
In Linear Algebra, however, the inner product
is defined as a sum of element-wise products of two vectors:
given two vectors $\mathbf x$ and $\mathbf z$, the inner product is
$\mathbf x^T \mathbf z = \sum_{i = 1}^n x_i \, z_i$ where $x_i$ and $z_i$
are $i$th elements of $\mathbf x$ and $\mathbf z$, respectively.
The geometric and algebraic definitions are equivalent \cite{huges2013calculus}.



\subsubsection{Cosine Similarity} \label{sec:cosine} \ \\


% However the magnitude of each individual vector still matters. If one
% document vector is particularly long compared to other vectors and it is not
% orthogonal to them (i.e. $\cos \theta \ne 0$), then it is likely to be
% one of the most similar vectors to others -- only because of its length.


Inner product is sensitive to the length of vectors, and thus
it may make sense to consider only the angle between them:
the angle does not depend on the magnitude, but it is still
a very good indicator of vectors being similar or not.

The angle between two vectors can be calculated from the geometric
definition of inner product:
$\mathbf x^T \mathbf z = \|\mathbf x \| \, \| \mathbf z \| \, \cos \theta$.
By rearranging the terms we get
$\cos \theta = \mathbf x^T \mathbf z \, / \, (\|\mathbf x \| \, \| \mathbf z \|)$.

We do not need the angle itself and can use the cosine directly
\cite{manning2008introduction}.
Thus can define \emph{cosine similarity} between two documents $\mathbf d_1$ and
$\mathbf d_2$ as
$$\text{cosine}(\mathbf d_1, \mathbf d_2) = \cfrac{\mathbf d_1^T \mathbf d_2}{\|\mathbf d_1 \| \, \| \mathbf d_2 \|} \ .$$
If the documents have unit lengths, then cosine similarity is the same as
dot product: $\text{cosine}(\mathbf d_1, \mathbf d_2) = \mathbf d_1^T \mathbf d_2$.

The cosine similarity can be converted to a distance function.
The maximal possible cosine is 1 for two identical documents.
Therefore we can define \emph{cosine distance} between two vectors
$\mathbf d_1$ and $\mathbf d_2$ as
$d_c(\mathbf d_1, \mathbf d_2) = 1 - \text{cosine}(\mathbf d_1, \mathbf d_2)$.
The cosine distance is not a proper metric \cite{korenius2007principal},
but it is nonetheless useful.

The cosine distance and the Euclidean distance are connected \cite{korenius2007principal}.
For two unit-normalized vectors $\mathbf d_1$ and $\mathbf d_2$ the Euclidean distance
between them is $\| \mathbf d_1 - \mathbf d_2 \|^2 = 2 - 2 \, \mathbf d_1^T \mathbf d_2 =2 \, d_c(\mathbf d_1, \mathbf d_2)$. Thus we can use Euclidean distance on
unit-normalized vectors and interpret it as cosine distance.


\subsubsection{Jaccard Coefficient} \ \\

Finally, the Jaccard Coefficient is a function that compares how similar
two sets are. Given two sets $A$ and $B$, it is computed as
$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$.
It is also applicable to document vectors with binary weights, and it can
be defined as $J(\mathbf d_1, \mathbf d_2) =
\frac{\mathbf d_1^T \mathbf d_2}{\| \mathbf d_1^T \|^2 + \| \mathbf d_2^T \|^2 - \mathbf d_1^T \mathbf d_2}$ \cite{manning2008introduction}.



\subsection{Document Clustering Techniques} \label{sec:doc-clustering}

Cluster analysis is a set of techniques for organizing collection
of items into coherent groups. In Text Mining clustering is often
used for finding topics in a collection of document \cite{aggarwal2012survey}.
In Information Retrieval clustering is used to assist the users and group
retrieved results into clusters \cite{cutting1992scatter}.

There are several types of clustering algorithms:
hierarchical (agglomerative and divisive), partitioning,
density-based, and others.


\subsubsection{Agglomerative clustering} \label{sec:clustering-heierarchical} \ \\

The general idea of agglomerative clustering algorithms is to start with
each document being its own cluster and iteratively merge clusters based
on best pair-wise cluster similarity.

Thus, a typical agglomerative clustering algorithms consists of the following steps:

\begin{enumerate}
\itemsep1pt\parskip0pt\parsep0pt
  \item let each document be a cluster on its own
  \item compute similarity between all pairs of clusters an store the
      results in a similarity matrix
  \item merge two most similar clusters
  \item update the similarity matrix
  \item repeat until everything belongs to the same cluster
\end{enumerate}

These algorithms differ only in the way they calculate similarity between
clusters. It can be \textbf{Single Linkage}, when the clusters are merged based
on the closest pair; \textbf{Complete Linkage}, when the clusters are merged
based on the worst-case similarity -- the similarity between the most
distant objects on the clusters; \textbf{Group-Average Linkage}, based
on the average pair-wise similarity between all objects in the clusters;
and \textbf{Ward's Method} when the clusters to merge are chosen to
minimize the within-cluster error between each object and its centroid
is minimized \cite{oikonomakou2005review}.

Among these algorithms only Single Linkage is computationally feasible
for large data sets, but it doesn't give good results compared to other
agglomerative clustering algorithms. Additionally, these algorithms
are not always good for document clustering because they tend to
make mistakes at early iterations that are impossible to correct
afterwards \cite{steinbach2000comparison}.



\subsubsection{$K$-Means} \label{sec:kmeans} \ \\

Unlike agglomerative clustering algorithms, K-Means is an iterative
algorithm, which means that it can correct the mistakes made
at earlier iterations. Lloyd's algorithm is the most popular way
of implementing K-Means \cite{xu2005survey}: given a desired number of clusters $K$,
it iteratively improves the Euclidean distance between each data
point and the centroid, closest to it.


Let $\mathcal D = \{  \mathbf d_1, \mathbf d_2, \ ... \ , \mathbf d_n \}$
be the document collection, where documents $\mathbf d_i$ are represented
is a document vector space $\mathbb R^m$ and $K$ is the desired
number of clusters. Then we define $k$ cluster centroids $\boldsymbol \mu_j$ that are
also in the same document vector space $\mathbb R^m$.
Additionally for each document $\mathbf d_i$ we maintain the assignment
variable $c_i \in \{ 1, 2, \ ... \ , k \}$, which specifies to what
cluster centroid $\boldsymbol \mu_1, \boldsymbol \mu_2, \ ... \ , \boldsymbol \mu_k$
the document $\mathbf d_i$ belongs.


The algorithms consists of three steps: (1) seed selection step,
where each $\boldsymbol \mu_j$ is randomly assigned some value,
(2) cluster assignment step, where we iterate over all document vectors
$\mathbf d_i$ and find its closest centroid, and (3)  move centroids step,
where the centroids are re-calculated. Steps (2) and (3) are repeated
until the algorithm converges. The pseudocode for $K$-Means is presented
in the listing~\ref{algo:k-means}.

\begin{algorithm}
\caption{Lloyd's algorithm for $K$-Means}
\label{algo:k-means}

\begin{algorithmic}[0]
  \Statex
  \Function{K-Means}{no. clusters $k$, documents $\mathcal D$}
    \For{$j \leftarrow 1 \ .. \ k$} \Comment{random seed selection}
      \Let{$\boldsymbol \mu_j$}{random $\mathbf d \in \mathcal D$}
    \EndFor

    \While{not converged}
      \For{each $\mathbf d_i \in \mathcal D$} \Comment{cluster assignment step}
        \Let{$c_i$}{$\operatorname{arg\, min}_j \| \mathbf d_i - \boldsymbol \mu_j \|^2$}
      \EndFor

      \For{$j \leftarrow 1 \ .. \ k$} \Comment{move centroids step}
        \Let{$\mathcal C_j$}{$\{\, \mathbf d_i \text{ s.t. } c_i = j \, \}$}
        \Let{$\boldsymbol \mu_j$}
            {$\cfrac{1}{| \mathcal C_j |} \sum_{\mathbf d_i \in \mathcal C_j} \mathbf d_i$}
      \EndFor
    \EndWhile

    \State \Return{$(c_1, c_2, \ ... \ , c_n)$}
  \EndFunction
\end{algorithmic}
\end{algorithm}

Usually, $K$-Means shows very good results for document clustering, and in
several studies it (or its variations) shows the best performance
\cite{steinbach2000comparison} \cite{hall2012evaluating} .

However for large document collections Lloyd's classical $K$-Means takes a lot
of time to converge. The problem is caused by the fact that it goes through
the entire collection many times. Mini-Batch $K$-Means \cite{sculley2010web}
uses Mini-Batch Gradient Descent method, which is a different optimization technique
that converges faster.

$K$-Means uses Euclidean distance, which does not always behave
well in high-dimensional sparse vector spaces like document vector
spaces. However, as discussed in section~\ref{sec:similarity-distance}, if
document vectors are normalized, the Euclidean distance and cosine distance
are related, and therefore Euclidean $K$-means is the same as
``Cosine Distance'' $K$-Means.

$K$-Means is the most popular clustering algorithms and there are
many extensions. For example, Bisecting K-Means
\cite{steinbach2000comparison} is a combination
of partitioning and hierarchical (divisive) algorithms. It's a
variant of $K$-Means that gradually splits the document space in halves
until the desired number of clusters is obtained. Bisecting $K$-Means can
achieve good performance while giving the user additional insight into
the clustering process. Additionally, in the results it produces
clusters of comparable sizes.

The algorithm is simple: (1) start with a single cluster;
(2) choose a cluster to split (for example, the largest one);
(3) apply traditional $K$-Means to this cluster with $K=2$ to split it;
(4) repeat until have desired number of clusters.


However, when there are many documents, the centroids tend
to contain a lot of words, which leads to a significant slowdown.
To solve this problem, some terms of the centroid can be truncated.
There are several possible ways of truncating the terms: for example,
we can keep only the top $c$ terms, or remove the least frequent words
such that at least 90\% (or 95\%) of the original vector norm is
retained \cite{schutze1997projections}.


\subsubsection{DBSCAN} \label{sec:dbscan} \ \\

DBSCAN is a density-based clustering algorithm that can discover
clusters of complex shapes based on the density of data points \cite{ester1996density}.

The \emph{density} associated with a data point is obtained by
counting the number of points in a region of radius $\varepsilon$
around the point, where $\varepsilon$  is defined by the user.
If a point has a density of at least some user defined
threshold \verb|MinPts|, then it is considered a \emph{core point}.
The clusters are formed around these core points, and if two core points
are within the radius $\varepsilon$, then they belong to the same cluster.
If a point is not a core point itself, but it belong to the neighborhood of some
core point, then it is a \emph{border point}. But if a point is not a core point
and it is not in the neighborhood of any other core point, then it does not
belong to any cluster and it is considered \emph{noise}.

DBSCAN works as follows: it selects an arbitrary data point $p$, and then
finds all other points in $\varepsilon$-neighborhood of $p$. If
there are more than  \verb|MinPts| points around $p$, then it is a core point,
and it is considered a cluster. Then the process is repeated for all points in
the neighborhood, and they all are assigned to the same cluster, as $p$.
If $p$ is not a core point, but it has a core point in its neighborhood, then
it's a border point and it is assigned to the same cluster and the core point.
But if it is a noise point, then it is marked as noise or discarded
(see listing~\ref{algo:dbscan}).


\begin{algorithm}
\caption{DBSCAN}
\label{algo:dbscan}

\begin{algorithmic}[0]
  \Statex
  \Function{DBSCAN}{database $\mathcal D$, radius $\varepsilon$, MinPts}
    \Let{$\text{result}$}{$\varnothing$}

    \ForAll{$p \in \mathcal D$}
      \If{$p$ is visited}
        \State{\textbf{continue}}
      \EndIf
      \State{mark $p$ as visited}
      \Let{$\mathcal N$}{\textsc{Region-Query}($p, \varepsilon$)}
          \Comment{$\mathcal N$ is the neighborhood of $p$}
      \If{$\mathcal N < \text{MinPts}$}
        \State{mark $p$ as \texttt{NOISE}}
      \Else
        \Let{$\mathcal C$}
            {\textsc{Expand-Cluster}$(p, \mathcal N, \varepsilon, \text{MinPts})$}
        \Let{result}{result $\cup \ \{ \mathcal C \}$}
      \EndIf
    \EndFor
    \State \Return{result}
  \EndFunction
\end{algorithmic}


\begin{algorithmic}[0]
  \Statex
  \Function{Expand-Cluster}{point $p$, neighborhood $\mathcal N$, radius $\varepsilon$, MinPts}
     \Let{$\mathcal C$}{$\{ p \}$}
     \ForAll{$x \in \mathcal N$}
        \If{$x$ is visited}
          \State{\textbf{continue}}
        \EndIf

        \State{mark $x$ as visited}
        \Let{$\mathcal N_x$}{\textsc{Region-Query}$(x, \varepsilon)$}
            \Comment{$\mathcal N_x$ is the neighborhood of $x$}
        \If{$| \mathcal N_x | \geqslant \text{MinPts}$}
          \Let{$\mathcal N$}{$\mathcal N \cup \mathcal N_x$}
        \EndIf

        \Let{$\mathcal C$}{$\mathcal C \cup \{ x \}$}
     \EndFor

     \State \Return{$\mathcal C$}
  \EndFunction
\end{algorithmic}

\begin{algorithmic}[0]
  \Statex
  \Function{Region-Query}{point $p$, radius $\varepsilon$}
     \State \Return{$\{ x \ : \ \| x - p \| \leqslant \varepsilon \}$} \Comment{all points within distance $\varepsilon$ from $p$}
  \EndFunction
\end{algorithmic}

\end{algorithm}

The details of implementation of \textsc{Region-Query} are not specified,
and it can be implemented differently. For example, it can use
Inverse Index to make the similarity search faster
\cite{manning2008introduction} \cite{ertoz2003finding}.


The DBSCAN algorithm uses the Euclidean distance, but can be adapted to
use any other distance or similarity function. For example, to modify the
algorithm to use the cosine similarity (or any other similarity function)
the \textsc{Region-Query} has to be modified to return
$\{ x \ : \ \text{similarity}(x, p) \geqslant \varepsilon \}$.

Shared Nearest Neighbors Similarity (SNN Similarity) \cite{ertoz2003finding}
is a special similarity function that is particularity useful for
high-dimensional spaces, it works well with DBSCAN, and it is
applicable to document clustering and topic discovery \cite{ertoz2004finding}.

SNN Similarity is specified in terms of the $K$ nearest neighbors.
Let $\text{NN}_{K, \, \text{sim}}(p)$ be a function that returns
top $K$ closest points of $p$ according to some similarity function
\texttt{sim}. Then the SNN similarity function is  defined as
$$\text{snn}(p, q) = \big| \text{NN}_{K, \, \text{sim}}(p) \cup \text{NN}_{K, \, \text{sim}}(q) \big|.$$


The extension of DBSCAN that uses the SNN Similarity is called
SSN Clustering algorithm. The user needs to specify the SSN similarity
function by setting parameter $K$ and choosing the base similarity
function $\text{\texttt{sim}}(\cdot, \cdot)$ (typically Cosine, Jaccard
or Euclidean). The algorithm itself has the same
parameters as DBSCAN: radius $\varepsilon$ (such that $\varepsilon < K$)
and the core points density threshold \verb|MinPts|. The
$\textsc{Region-Query}$ function is modified to return
$\{ q \ : \ \text{snn}(p, q) \geqslant \varepsilon \}$. For pseudocode,
see the listing~\ref{algo:snn-clustering}.

\begin{algorithm} \caption{SNN Clustering Algorithm} \label{algo:snn-clustering}

\begin{algorithmic}[0]
  \Statex
  \Function{SNN-Cluster}{database $\mathcal D$, $K$, similarity function \texttt{sim}, radius $\varepsilon$, MinPts}
    \ForAll{$p \in \mathcal D$} \Comment{Pre-compute the $K$NN lists}
      \Let{$\text{NN}[p]$}{$\text{NN}_{K, \, \text{sim}}(p)$}
    \EndFor

    \ForAll{$(p, q) \in (\mathcal D \times \mathcal D)$} \Comment{Pre-compute the SNN similarity matrix}
      \Let{$A[p, q]$}{$\big| \, \text{NN}[p] \ \cup \ \text{NN}[q] \, \big|$}
    \EndFor

    \State \Return{\textsc{DBSCAN}$(A, \varepsilon, \text{MinPts})$}
  \EndFunction
\end{algorithmic}

\end{algorithm}


The algorithm's running time complexity is $O(n^2)$ time, where $n = |\mathcal D|$,
but it can be sped up by using the Inverted Index \cite{ertoz2003finding}.


\subsection{Latent Semantic Analysis} \label{sec:lsa}

In section~\ref{sec:clusters-namespaces} we have discussed the
lexical variability and ambiguity problems in natural language: synonymy
and polysemy. We can treat these problems as ``statistical noise'' and
apply dimensionality reduction techniques to find the optimal dimensionality
for the data and thus reduce the amount of noise there.
This technique is called Latent Semantic Analysis (LSA) \cite{landauer1998introduction}
or Latent Semantic Indexing \cite{deerwester1990indexing}, and
it is often used for document clustering \cite{aggarwal2012survey} \cite{osinski2004lingo}.

There are three major steps in Latent Semantic Analysis  \cite{evangelopoulos2012latent}:
(1) preprocess documents;
(2) construct a term-document matrix $D$ using the Vector Space Model;
(3) de-noise $D$ by reducing its dimensionality with Singular Value Decomposition (SVD).

The first two steps are the same as for traditional Vector Space Models
and in the result we obtain a term-document matrix $D$.
If $D$ has rank $r$, then the SVD of $D$ is $D = U  \Sigma V^T$, where
$U$ is an $m \times r$ orthogonal matrix;
$\Sigma$ is a diagonal $r \times r$ matrix with singular values ordered by their magnitude;
and $V$ is an $n \times r$ orthogonal matrix.

The dimensionality reduction is done by finding the best $k$-rank approximation
of $D$, which is obtained by keeping only the first $k$ singular values of $\Sigma$
and setting the rest to 0.
Typically, not only $\Sigma$ is truncated, but also $U$ and $V$,
and therefore, the $k$-rank approximation of $D$ using SVD is written as
$D \approx D_k = U_k \Sigma_k V_k^T$ where $U_k$ is an $m \times k$
matrix with first $k$ columns of $U$, $\Sigma_k$ is an $k \times k$
diagonal matrix with singular values, and $V_k$ is an $n \times k$
matrix with first $k$ columns of $V$.  This decomposition
is called \emph{rank-reduced} SVD and when applied to text data
it reveals the ``true'' latent semantic space. The parameter $k$ corresponds
to the number of ``latent concepts'' in the data. The idea
of LSA is very nicely illustrated by examples  in
\cite{deerwester1990indexing} and \cite{landauer1998introduction}.

LSA can be used for clustering as well, and this is usually done
by first transforming the document space to the LSA space
and then doing applying transitional cluster analysis techniques
there \cite{schutze1997projections}.
Once $D$ is decomposed as $D \approx U_k \Sigma_k V_k^T$
it is enough to keep only the low dimensional representation $\hat D = V_k \Sigma_k$:
the calculation of inner product between two documents $i$ and $j$ in
the reduced semantic  space corresponds to computing the inner product
between $i$th and $j$th rows of $\hat D$ \cite{deerwester1990indexing}. Since the
Euclidean distance is defined in terms of inner product, it can also be used
directly on the rows of $\hat D$.

Therefore, a generic LSA-based clustering algorithm consists of the following steps:

\begin{enumerate}
\itemsep1pt\parskip0pt\parsep0pt
  \item Build a term-document matrix $D$ from the document collection;
  \item Select number of latent concepts $k$ and apply rank-reduced SVD on $D$
      to get $\hat D= V_k \Sigma_k$;
  \item Apply the cluster algorithm on the rows of $V_k \Sigma_k$.
\end{enumerate}


LSA has some drawbacks. Because SVD looks for an orthogonal basis for the new
reduced document space, there could be negative values that are harder
to interpret, and what is more, the cosine similarity can become negative as well.
However, it does not significantly affect the cosine distance: it still
will always give non-negative results.

Apart from SVD there are many other different matrix decomposition
techniques that can be applied for document clustering and for discovering
the latent structure of the term-document matrix \cite{osinski2006improving},
and one of them in Non-Negative Matrix Factorization (NMF) \cite{lee1999nnmf}.
Using NMF solves the problem of negative coefficients:
when it is applied to non-negative data such as term-document matrices,
NMF produces non-negative rank-reduced approximations.

The main conceptual difference between SVD and NMF is that SVD looks for
orthogonal directions to represent document space, while NMF does not
require orthogonality \cite{xu2003document} (see fig.~\ref{fig:nmf-svd}).


\begin{figure}[h]
\centering\includegraphics[width=0.8\textwidth]{nmf-svd.pdf}
\caption{Directions found by  SVD (on the left) vs directions by NMF (on the right)}
\label{fig:nmf-svd}
\end{figure}


The NMF of an $m \times n$ term-document matrix $D$ is $D \approx D_k = U  V^T$
where $U$ is an $m \times k$ matrix, $V$ is an $n \times k$ matrix and
$k$ is the number of semantic concepts in $D$.
Non-negativity of elements in $D_k$ is very good for interpretability: it
ensures that documents can be seen as a non-negative combination of
the key concepts.

Additionally, NMF is useful for clustering: the results of NMF can
be directly interpreted as cluster assignment and there is no need
to use separate clustering algorithms \cite{xu2003document}. When $D$ is a
term-document matrix and $D \approx U V^T$, then elements $(V)_{ij}$
represent the degree to which document $i$ belongs to cluster $j$.

The document clustering using NMF consists of the following steps \cite{xu2003document}:

\begin{enumerate}
  \item Construct the term-document matrix $D$ and perform NMF on $D$ to get $U$ and $V$;
  \item Normalize rows $\mathbf v_i$ of $V$ by using the rule $\mathbf v_i \leftarrow \mathbf v_i \, \| \mathbf u_i \|$;
  \item Assign document $\mathbf d_i$ to cluster $x$ if $x = \operatorname{arg \, max}_j (V)_{ij}$.
\end{enumerate}

If the desired number of clusters $K$ is larger than the rank $k$ of the
reduced matrix $D_k$, the clustering can be performed directly on the rows
of $V$, for example, by using $K$-Means.

% The computational complexity on NMF is $O(kn)$
