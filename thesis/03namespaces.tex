\section{Namespaces as Document Clusters}
\subsection{Discovering Namespaces with Cluster Analysis}

Why it will work?

List properties of text that identifier-based representation of documents share as well

list characteristics of textual data



\begin{itemize}
  \item dimensionality is very large, but vectors are very sparse.
    e.g. vocabulary size $| V | = 10^5$, but documents may contain only 500 distinct words,
    or even less - when we consider sentences or tweets
  \item lexicon of document may be large, but words are typically correlated with each other
    so number of concepts ("principal components") is $\ll | V |$
  \item number of words across different documents may wary a lot
  \item word distributions follow Power Laws (Zipf's law etc - cite)
\end{itemize}

Problems in text:

\begin{itemize}
  \item problems of polysemy, homonymy and synonymy: semantic relations between words
  \item Polysemy is the capacity for a sign (such as a word, phrase, or symbol) to have multiple meanings (multiple senses)
  \item The state of being a homonym is called homonymy. In linguistics, a homonym is, in the strict sense, one of a group of words that share the same spelling and pronunciation but have different meanings.[1] http://dictionary.reference.com/browse/homonym
  \item Words that are synonyms are said to be synonymous, and the state of being a synonym is called synonymy.
  \item The analysis of synonymy, polysemy, and hyponymy and hypernymy is vital to taxonomy and ontology in the information-science senses of those terms.
  \item Homonyms are words that have the same pronunciation and spelling, but have different meanings. For example, rose (a type of flower) and rose (past tense of rise) are homonyms.
\end{itemize}

This is also true for identifiers and they have the same problems (illustrate that)
These problems are studied in IR and NLP literature, so we can apply them for identifiers


Identifier spaces (need to define what it is - analogous to documents vector space models) have the same characteristics so we can apply vector space techniques

in VSM for word sense disambiguation often word meaning is attached e.g. "bank\_finances", can do the same e.g. "E\_energy"

Then we discover namespaces in the identifier namespaces - it's done by clustering document-identifier matrix


\textbf{How to deal with these problems?} Term Extraction techniques:
these techniques create "artificial" terms that aren't really terms - they are generated, and not the ones that actually occurred in the text
The original terms don't have the optimal dimensionality for document content representation
because of the problems of polysemy, homonymy and synonymy
so we want to find better representation that doesn't suffer from these issues


\subsection{Similarity Measures and Distances}



\subsection{Vector Space Model}

Vector Space model: term selection, weighting schemes; how to build identifier space and include definition information

\cite{sebastiani2002machine} - \emph{classification}



TF-IDF


tf is normalized by idf

idf - reduces weights of terms that occur more frequently

When applied to identifiers: some identifiers like $x$ or $y$ occur very frequently and don't have much discriminating power


to ensure that document matching is done with more discriminative words
apply sub-linear transformation to to avoid the dominating effect of words that occur very frequently


\subsection{Identifier Space Model}


There are three ways of incorporating the definition information into the identifier space.

Given ($\lambda$, regularization) and ($w$, weight vector)

\begin{itemize}
  \item use only identifier information. vector is ($\lambda$, $w$)
  \item use "weak" identifier-definition association: vector is ($\lambda$, $w$, regularization, weight vector)
  \item use "strong" association: document representation is ($\lambda$\_regularization, $w$\_weight vector)
\end{itemize}
