\section{Document Clustering Techniques} \label{sec:doc-clustering}

Cluster analysis is a set of techniques for
organizing collection of items into coherent groups. 
In Text Mining clustering is often used for finding topics 
in a collection of document. 
In Information Retrieval clustering is used to assist the users and group 
retrieved results into clusters. 


There are several types of clustering algorithms: 

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item Hierarchical (Agglomerative and Divisive)
  \item Partitioning
  \item Density-based
  \item and others
\end{itemize}

In this chapter we will review some of the clustering techniques: 
in section~\ref{sec:clustering-heierarchical} we will discuss 
Agglomerative clustering. We discuss K-Means, a partitioning algorithms,
in section~\ref{sec:kmeans} and its extensions in section~\ref{sec:kmeans-ext}.
Finally, a density-based algorithm DBSCAN is explained in 
section~\ref{sec:dbscan} along with its extensions in section~\ref{sec:dbscan-ext}.


\subsection{Agglomerative clustering} \label{sec:clustering-heierarchical}

The general idea of agglomerative clustering algorithms is to start with 
each document being its own cluster and iteratively merge clusters based 
on best pair-wise cluster similarity.

Thus, a typical agglomerative clustering algorithms consists of the following steps:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item let each document be a cluster on its own
  \item compute similarity between all pairs of clusters an store the 
      results in a similarity matrix
  \item merge two most similar clusters
  \item update the similarity matrix
  \item repeat until everything belongs to the same cluster
\end{itemize}

These algorithms differ only in the way they calculate 
similarity between clusters. 

It can be: 

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item \textbf{Single Linkage (SLINK)}: The clusters are merged based
on the closest pair. It can be very efficient, but it encourages 
chaining behavior.

% similarity is usually not transitive:
%i.e. if $A$ is similar to $B$, and $B$ is similar to $C$, it doesn't mean that $A$ must be similar to $C$
% but single linkage encourages grouping through transitivity chains
% Sibson, Robin. "SLINK: an optimally efficient algorithm for the single-link cluster method." 1973.

  \item \textbf{Complete Linkage (CLINK)}: The clusters are merged 
  based on the worst-case similarity - the similarity between the most 
  distant objects on the clusters. It's very expensive computationally, 
  but it avoids chaining altogether. 

% Defays, Daniel. "An efficient algorithm for a complete link method." 1977. 

  \item \textbf{Group-Average Linkage}: Similarity between clusters 
  is calculated as average pair-wise similarity between all objects 
  in the clusters, and the most similar clusters are merged.

  \item \textbf{Ward's Method}: The clusters to merge are chosen such that the within-cluster
  error (e.g. sum of squares) between each object and its centroid 
  is minimized. 

% El-Hamdouchi, Abdelmoula, and Peter Willett. "Hierarchic document classification using Ward's clustering method." 1986.

\end{itemize}


Among these algorithms only SLINK is computationally feasible 
for large data sets, but it doesn't give good results compared to other 
agglomerative clustering algorithms. Additionally, these algorithms 
are not always good for document clustering because they tend to 
make mistakes at early iterations that are impossible to correct
afterwards \cite{steinbach2000comparison}.



\subsection{$K$-Means} \label{sec:kmeans}

Unlike agglomerative clustering algorithms, K-Means is an iterative 
algorithm, which means that it can correct the mistakes made 
at earlier iterations. 

Lloyd's algorithm  is the most popular way of implementing K-Means 
\cite{xu2005survey}: given a desired number of clusters $k$, 
it iteratively improves the Euclidean distance between each data 
point and the centroid, closest to it. 


Let $\mathcal D = \{  \mathbf d_1, \mathbf d_2, \ ... \ , \mathbf d_n \}$ 
be the document collection, where documents $\mathbf d_i$ are represented 
is some document vector space $\mathbb R^m$ and $k$ is the desired 
number of clusters. Then we define $k$ cluster centroids $\boldsymbol \mu_j$ that are 
also in the same document vector space $\mathbb R^m$. 
Additionally for each document $\mathbf d_i$ we maintain the assignment 
variable $c_i \in \{ 1, 2, \ ... \ , k \}$, which specifies to what
cluster centroid $\boldsymbol \mu_1, \boldsymbol \mu_2, \ ... \ , \boldsymbol \mu_k$ 
the document $\mathbf d_i$ belongs. 


The algorithms consists of three steps: (1) seed selection step, 
where each $\boldsymbol \mu_j$ is randomly assigned some value, 
(2) cluster assignment step, where we iterate over all document vectors
$\mathbf d_i$ and find its closest centroid, and (3)  move centroids step,
where the centroids are re-calculated. Steps (2) and (3) are repreated 
until the algorithm converges. The pseudocode for $K$-Means is presented 
in the listing~\ref{algo:k-means}.


\begin{algorithm}
\caption{Lloyd's algorithm for $K$-Means}
\label{algo:k-means}

\begin{algorithmic}[0]
  \Statex
  \Function{K-Means}{no. clusters $k$, documents $\mathcal D$}
    \For{$j \leftarrow 1 \ .. \ k$} \Comment{random seed selection}
      \Let{$\boldsymbol \mu_j$}{random $\mathbf d \in \mathcal D$}
    \EndFor

    \While{not converged}
      \For{each $\mathbf d_i \in \mathcal D$} \Comment{cluster assignment step}
        \Let{$c_i$}{$\operatorname{arg\, min}_j \| \mathbf d_i - \boldsymbol \mu_j \|^2$}
      \EndFor

      \For{$j \leftarrow 1 \ .. \ k$} \Comment{move centroids step}
        \Let{$\mathcal C_j$}{$\{\, \mathbf d_i \text{ s.t. } c_i = j \, \}$}
        \Let{$\boldsymbol \mu_j$}
            {$\cfrac{1}{| \mathcal C_j |} \sum_{\mathbf d_i \in \mathcal C_j} \mathbf d_i$}
      \EndFor
    \EndWhile

    \State \Return{$(c_1, c_2, \ ... \ , c_n)$}
  \EndFunction
\end{algorithmic}
\end{algorithm}

Usually, $K$-Means shows very good results for document clustering, and in 
several studies it (or its variations) shows the best performance \cite{hall2012evaluating} \cite{steinbach2000comparison}. 

However for large document collections Lloyd's classical $K$-Means takes a lot 
of time to converge. The problem is caused by the fact that it goes through 
the entire collection many times. Mini-Batch $K$-Means \cite{sculley2010web} 
uses Mini-Batch Gradient Descent method, which is a different optimization technique
that converges faster. The pseudocode for Mini-Batch $K$-Means is presented 
in listing~\ref{algo:minibatch-k-means}.

\begin{algorithm}
\caption{MiniBatch $K$-Means}
\label{algo:minibatch-k-means}

\begin{algorithmic}[0]
  \Statex
  \Function{MiniBatch-K-Means}{no. clusters $k$, no. iterations $t$, batch size $b$, documents $\mathcal D$}
    \For{$j \leftarrow 1 \ .. \ k$} \Comment{random initialization}
      \Let{$\boldsymbol \mu_j$}{random $\mathbf d \in \mathcal D$}
    \EndFor

    \Repeat{\ $t$ times}
      \Let{$\mathcal M$}{$b$ random examples from $\mathcal D$}

      \For{each $\mathbf d_i \in \mathcal M$} 
        \Let{$\text{centroids}[\mathbf d_i]$}
            {$\operatorname{arg\, min}_j \| \mathbf d_i - \boldsymbol \mu_j \|^2$}
             \Comment{cache the centroid nearest to $\mathbf d_i$}
      \EndFor

      \For{each $\mathbf d_i \in \mathcal M$}
        \Let{$c_i$}{$\text{centroids}[\mathbf d_i]$}  
                                       \Comment{the centroid index of document $\mathbf d_i$}
        \Let{$v[c_i]$}{$v[c_i] + 1$}   \Comment{counts per centroid $c_i$}
        \Let{$\eta$}{$1 / v[c_i]$}     \Comment{per-centroid learning rate}
        \Let{$\boldsymbol \mu_{c_i}$}
        {$(1 - \eta) \cdot \boldsymbol \mu_{c_i} + \eta \cdot \mathbf d_i$}
                                        \Comment{gradient step}
      \EndFor
    \Until{converged}

    \State \Return{$(c_1, c_2, \ ... \ , c_n)$}
  \EndFunction
\end{algorithmic}
\end{algorithm}


Note that $K$ means uses Euclidean distance, and Euclidean distance
does not always behave well in high-dimensional sparse vector spaces 
like Document VSMs (see section~\ref{sec:similarity-distance}). However, 
as discussed in section~\ref{sec:similarity-distance}, 
if document vectors are normalized, the Euclidean distance
and Cosine distance are related, and therefore 
Euclidean $K$-means is the same as ``Cosine Distance'' $K$-Means.

$K$-Means is the most popular clustering algorithms and there are 
many extensions to this algorithm. In the next section we will
discuss some of the extensions related to document clustering. 


\subsection{Extensions of $K$-Means} \label{sec:kmeans-ext}

There are several extensions of $K$-Means. 


For example, Bisecting K-Means \cite{steinbach2000comparison} is a combination 
of partitioning and hierarchical (divisive) algorithms. It's a variant of $K$-Means that 
gradually splits the document space in halves until the desired number of clusters 
is obtained. Bisecting K-Means can achieve good performance while 
giving the user additional information about ... ?

Algorithm:

\begin{itemize}
  \item start with a single cluster
  \item choose a cluster to split (for example, the largest one)
  \item apply $K$-means to this cluster with $K=2$ to split it
  \item repeat until have desired number of clusters
\end{itemize}


\textbf{TODO: pseudocode}

% may repeat this procedure several times and take the clusters with highest overall similarity

\ \\

Scatter/Gather is another popular variation of $K$-means, but 
initially used for clustering retrieved documents for Information 
Retrieval systems \cite{cutting1992scatter}. This variation
includes: special smart seed selection procedure (applying 
hierarchical cluster on a subset of document vectors to
initialize the centroids at the initialization step) and
several cluster refinement operations. 
Additionally, in Scatter/Gather cluster centroids are concatenations 
of all terms in the cluster documents, not a mean value;
and the cosine similarity is used instead of Euclidean 
distance. 


There are two cluster refinement operations: split and join.

The split operation is used to continue splitting clusters,
and it's applied only to the clusters that are not coherent 
enough. Essentially, the split operation splits the non-coherent
clusters in the same way as Bisecting $K$-Means. 
The coherence is measured via \emph{self-similarity} of a cluster,
which is the mean similarity of all documents in the cluster to 
its centroid, or the mean pair-wise similarity between all documents 
of the cluster. 

The join operation merges the clusters that are very similar 
to each other. The similarity is measured by computing ``typical''
terms for each cluster (usually the most frequent terms of 
the centroid) and examining which clusters have significant
overlaps between their typical terms. 


However, when there are many documents, the centroids tend 
to contain a lot of words, which leads to a significant slowdown. 
A solution to this problem is a center adjustment method, called
vector average dumping \textbf{TODO} \cite{larsen1999fast}. 
Alternatively, some terms of the centroid can be 
truncated. There are several possible ways of trucating
the terms: for example, we can keep only the top $c$ terms, or
remove the least frequent words such that at least 90\% (or 95\%) of 
the original vector norm is retained \cite{schutze1997projections}.



\subsection{DBSCAN} \label{sec:dbscan}


DBSCAN is a clustering algorithm that can discover 
clusters of complex shapes based on the density of 
data points \cite{ester1996density}. 

The \emph{density} associated with a data point is obtained by 
counting the number of points in a region of specified radius $\varepsilon$
around the point. If a point has a density of at least some user defined 
threshold $\text{MinPts}$, then it is considered a \emph{core point}. 
The clusters are formed around these core points, and if two core points 
are within the radius $\varepsilon$, then they belong to the same cluster. 
If a point is not a core point itself, but it belong to the neighborhood of some 
core point, then it is a \emph{border point}. But if a point is not a core point 
and it is not in the neighborhood of any other core point, then it does not 
belong to any cluster and it is considered \emph{noise}. 

DBSCAN works as follows: it selects an arbitrary data point $p$, and then 
finds all other points in $\varepsilon$-neighborhood of $p$. If 
there are more than $\text{MinPts}$ points around $p$, then it is a core point, 
and it is considered a cluster. Then the process is repeated for all points in 
the neighborhood, and they all are assigned to the same cluster, as $p$. 
If $p$ is not a core point, but it has a core point in its neighborhood, then 
it's a border point and it is assigned to the same cluster and the core point.
But if it is a noise point, then it is marked as noise or discarded. 


\begin{algorithm}
\caption{DBSCAN}
\label{algo:dbscan}

\begin{algorithmic}[0]
  \Statex
  \Function{DBSCAN}{database $\mathcal D$, radius $\varepsilon$, MinPts}
    \Let{$\text{result}$}{$\varnothing$}

    \ForAll{$p \in \mathcal D$} 
      \If{$p$ is visited}
        \State{\textbf{continue}}
      \EndIf
      \State{mark $p$ as visited}
      \Let{$\mathcal N$}{\textsc{Region-Query}($p, \varepsilon$)} 
          \Comment{$\mathcal N$ is the neighborhood of $p$}
      \If{$\mathcal N < \text{MinPts}$}
        \State{mark $p$ as \texttt{NOISE}}
      \Else
        \Let{$\mathcal C$}
            {\textsc{Expand-Cluster}$(p, \mathcal N, \varepsilon, \text{MinPts})$}
        \Let{result}{result $\cup \ \{ \mathcal C \}$}
      \EndIf
    \EndFor
    \State \Return{result}
  \EndFunction
\end{algorithmic}


\begin{algorithmic}[0]
  \Statex
  \Function{Expand-Cluster}{point $p$, neighborhood $\mathcal N$, radius $\varepsilon$, MinPts}
     \Let{$\mathcal C$}{$\{ p \}$}
     \ForAll{$x \in \mathcal N$}
        \If{$x$ is visited}
          \State{\textbf{continue}}
        \EndIf

        \State{mark $x$ as visited}
        \Let{$\mathcal N_x$}{\textsc{Region-Query}$(x, \varepsilon)$}
            \Comment{$\mathcal N_x$ is the neighborhood of $x$}
        \If{$| \mathcal N_x | \geqslant \text{MinPts}$}
          \Let{$\mathcal N$}{$\mathcal N \cup \mathcal N_x$}
        \EndIf

        \Let{$\mathcal C$}{$\mathcal C \cup \{ x \}$}
     \EndFor

     \State \Return{$\mathcal C$}
  \EndFunction
\end{algorithmic}

\begin{algorithmic}[0]
  \Statex
  \Function{Region-Query}{point $p$, radius $\varepsilon$}
     \State \Return{$\{ x \ : \ \| x - p \| \leqslant \varepsilon \}$} \Comment{all points within distance $\varepsilon$ from $p$}
  \EndFunction
\end{algorithmic}

\end{algorithm}


The DBSCAN algorithm uses the Euclidean distance, 
but can be adapted to use any other distance or similarity function.
For example, to modify the algorithm to use the Cosine similarity 
(or any other similarity function)  
the \textsc{Region-Query} has to be modified to return
$\{ x \ : \ \text{similarity}(x, p) \geqslant \varepsilon \}$.

The details of implementation of \textsc{Region-Query} 
are not specified, and it can be implemented differently. 
For example, it can use Inverse Index (see section~\ref{sec:index}, and listing~\ref{algo:inverted-index} for the pseudocode) 
to make the similarity search faster. 


\subsection{Extensions of DBSCAN} \label{sec:dbscan-ext}

As discussed, DBSCAN can be extended to use any distance or similarity function. 
Shared Nearest Neighbors Similarity (SNN Similarity) \cite{ertoz2003finding} 
is a special similarity function that is particularity useful for 
high-dimensional spaces. 
And this similarity function is applicable to document clustering 
and topic discovery \cite{ertoz2004finding}. 

SNN Similarity is specified in terms of the $K$ nearest neighbors. 
Let $\text{NN}_{K, \, \text{sim}}(p)$ be a function that returns 
top $K$ closest points of $p$ according to some similarity function 
\texttt{sim}. Then the SNN similarity function is  defined as 
$$\text{snn}(p, q) = \big| \text{NN}_{K, \, \text{sim}}(p) \cup \text{NN}_{K, \, \text{sim}}(q) \big|$$


The extension of DBSCAN that uses the SNN Similarity is called 
SSN Clustering algorithm. The user needs to specify the SSN similarity
function by setting parameter $K$ and choosing the base similarity 
function $\text{\texttt{sim}}(\cdot, \cdot)$ (typically Cosine, Jaccard 
or Euclidean). The algorithm itself has the same
parameters as DBSCAN: radius $\varepsilon$ (such that $\varepsilon < K$) 
and the core points density threshold MinPts. The 
$\textsc{Region-Query}$ function is modified to return
$\{ q \ : \ \text{snn}(p, q) \geqslant \varepsilon \}$. For pseudocode, 
see the listing~\ref{algo:snn-clustering}. 

\begin{algorithm} \caption{SNN Clustering Algorithm} \label{algo:snn-clustering}

\begin{algorithmic}[0]
  \Statex
  \Function{SNN-Cluster}{database $\mathcal D$, $K$, similarity function \texttt{sim}, radius $\varepsilon$, MinPts}
    \ForAll{$p \in \mathcal D$} \Comment{Pre-compute the $K$NN lists}
      \Let{$\text{NN}[p]$}{$\text{NN}_{K, \, \text{sim}}(p)$}
    \EndFor

    \ForAll{$(p, q) \in (\mathcal D \times \mathcal D)$} \Comment{Pre-compute the SNN similarity matrix}
      \Let{$A[p, q]$}{$\big| \, \text{NN}[p] \ \cup \ \text{NN}[q] \, \big|$}
    \EndFor

    \State \Return{\textsc{DBSCAN}$(A, \varepsilon, \text{MinPts})$}
  \EndFunction
\end{algorithmic}

\end{algorithm}

The algorithm's running time complexity is $O(n^2)$ time, where $n = |\mathcal D|$, 
but it can be sped up by using the Inverted Index.

% \subsection{Scaling?}
% LSH etc