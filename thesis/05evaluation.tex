\section{Evaluation} \label{sec:evaluation}

% Describe  the dataset and experimental setup; describe the results.
In this chapter we describe the experimental setup and the obtained 
results. 

First, section~\ref{sec:param-tuning} describes parameter tuning: there 
are many possible choices of parameters, and we find the best. 
Once the best algorithm and its parameters are selected, we analyze 
the obtained results in section~\ref{sec:result-analysis}. Next, 
we describe how the discovered clusters can be mapped to a hierarchy 
in section~\ref{sec:hierarchy}, and finish by summarizing our findings 
in section~\ref{sec:evaluation-summary}.



\subsection{Parameter Tuning} \label{sec:param-tuning}

There are many different clustering algorithms, each with its own set
of parameter. In this section we describe how we find the settings that
find the best namespaces.

The following things can be changed:

\begin{itemize}
  \item Ways to incorporate definition information (no definitions, soft association, hard association);
  \item Weighting schemes for the identifier-document matrix $D$: TF, sublinear TF, TF-IDF;
  \item There are different clustering algorithms: agglomerative clustering, DBSCAN, SNN
  clustering, $K$-Means, %Bisecting $K$-Means,
  and each algorithm has its own set of parameters;
  \item Dimensionality of $D$ can be reduced via SVD or NMF, parameter $k$ controls the
      rank of output.
\end{itemize}


To find the best parameters set we use the grid search approach: we try
different combinations of parameters and keep track on the number of
pure clusters and the purity.

The overall purity of cluster assignment is calculated as a weighed sum
of individual  cluster purities, where the weight is chosen proportionally to
the size of a cluster.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.75\textwidth}
  \centering\includegraphics[width=\textwidth]{purity.pdf}
  \caption{Number of clusters $K$ vs overall purity of clustering:
      the purity increases linearly with $K$ ($R^2 = 0.99$).}
  \label{fig:k-vs-purity}
\end{subfigure}
\begin{subfigure}[b]{0.75\textwidth}
  \centering\includegraphics[width=\textwidth]{pure-clusters.pdf}
  \caption{Number of clusters $K$ vs the number of pure clusters: it grows initially, but after $K\approx 8\,000$ starts to decrease.}
  \label{fig:k-vs-pureclusters}
\end{subfigure}
\caption{Purity and number of pure clusters as measures of algorithm performance.}
\label{fig:performace}
\end{figure}

However it is not enough just to find the most pure cluster assignment: because
as the number of clusters increases the overall purity also grows.
Thus we can also optimize for the number of clusters with purity $p$ of
size at least $n$.
When the number of clusters increase, the purity always grows
(see fig.~\ref{fig:k-vs-purity}), but at some point the number of pure clusters
will start decreasing (see fig.~\ref{fig:k-vs-pureclusters}).



\subsubsection{Baseline} \ \\

We compare the performance of clustering algorithms against a random
categorizer. The simplest version of such a categorizer is the random
cluster assignment categorizer, which assigns each document to some random
cluster.
In this case, we constrain the categorizer to include 3 documents in each
cluster, and once a document belongs to some cluster, it cannot be re-assigned.
It is done by first creating a vector of assignments and shuffling it.

Then we record how many pure clusters (at least 80\% pure) are in the cluster
assignment.

\begin{figure}[h!]
\centering\includegraphics[width=0.7\textwidth]{baseline.pdf}
\caption{Distribution of the number of pure clusters across 200 trials.}
\label{fig:baseline}
\end{figure}

To establish the baseline, we repeated this experiment for 200
times (see fig.~\ref{fig:baseline}), and the maximal achieved value is 39 pure
clusters, while the mean value is 23.85.


% \textbf{TODO }Also: experiment with not just three, but with some
% probability distribution say clusters can vary from 3 to 10 or whatever.



\subsubsection{Only Identifiers} \ \\

The first way of building the identifier space is to use only identifiers
and do not use definitions at all.
If we do this, the identifier-document matrix is $6075 \times 22512$
(we keep only identifiers that occur at least twice), and it contains  302\, 541
records, so the density of this matrix is just 0.002.

First, we try to apply agglomerative clustering, then DBSCAN with SNN similarity
based on Jaccard coefficient and cosine similarity, then we
do $K$-Means and finally we apply LSA using SVD and NMF and apply
$K$-Means on the reduced space.


\begin{figure}[h!]
\centering\includegraphics[width=0.75\textwidth]{agglo-time.pdf}
\caption{Runtime of agglomerative clustering is quadratic with the number of documents
in the collection ($R^2 = 0.99$).}
\label{fig:agglo-time}
\end{figure}

\textbf{Agglomerative clustering} algorithms are quite fast for small datasets,
but they become more computationally expensive as the dataset size grows.
We run a series of experiments on subsamples of our dataset and we can observe that the run
time is quadratic with the number of documents to cluster
(see fig.~\ref{fig:agglo-time}). The polynomial regression model that we built predicts
that it should process the entire dataset of 22\,512 documents in 90 minutes,
but it was not able to finish in several hours, so we stopped the computation.
Additionally, the implementation we use from scikit-learn
requires a dense matrix. But when densified, the identifier-document matrix
occupies a lot of space: if each element of the matrix is represented with a double
precision number, then this matrix occupies 1.01 Gb of RAM in total. While
it is small enough to fit into memory, matrices of larger dimensionality might not.
Therefore we exclude these clustering algorithms from further analysis.
% Agglomerative: Wald linkage: takes forever never finished


\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-jac10.pdf}
  \caption{Number of clusters when 10 nearest neighbors are considered}
  \label{fig:nodef-dbscan-jac10}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-jac10-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 10 nearest neighbors}
  \label{fig:nodef-dbscan-jac10-2}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-jac13.pdf}
  \caption{Number of clusters when 15 nearest neighbors are considered}
  \label{fig:nodef-dbscan-jac15}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-jac13-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 15 nearest neighbors}
  \label{fig:nodef-dbscan-jac15-2}
\end{subfigure}
\caption{Effect of parameters $\varepsilon$, \texttt{MinPts} and number of nearest
 neighbors on performance of SNN DBSCAN when Jaccard coefficient is used.}
\label{fig:nodef-dbscan-jac}
\end{figure}

The second method is \textbf{DBSCAN} with \textbf{SNN Similarity}.
To compute SSN similarity we need to use some other base similarity measure.
We start with Jaccard coefficient, and use a binarized identifier-document
matrix: a matrix with only ones and zeros.
For example, the closest article to ``Linear Regression'' is
``Linear predictor function'' with Jaccard coefficient of 0.59
and ``Low-rank approximation'' is the closest to ``Singular value decomposition''
with coefficient of 0.25. With Jaccard, we were able to discover 87
clusters, which is two times better than the baseline (see fig.~\ref{fig:nodef-dbscan-jac})
and the best parameters are 10 nearest neighbors,
$\varepsilon=3$ and \texttt{MinPts} $=4$ (see fig.~\ref{fig:nodef-dbscan-jac10-2}).




Then we run the same algorithm, but with cosine similarity, using an
identifier-document matrix with $(\log \text{TF}) \times \text{IDF}$
weights, and calculate pair-wise similarity between each document.
For example, let us take an article ``Linear regression''
%it contains $h_i, m, n, p, T, t, t_i, X, x, x_{11},$ $x_{21}, x_i, y, y_1,$
%$y_2, y_i, y_n, Z,  \beta, \beta_1, \beta_2, \beta_p, \varepsilon, \varepsilon_1,
%\varepsilon_2,$ $\varepsilon_i, \varepsilon_n, \Omega$
and calculate the cosine with the rest of the corpus. The closest document
is ``Linear predictor function''.
% with identifiers
%$b, c, c_1, f, m, p1, T, x, X,$ $x_{11}, x_i, x_K, y, y_2, y_i, y_n, \beta_0,
%\beta_1, \beta_2, \beta_p, \beta 2, \varepsilon,$ $\varepsilon_1, \varepsilon_2, \varepsilon_i,$
%$\phi, \phi_1, \phi_2$.
They have 23 identifiers in common, and they indeed look related.
However cosine is not always giving the best closets neighbors. For example,
the nearest neighbor of ``Singular value decomposition'' is ``Rule of Sarrus'',
and although their cosine score is 0.92, they have only 3 identifiers in common.

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-cos10.pdf}
  \caption{Number of clusters when 10 nearest neighbors are considered}
  \label{fig:nodef-dbscan-cos10}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-cos10-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 10 nearest neighbors}
  \label{fig:nodef-dbscan-cos10-2}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-cos15.pdf}
  \caption{Number of clusters when 15 nearest neighbors are considered}
  \label{fig:nodef-dbscan-cos15}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{nodef-dbscan-cos15-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 15 nearest neighbors}
  \label{fig:nodef-dbscan-cos15-2}
\end{subfigure}
\caption{Effect of parameters $\varepsilon$, \texttt{MinPts} and number of nearest
 neighbors on performance of SNN DBSCAN when cosine is used.}
\label{fig:nodef-dbscan-cos}
\end{figure}


With cosine as the base similarity function for SNN DBSCAN we
were able to discover 124 namespace-defining clusters (see fig.~\ref{fig:nodef-dbscan-cos}),
which is significantly better than the baseline. The best parameters
are 10 nearest neighbors and $\varepsilon=4$, \texttt{MinPts} $=3$
(see fig.~\ref{fig:nodef-dbscan-cos10-2}).


\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.75\textwidth}
  \centering\includegraphics[width=\textwidth]{k-vs-time.pdf}
  \caption{$K$ in $K$-Means vs time in minutes ($R^2 = 0.99$).}
  \label{fig:k-vs-time}
\end{subfigure}

\begin{subfigure}[b]{0.75\textwidth}
  \centering\includegraphics[width=\textwidth]{k-vs-time-minibatch.pdf}
  \caption{$K$ in MiniBatch $K$-Means vs time in seconds ($R^2 = 0.97$).}
  \label{fig:k-vs-time-minibatch}
\end{subfigure}
\caption{Runtime of $K$-Means and MiniBatch $K$-Means}
\label{fig:kmeans-vs-minibatch1}
\end{figure}

Next, we apply \textbf{$K$-Means}. We observe that increasing~$K$
leads to linear increase in time (see fig.~\ref{fig:k-vs-time}),
which means that for bigger values of~$K$, it takes longer, so it is not
feasible to run: for example, we estimate the runtime of $K$-Means with $K = 10\, 000$
to be about 4.5 hours. As \textbf{MiniBatch $K$-Means} is expected to be significantly
faster than usual $K$-Means, we use it as well. Although we observe that the run
time of  MiniBatch $K$-Means also increases linearly with~$K$
(see fig.~\ref{fig:k-vs-time-minibatch}), it indeed runs considerably faster.
For example, MiniBatch $K$-Means takes 15 seconds with $K=700$ while
usual $K$-Means takes about 15 minutes (see fig.~\ref{fig:k-vs-time}
and fig.~\ref{fig:k-vs-time-minibatch}).


\begin{figure}[h!]
\centering
\hfill
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{k-vs-purity2.pdf}
  \caption{Purity vs number of clusters $K$ in \mbox{$K$-Means} and MiniBatch $K$-Means}
  \label{fig:k-vs-purity2}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{k-vs-len2.pdf}
  \caption{Number of pure clusters vs $K$ in $K$-Means and MiniBatch $K$-Means}
  \label{fig:k-vs-len2}
\end{subfigure}
\hfill
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth]{k-vs-len-mb.pdf}
  \caption{Number of pure clusters vs $K$ in MiniBatch $K$-Means for larger $K$}
  \label{fig:k-vs-len-mb}
\end{subfigure}
\hfill
\caption{Effect of $K$ on performance in $K$-Means}
\label{fig:kmeans-vs-minibatch}
\end{figure}


Usual $K$-Means with small $K$ does not find many pure clusters,
and MiniBatch $K$-Means does even worse: independently of the choice of $K$,
the number of pure clusters and purity does not change significantly 
(see fig.~\ref{fig:k-vs-purity2} and fig.~\ref{fig:k-vs-len2}). This is also true 
for larger values of $K$ (see fig.~\ref{fig:k-vs-len-mb}).


% to the end of subsubsection?
%We can conclude that although MiniBatch $K$-Means is
%very fast, it does not perform well on the untransformed data when we don't use
%definitions.


The best result was found by usual $K$-Means with $K=600$: it was able
to discover 19 clusters with purity at least 0.8 (note that this is worse
than the baseline of 39 pure clusters).

Next, we use \textbf{Latent Semantic Analysis} with \textbf{SVD}
to reduce the dimensionality of the identifier-document
matrix $D$, and then apply $K$-Means on the reduced space. As discussed in the
LSA section (see section~\ref{sec:lsa}), it should reveal the latent structure of data.
Hence, we expect that it should improve the results achieved by usual $K$-Means.

\begin{figure}[h!]
\centering\includegraphics[width=0.75\textwidth]{k-svd-vs-time.pdf}
\caption{Effect of $k$ in $k$-rank-reduced randomized SVD on the runtime in seconds.}
\label{fig:k-svd-vs-time}
\end{figure}


Randomized SVD is very fast, but the runtime does not grow linearly with $k$,
it looks quadratic (see fig.~\ref{fig:k-svd-vs-time}).
However, the typical values of $k$ for SVD used in latent semantic analysis is
150-250 \cite{aggarwal2012survey} \cite{evangelopoulos2012latent},
therefore the run time is not prohibitive, and we do not need to rut it
with very large $k$.

\begin{figure}[h!]
\centering
\hfill
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{k-vs-mb-svd-purity.pdf}
  \caption{Purity vs number of clusters $K$ in \mbox{$K$-Means} and MiniBatch $K$-Means.}
  \label{fig:k-vs-mb-svd-purity}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{k-vs-mb-svd-len.pdf}
  \caption{Number of pure clusters vs $K$ in $K$-Means and MiniBatch $K$-Means.}
  \label{fig:k-vs-mb-svd-len}
\end{subfigure}
\caption{The performance of $K$-Means and MiniBatch $K$-Means on the reduced document space with $k=150$.}
\label{fig:kmeans-vs-minibatch-svd}
\end{figure}


When the dimensionality is reduced, the performance of $K$-Means and
MiniBatch $K$-Means is similar (see fig.~\ref{fig:k-vs-mb-svd-purity}),
but with MiniBatch $K$-Means we were able to discover more interesting pure
clusters (see fig.~\ref{fig:k-vs-mb-svd-len}). The reason for this may be the fact that in the reduced space there is less noise and both methods find equally good clusters,
but because MiniBatch $K$-Means works faster, we are able to run it multiple
times thus increasing its chances to find a good local optimum where there
are many pure document clusters. Note that the obtained result is below
the baseline.

We can observe that as $K$ increases, the number of interesting clusters increases
(see fig.~\ref{fig:k-vs-mb-svd-len}).
Therefore, we try a wide range of larger $K$ for different $k \in \{150, 250, 350, 500\}$.
The performance in terms of discovered pure clusters does not depend much on the
rank $k$ of the reduced space (see fig.~\ref{fig:k-vs-kmeans-len-nodef}). In fact,
it is very hard to distinguish different lines because they are quite perplexed.
The maximum for is achieved at $K \approx 10\,000$ for all $k$.

\begin{figure}[h!]
\centering\includegraphics[width=0.9\textwidth]{k-vs-kmeans-len-nodef.pdf}
\caption{Number of discovered pure clusters in $K$-Means for different number of clusters $K$ and rank $k$.}
\label{fig:k-vs-kmeans-len-nodef}
\end{figure}


\begin{figure}[h!]
\centering
\includegraphics[width=0.6\textwidth]{nmf-runtime.pdf}
\caption{Runtime of NMF for different $k$.}
\label{fig:nmf-runtime}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{k-vs-kmeans-len-nmf-nodef.pdf}
\caption{Number of discovered pure clusters in $K$-Means and NMF for different number of clusters $K$ and rank $k$.}
\label{fig:k-vs-kmeans-len-nmf-nodef}
\end{figure}

We also can apply \textbf{Non-Negative Matrix Factorization} for LSA.
NMF takes significantly more time than randomized SVD (see fig.~\ref{fig:nmf-runtime}).
In addition, although the runtime should be $O(nk)$ \cite{xu2003document}, we do not
observe that it grows linearly with $k$. On the contrary, it appears that there is rather
quadratic relationship. We expected that the results produced by NMF will
be better than SVD because of non-negativity of produced results,
but the performance is quite similar (see fig.~\ref{fig:k-vs-kmeans-len-nmf-nodef}).
For NMF, however, it is easier to see the difference in performance when different rank $k$ is used, and the curves are not as perplexed as for SVD. We see that $k=250$ does better on
$K=[8000; 12000]$ than $k=150$ and $k=350$. For example, $K$-Means with
$K=9500$ and $k=250$ discovered a clustering with 200 namespace-defining clusters.

We can also observe that generally clustering works better on reduced spaces.


\begin{figure}[h!]
\centering\includegraphics[width=0.9\textwidth]{nodef-weighing.pdf}
\caption{The effect of using different weighting systems on $K$-Means with SVD.}
\label{fig:nodef-weighing}
\end{figure}


In the experiments above we used the $(\log \text{TF}) \times \text{IDF}$ weighting scheme.
Let us compare the effect of different weighting on the resulting clusters.
To do that, we apply SVD  with $k=150$ and run MiniBatch $K$-Means for a set of smaller $K$'s
because it is computationally faster. We can observe that performance of $K$-Means
does not depend significantly on the weighting system when no definitions
are used (see fig.~\ref{fig:nodef-weighing}).



\subsubsection{Weak Association} \ \\

The identifier-document matrix has the dimensionality $10419 \times 22512$,
and there are 485\,337 elements in the matrix, so the density is about 0.002.


\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{soft-dbscan-cos10.pdf}
  \caption{Number of clusters when 10 nearest neighbors are considered}
  \label{fig:soft-dbscan-cos10}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{soft-dbscan-cos10-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 10 nearest neighbors}
  \label{fig:soft-dbscan-cos10-2}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{soft-dbscan-cos13.pdf}
  \caption{Number of clusters when 15 nearest neighbors are considered}
  \label{fig:soft-dbscan-cos15}
\end{subfigure}%
\begin{subfigure}[b]{0.5\textwidth}
  \centering
  \includegraphics[width=\textwidth]{soft-dbscan-cos13-2.pdf}
  \caption{Performance of selected $\varepsilon$ with 15 nearest neighbors}
  \label{fig:soft-dbscan-cos15-2}
\end{subfigure}
\caption{Effect of parameters $\varepsilon$, \texttt{MinPts} and number of nearest
 neighbors on performance of SNN DBSCAN when cosine is used.}
\label{fig:soft-dbscan-cos}
\end{figure}

We do not attempt to use hierarchical methods and start with DBSCAN. Previously we have
observed that Jaccard is inferior to cosine similarity, and therefore we start directly
with cosine.

Like in no-definition case, we calculate the cosine similarity on document vectors
where elements are weighed with $(\log \text{TF}) \times \text{IDF}$. Using definitions
it gives better results, than just identifiers. For example,
for ``Linear regression'' the closest document is ``Linear predictor function'' with
cosine of 0.62, which is the same result, obtained when no definitions are used.
However, for ``Singular value decomposition'' the most similar document is
``Moore–Penrose pseudoinverse'' with cosine score of 0.386, and this is more meaningful
than the most similar document when no definitions are used.
As previously, we applied \textbf{SNN DBSCAN} with 10 and 15 nearest neighbors, and the best
result was obtained with 10 nearest neighbors, $\varepsilon=3$ and \texttt{MinPts}$=3$
(see fig.~\ref{fig:soft-dbscan-cos}). It was able to discover 223 namespace-defining
clusters, which is slightly better than the best case when no definitions are
used.

With  \textbf{MiniBatch $K$-Means} applied on the plain untransformed document
space we are able to find some interesting clusters, but it general, similarity to the no-definition case, the does not show good results overall. Therefore we
apply it to the LSA space reduced by \textbf{SVD}, when identifier-document matrix
is reduced to rank $k$. We search for
the best combination trying $K = [500; 15000]$ and $k \in \{150, 250, 350\}$.
Unlike the case where no definitions are used, the space produced by the
soft definition association is affected by $k$ (see fig.~\ref{fig:k-vs-kmeans-len-svd-soft})
and the results produced by $k = 350$ are almost always better.
The weighing scheme used for this experiment is $(\log \text{TF}) \times \text{IDF}$.


\begin{figure}[h!]
\centering\includegraphics[width=0.9\textwidth]{k-vs-kmeans-len-svd-soft.pdf}
\caption{Number of discovered pure clusters in $K$-Means and SVD for different number of clusters $K$ and rank $k$.}
\label{fig:k-vs-kmeans-len-svd-soft}
\end{figure}



\begin{figure}[h!]
\centering\includegraphics[width=0.9\textwidth]{k-vs-kmeans-len-nmf-soft.pdf}
\caption{The effect of rank $k$ used in NMF on $K$-Means.}
\label{fig:nnmf-soft}
\end{figure}

\textbf{Non-Negative Matrix Factorization} gives good results, but does not improve on
the best result obtained with SVD (see fig.~\ref{fig:nnmf-soft}).
The largest number of namespace-defining clusters is 370 and it is achieved
with $K=10000$ and $k=350$.


\begin{figure}[h!]
\centering\includegraphics[width=0.9\textwidth]{soft-weighing.pdf}
\caption{The effect of using different weighting systems on $K$-Means with SVD
($k=150$).}
\label{fig:soft-weighing}
\end{figure}


We also experiment with different weighting schemes, and, unlike the no-definition
case, it has a significant effect on the results: we can observe that sublinear
TF is better that untransformed TF, and $(\log \text{TF}) \times \text{IDF}$
achieves the best performance (see fig.~\ref{fig:soft-weighing}).




\subsubsection{Strong Association} \ \\

In the case when we use the strong association, the identifier-document 
matrix has the dimensionality of $37879 \times 22512$ identifier-document matrix.
It has 499070 entries, so the density of this matrix is just 0.00058.

Like for the soft association case, we choose not to perform usual $K$-Means,
and instead proceed directly to \textbf{MiniBatch $K$-Means} on the LSA space
reduced with \textbf{SVD}. With rank $k=500$ and number of clusters $K = 8250$
is achieves the best result of 340 clusters (see fig.~\ref{fig:k-vs-kmeans-len-svd-strong}),
which is slightly worse than in the weak association case. The purity
of obtained clustering is 0.5683.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{k-vs-kmeans-len-svd-strong.pdf}
\caption{Number of discovered pure clusters in $K$-Means and SVD for different number of clusters $K$ and rank $k$.}
\label{fig:k-vs-kmeans-len-svd-strong}
\end{figure}


We do not attempt to perform \textbf{Non-Negative Matrix Factorization} as we have previously
established that it usually does not give better results while taking significantly
longer time.


\subsubsection{Russian Wikipedia}  \ \\

Based on the experiments we have performed on English Wikipedia, we see that the best
performance is obtained with weak association by using MiniBatch $K$-Means on
LSA space reduced by SVD.
We apply the same best performing technique on Russian Wikipedia.

The identifier-document matrix has the dimensionality of $3948 \times 5319$
with 79171 non-zero elements, so the density of this matrix is 0.0038.


\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{k-vs-kmeans-rus.pdf}
\caption{Performace of $K$-Means with SVD for Russian Wikipedia.}
\label{fig:k-vs-kmeans-rus.pdf}
\end{figure}

As usually, we applied SVD with different values of rank $k \in \{150, 250, 350\}$,
and, similarity to no-definitions case for English Wikipedia, we do not
observe significant differences across different values of $k$
(see fig.~\ref{fig:k-vs-kmeans-rus.pdf}). The best achieved result is 
105 namespace-defining clusters. 


\subsection{Result Analysis} \label{sec:result-analysis}

\subsubsection{English Wikipedia} \ \\


\textbf{TODO: rewrite slightly}


The best performing method is $K$-Means, and the best way to include 
definitions into Identifier Space is by using the soft association. 


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  Name & Size & Purity \\
  \hline
Astronomical catalogues & 53 & 0.9811 \\
Statistics & 20 & 0.8500 \\
Category theory & 16 & 0.8125 \\
Electromagnetism & 12 & 0.8333 \\
Thermodynamics & 11 & 0.8182 \\
Mathematical analysis & 11 & 0.8182 \\
Graph theory & 10 & 0.9000 \\
Graph algorithms & 10 & 0.8000 \\
Fluid dynamics & 10 & 1.0000 \\
Numerical analysis & 9 & 0.8889 \\
Group theory & 9 & 1.0000 \\
Stochastic processes & 9 & 1.0000 \\
Measure theory & 8 & 1.0000 \\
\hline
\end{tabular}
\caption{Top namespace-defining clusters.}
\label{tab:soft-kmeans-svd}
\end{table}

\begin{table}[h!]
\centering
\begin{subtable}{0.7\textwidth}
\centering
\begin{tabular}{|c|c|}
  \hline
  Article & Identifiers \\
  \hline
Diagonalizable matrix & $v_1,\lambda_1,v_k,\lambda_3,\lambda_2,\lambda_i,\lambda_k,\lambda_j,\lambda_n,...$\\
Eigenvalues and eigenvectors & $v_i,\mu_A,\lambda_i,d,\lambda_n,...$ \\
Principal axis theorem & $v_1,u_1,\lambda_1,\lambda_2,D,S,u,...$ \\
Eigendecomposition of a matrix & $\lambda,\lambda_1,\lambda,\lambda_2,\lambda_k,R,U,T,...$ \\
Min-max theorem & $\sigma,u_n,u_k,u_i,u_1,\alpha,\lambda_1,\lambda,\lambda_i,...$ \\
Linear independence & $\Lambda,v_j,u_2,v_3,u_n,\lambda_1,\lambda_3,\lambda_2,...$ \\
Symmetric matrix & $\Lambda,\lambda_1,\lambda_2,D,Q,P,\lambda_i,...$ \\
\hline
\end{tabular}
\caption{Wiki Articles in the cluster ``Linear Algebra''}
\label{tab:soft-kmeans-la}
\end{subtable}% mask EOL
\begin{subtable}{0.3\textwidth}
\centering
\begin{tabular}{|c|c|c|}
\hline
ID & Definition & Score\\
\hline
$D$ & diagonal matrix & 0.72 \\
$t$ & real argument & 0.46 \\
$u$ & eigenvalues & 0.42 \\
$u_i$ & eigenvector & 0.42 \\
$v_1$ & eigenvectors & 0.73 \\
$\Lambda$ & diagonal matrix & 0.87 \\
$\lambda$ & eigenvalue & 0.4  \\
$\lambda_1$ & eigenvalues & 0.95 \\
$\lambda_2$ & eigenvalues & 0.71 \\
$\lambda_3$ & eigenvalues & 0.39 \\
$\lambda_i$ & eigenvalue & 0.98 \\
\hline
\end{tabular}
\caption{Definitions in ``Linear Algebra''}
\label{tab:soft-kmeans-la-def}
\end{subtable}
\caption{A ``Linear Algebra'' cluster.}
\label{tab:soft-kmeans-lsa}
\end{table}

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
  \hline
  \multicolumn{4}{|c|}{$\lambda$}\\
  \hline
  Size & Namespace Name & Definition & Score \\
  \hline
3 & Algebra & multiplicity & 0.43 \\
4 & Analysis of variance & marquardt & 0.69 \\
3 & Applied and interdisciplinary physics & wavelength & 0.98 \\
6 & Cartographic projections & longitude & 1.00 \\
3 & Cartography & longitude & 1.00 \\
3 & Category theory & natural isomorphisms & 0.40 \\
4 & Condensed matter physics & penetration depth & 0.44 \\
5 & Continuous distributions & affine parameter & 0.46 \\
3 & Coordinate systems & longitude & 0.88 \\
3 & Differential equations & differential operator & 0.42 \\
8 & Differential geometry & vector fields & 0.72 \\
7 & Electronic amplifiers & typical value & 0.43 \\
3 & Electrostatics & unit length & 0.43 \\
10 & Fluid dynamics & wavelength & 1.00 \\
6 & Fluid dynamics & free path & 0.43 \\
3 & Infinity & limit ordinals & 0.87 \\
7 & Linear algebra & eigenvalue & 0.4  \\
5 & Linear algebra & matrix & 0.41 \\
3 & Linear algebra & eigenvalue & 0.85 \\
3 & Liquids & relaxation time & 0.41 \\
3 & Materials science & rate & 0.44 \\
3 & Mathematical analysis & eigenvalue & 0.41 \\
3 & Mathematical theorems & poisson distribution & 0.41 \\
4 & Measure theory & lebesgue measure & 0.44 \\
3 & Measurement & order & 0.42 \\
8 & Mechanics & previous expression & 0.44 \\
4 & Mechanics & power series & 0.41 \\
3 & Metalogic & empty word & 0.45 \\
7 & Number theory & partition & 0.74 \\
4 & Number theory & modular lambda function & 0.46 \\
3 & Operator theory & algebraic multiplicity & 0.44 \\
5 & Optics & wavelength & 0.71 \\
5 & Partial differential equations & constants & 0.41 \\
4 & Physical optics & wavelength & 0.95 \\
5 & Physics & exciton state & 0.88 \\
6 & Probability distributions & references & 0.42 \\
4 & Quantum field theory & coupling constant & 0.75 \\
5 & Quantum mechanics & wavelength & 1.00 \\
5 & Quantum mechanics & state & 0.87 \\
3 & Radioactivity & decay & 0.72 \\
4 & Representation theory of Lie groups & weight & 1.00 \\
3 & Riemannian geometry & contravariant vector field & 0.45 \\
4 & Rubber properties & engineering strain & 1.00 \\
3 & Statistical data types & regularization parameter & 0.45 \\
20 & Statistics & words & 0.43 \\
3 & Statistics & expectation & 0.46 \\
3 & Stellar astronomy & mean free path & 0.43 \\
3 & Surface chemistry & ideal gas & 0.39 \\
3 & Theoretical physics & eigenvalue & 0.88 \\
5 & Theories of gravitation & dicke & 0.44 \\
3 & Wave mechanics & wavelength & 0.8 \\
\hline
\end{tabular}
\caption{Some of definitions of ``$\lambda$''.}
\label{tab:soft-kmeans-lambda}
\end{table}

The best result is 414 namespace-defining clusters, it is ten times better
than the baseline result, and it is achieved by $K$-Means with
soft association using parameters $K=9750$ and $k=350$. The purity of this 
clustering is $0.63$. The largest namespace-defining clusters discovered 
by this methods are presented in the table~\ref{tab:soft-kmeans-svd}.

Let us consider a ``Linear Algebra'' cluster (table~\ref{tab:soft-kmeans-lsa})
with 6 documents and some of extracted definitions in documents
of this cluster, and all these articles share identifers $\lambda_1$, $m$ and $n$.
Let us consider all definitions of identifier ``$\lambda$''. In total, there
are 93 clusters where ``$\lambda$'' is used (see table~\ref{tab:soft-kmeans-lambda}),
and in many cases it is possible to determine that the assignment is correct
(e.g. ``eigenvalue'', ``wavelength'', ``regularization parameter'').
Some cases are not correct, for example, when we have clusters with the same name
where $\lambda$ denotes different things (e.g. in two ``Quantum Mechanics'' clusters),
or in the case of ``Linear Algebra'' cluster where it denotes a matrix.


Clustering results with sort association are better than results obtained
with hard association. One of the reasons for that can the the fact that
definitions may act as keywords that describe the document and they are
better in describing the semantic content of the document. 


Additionally, we see that clustering on the reduced space works better, 
and in our case the best dimensionality reduction method is SVD.

We also note that we do not discover many namespace-defining clusters. The 
best result identifiers 414 clusters, while the desired number of clusters $K$
is almost 10\,000. It means that information from about 9\,000 clusters is 
discarded: in total there are 22\,512 documents, but identifiers from only 1\,773 
are used, and the rest of the document (about 92\%) are not utilized at all.


\subsubsection{Russian Wikipedia} \ \\

For Russian Wikipedia, the best results is 105 namespace-defining clusters 
with overall purity of 0.73. It was obtained by $K=3000$ and $k = 250$. 
The largest namespace-defining clusters
are shown in table~\ref{tab:rus-wiki-categories}. Interestingly, there is a cluster
``Животные'' (``Animals'') where mathematical formulae are used to describe ``tooth formula''.
Let us consider a cluster about Linear Algebra (see table~\ref{tab:rus-la})
and definitions extracted from it (see table~\ref{tab:rus-la}). Similarity to English wikipedia,
some of the definitions are correct and valid, for example, ``сингулярный число''
(``singular value'') for ``$\sigma$''  or ``ранг'' (``rank'') for ``$r$'', while some
are not quite correct, e.g. ``скаляр'' (``scalar'') for ``$\lambda$''.
Additionally, some of the non-valid definitions seem to result from misclassifications
by the POS-tagger, for example, ``$\Sigma$'' is defined as ``вдоль'' (literally ``along''),
which do not make much sense.
Additionally we can look at all definitions of ``$\lambda$'' across all discovered namespaces
(see table~\ref{tab:rus-lambda}). The scores of namespace relations extracted from Russian 
wikipedia are generally smaller than in English, but it is most likely because 
there are fewer relations in the Russian dataset.

For the Russian part of Wikipedia, we also utilize just 382 documents, which is only 7\%, 
and the rest of the documents (93\%) are filtered out.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
  \hline
  Article (Original name) & Article (English) & Size & Purity \\
  \hline
Общая алгебра & Algebra & 7 & 0.8571 \\
Диф. геометрия и топология & Differential geometry and topology & 7 & 1.0000 \\
Диф. геометрия и топология & Differential geometry and topology & 6 & 0.8333 \\
Функциональный анализ &  Functional analysis & 6 & 0.8333 \\
Животные & Animals & 6 & 0.8333 \\
Картография & Cartography & 6 & 1.0000 \\
Математический анализ & Mathematical analysis & 5 & 1.0000 \\
Математический анализ & Mathematical analysis & 5 & 0.8000 \\
Теория вероятностей & Probability theory & 5 & 1.0000 \\
Механика & Mechanics & 5 & 0.8000 \\
Диф. уравнения в частных производных & Partial differential equations & 5 & 0.8000 \\
Математический анализ & Mathematical analysis & 5 & 0.8000 \\
Релятивистские и гравитационные явления & Relativity and gravitation & 5 & 0.8000 \\
Линейная алгебра & Linear algebra & 5 & 1.0000 \\
Математический анализ & Mathematical analysis & 5 & 1.0000 \\
Физика твёрдого тела & Solid-state physics & 5 & 0.8000 \\
\hline
\end{tabular}
\caption{Largest namespace-defining clusters extracted from Russian Wikipedia.}
\label{tab:rus-wiki-categories}
\end{table}

\begin{table}[h!]
\centering
\begin{subtable}{\textwidth}
\centering
\begin{tabular}{|c|c|c|}
  \hline
  Article (Original name) & Article (English) & Identifiers \\
  \hline
Линейная алгебра & Linear algebra &  $V, v_n, \alpha, v_1, v_2, x, \alpha_1, \beta, \lambda, f, U,...$ \\
Спектральная теорема & Spectral theorem & $\Lambda, \lambda, H, K, P, U, T, V, X, f, y, x,...$ \\
Сингулярное разложение & Singular value decomposition & $\Sigma, V_k, \sigma, M, U, T, V, k, r, u, v,...$ \\
\begin{tabular}[x]{@{}c@{}} Ковариантность и \\ контравариантность \end{tabular}
 & Covariance and contra-variance & $S, V, d, g, f, k, m, f_i, n, u, v, x,...$ \\
Теорема Куранта -- Фишера & Courant–Fischer theorem & $k, V, S, L_k,...$ \\
\hline
\end{tabular}
\caption{A namespace-defining cluster about Linear algebra.}
\label{tab:rus-la}
\end{subtable}

\begin{subtable}{\textwidth}
\centering
\begin{tabular}{|c|c|c|c|}
  \hline
  ID & Definition & Definition (English) & Score \\
  \hline
$H$ & гильбертов & hilbert & 0.44 \\
$L$ & отображение & transformation & 0.95 \\
$L_k$ & оператор & operator & 0.41 \\
$M$ & воздействие матрица & matrix & 0.71 \\
$T$ & линейный отображение & linear transformation & 0.43 \\
$U$ & унитарный матрица & unitary matrix & 0.71 \\
$V$ & пространство & space  & 0.99 \\
$g$ & билинейный форма & bilinear form & 0.71 \\
$r$ & ранг & rank & 0.46 \\
$x$ & собственный вектор & eigenvector & 0.44 \\
$\Lambda$ & диагональный матрица & diagonal matrix & 0.88 \\
$\Sigma$ & вдоль & ``along'' & 0.44 \\
$\lambda$ & скаляр & scalar & 0.46 \\
$\sigma$ & сингулярный число & singular value & 0.45 \\
\hline
\end{tabular}
\caption{Definitions in the ``Linear Algebra'' namespace.}
\label{tab:rus-la-def}
\end{subtable}

\begin{subtable}{\textwidth}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
  \multicolumn{6}{|c|}{$\lambda$}\\
  \hline
  Size & Original name & English name & Original definition & English definition & Score \\
  \hline
3 & Алгебра & Algebra & поль & field & 0.74 \\
5 & Гидродинамика & Fluid dynamics & тепловой движение & thermal motion & 0.42 \\
4 & Гравитация & Gravitation & коэф. затухание & damping coefficient & 0.46 \\
6 & Картография & Cartography & долгота & longitude & 0.98 \\
5 & Линейная алгебра & Linear algebra & скаляр & scalar & 0.46 \\
4 & Оптика & Optics & длина & length & 0.88 \\
3 & Оптика & Optics & длина волна & wavelength & 0.44 \\
5 & \begin{tabular}[x]{@{}c@{}} Релятивистские и \\ гравитационные явления \end{tabular} & \begin{tabular}[x]{@{}c@{}} Relativity and \\ gravitation \end{tabular} & частота & frequency & 0.42 \\
3 & Статистическая физика & Statistical physics & итоговый выражение & final expression & 0.42 \\
3 & \begin{tabular}[x]{@{}c@{}} Теоремы \\ комплексного анализа \end{tabular}  & \begin{tabular}[x]{@{}c@{}} Theorems of \\ complex analysis \end{tabular}  & нуль порядок & ``zero order'' & 0.45 \\
3 & Теория алгоритмов & Algorithms & функция переход & transition function & 0.89 \\
5 & Физические науки & Physical sciences & длина & length & 0.43 \\
  \hline
\end{tabular}
\caption{Definitions of ``$\lambda$'' across all namespaces.}
\label{tab:rus-lambda}
\end{subtable}
\caption{Namespaces extracted from Russian wikipedia.}
\label{tab:rus-namespaces}
\end{table}




\subsection{Building Hierarchy} \label{sec:hierarchy}


After the namespaces are found, we need to organize them into a hierarchical
structure. It is hard to do automatically, and we choose to use
existing hierarchies for mathematical knowledge, and then map the
found namespaces to these hierarchies.

The first hierarchy that we use is ``Mathematics Subject Classification'' (MSC)
hierarchy \cite{ams2010msc} by the American Mathematical Society, and it
is used for categorizing mathematical articles. In this scheme there are
64 top-level categories such as ``Mathematical logic'', ``Number theory'',
or ``Fourier analysis''. It also includes some physics categories such
as ``Fluid mechanics'' or ``Quantum Theory''. The following top level
categories are excluded: ``General'', ``History and biography'' and
``Mathematics education''.

Each top-level category contains second-level categories and third-level
categories. In this work we exclude all subcategories those code
ends with 99: they are usually ``Miscellaneous topics'' or
``None of the above, but in this section''.

Additionally, we excluded the following second level categories because
they interfere with PACS, a hierarchy for Physics:

\begin{itemize}
\item Quantum theory $\to$ Axiomatics, foundations, philosophy
\item Quantum theory $\to$ Applications to specific physical systems
\item Quantum theory $\to$ Groups and algebras in quantum theory
\item Partial differential equations $\to$ Equations of mathematical physics and other areas of application
\end{itemize}


% \begin{itemize}
% \item Statistics $\to$ Sufficiency and information
% \item Functional analysis $\to$ Other (nonclassical) types of functional analysis
% \item Functional analysis $\to$ Miscellaneous applications of functional analysis
%\end{itemize}

The second hierarchy is ``Physics and Astronomy Classification Scheme'' (PACS)
\cite{aps2010pacs}, which is a scheme for categorizing articles about Physics.
Like in MSC, we remove the top-level category  ``GENERAL''.

Finally, we also use the ACM Classification Scheme \cite{rous2012acm}
available as a SKOS \cite{miles2005skos} ontology at their website \cite{amc2012ccs}.
The SKOS ontology graph was processed with RDFLib \cite{rdflib}.
We use the following top level categories:
``Hardware'', ``Computer systems organization'', ``Networks'',
``Software and its engineering'', ``Theory of computation'',
``Information systems'', ``Security and privacy'',
``Human-centered computing'', ``Computing methodologies''.

After obtaining and processing the data, the three hierarchies
are merged into one.


However these categories are only good for English articles and
a different hierarchy is needed for Russian. One of such hierarchies is
``Госу\-дар\-ствен\-ный руб\-ри\-ка\-тор научно-тех\-ни\-чес\-кой инфор\-ма\-ции''
(ГРНТИ)~-- ``State categorizator of scientific and technical information'', which
is a state-recommended scheme for categorizing scientific articles published
in Russian  \cite{feodosimov2000grnti}. The hierarchy  is extracted from the
official website\footnote{\url{http://grnti.ru/}}. It provides
a very general categorization and therefore we keep only the following math-related
categories: ``Астрономия'' (``Astronomy''), ``Биология'' (``Biology''),
``Информатика'' (``Informatics''), ``Математика'' (``Mathematics''),
``Механика'' (``Mechanics''), ``Ста\-тис\-тика'' (``Statistics''),
``Физика'' (``Physics''), ``Химия'' (``Chemistry''),
``Экономика. Экономические Науки'' (``Economics'') and others.


Once the hierarchy is established, each found namespace is mapped to
the most suitable second-level category. This is done by keywords matching.
First, we extract all key words from the category, which includes
top level category name, subcategory name and all third level categories.
Then we also extract the category information from the namespace, but
we also use the names of the articles that form the namespace.
Finally, the keyword matching is done by using the cosine similarity
between the cluster and each category. The namespace is assigned to the
category with the best (largest) cosine score.

If the cosine score is low (below $0.2$) or there is only one
keyword matched, then the cluster is assigned to the ``OTHERS''
category.

For example, consider a namespace  derived from the cluster consisting of
``Tautology (logic)'', ``List of logic systems'', ``Regular modal logic''
``Combinational logic'' documents. Among others, these articles belong to categories
``Mathematical logic'' and ``Logic''. Then the following is the list of keywords
extracted from the cluster:
``tautology'', ``logic'', ``list'', ``systems'', ``regular'', ``modal'', ``combinational'',
``logical'', ``expressions'', ``formal'', ``propositional'', ``calculus'' and so on.
Apparently, this namespace is about mathematical logic.

Then consider a list of keywords for ``'General logic'', a subcategory of
``Mathematical logic and foundations'' from MSC:
``mathematical'', ``logic'', ``foundations'', ``general'', ``classical'', ``propositional'', ``type'', ``subsystems'' and others.

These keywords are represented as vectors in a vector space and the cosine score
between these vectors is calculated. For this example, the cosine is
approximately 0.75, and this is the largest similarity, and therefore this namespace
is mapped to the ``General logic'' subcategory.

\ \\

For example, let us consider the namespaces discovered from English Wikipedia.

The majority of namespaces are mapped correctly. For example:

\begin{small}
\begin{itemize}
  \item ATOMIC AND MOLECULAR PHYSICS
    \begin{itemize}
      \item Atomic properties and interactions with photons (wiki: Atomic physics; Quantum mechanics; Atomic, molecular, and optical physics)
    \end{itemize}

  \item Algebraic geometry
    \begin{itemize}
      \item Computational aspects in algebraic geometry (wiki: Sheaf theory, Theorems in geometry, Theorems in algebraic geometry
      \item Computational aspects in algebraic geometry (wiki: Algebraic geometry, Algebraic varieties, Manifolds)
      \item Surfaces and higher-dimensional varieties (wiki: Algebraic varieties, Threefolds, Surfaces
    \end{itemize}

  \item Algebraic topology
    \begin{itemize}
      \item Fiber spaces and bundles (wiki: Differential geometry, Fiber bundles, Topology)
      \item Spectral sequences (wiki: Abstract algebra, Homological algebra, Algebraic topology)
      \item Applied homological algebra and category theory (wiki: Continuous mappings, Algebraic topology, Homotopy theory)
    \end{itemize}

  \item Biology and other natural sciences
    \begin{itemize}
      \item Mathematical biology in general (wiki: Evidence-based practices, Public health, Epidemiology)
      \item Genetics and population dynamics (wiki: Population genetics, Genetics, Subfields and areas of study related to evolutionary biology)
    \end{itemize}

  \item  Computing methodologies
    \begin{itemize}
      \item Machine learning (wiki: Machine learning, Learning, Artificial intelligence)
      \item Machine learning (wiki: Statistical data types, Multivariate statistics, Statistical classification)
    \end{itemize}

  \item Information systems
    \begin{itemize}
      \item Data management systems (wiki: Databases, Information technology management, Computer data)
    \end{itemize}
\end{itemize}
\end{small}


Some of the mapped namespaces are partially accurate:

\begin{small}
\begin{itemize}
  \item Computer systems organization
    \begin{itemize}
      \item Real-time systems (wiki: Computer languages, Type systems, Data types;
            matched keywords: languages computer systems architecture)
    \end{itemize}

\item  Fluid mechanics
    \begin{itemize}
      \item Biological fluid mechanics (wiki: Fluid mechanics, Soft matter, Gases;
             matched keywords: mechanics fluid)
      \item Biological fluid mechanics
             (wiki: Fluid dynamics, Fluid mechanics, Computational fluid dynamics;
             matched keywords: mechanics fluid)
    \end{itemize}
\item Functional analysis
    \begin{itemize}
      \item Distributions, generalized functions, distribution spaces
             (wiki: Probability distributions, Exponential family distributions, Continuous distributions;
             matched keywords: analytic distribution distributions generalized)
    \end{itemize}
\item $K$-theory
    \begin{itemize}
      \item Whitehead groups and $K_1$
             (wiki: Group theory, Subgroup properties, Metric geometry;
             matched keywords: group subgroup)
    \end{itemize}

\item Partial differential equations
    \begin{itemize}
      \item Close-to-elliptic equations (wiki: Differential equations, Numerical analysis, Numerical differential equations;
             matched keywords: partial differential equations)
    \end{itemize}
\end{itemize}
\end{small}


Finally, namespaces marked as ``OTHER'' are usually matched incorrectly:

\begin{small}
\begin{itemize}
  \item OTHER
    \begin{itemize}
   \item Randomness, geometry and discrete structures
             (wiki: Coordinate systems, Cartography, Cartographic projections;
             matched keywords: projections)
   \item Other generalizations
             (wiki: Electrostatics, Concepts in physics, Electromagnetism;
             matched keywords: potential)
   \item Computational methods
             (wiki: General relativity, Exact solutions in general relativity, Equations;
             matched keywords: relativity)
   \item Other classes of algebras
             (wiki: International sailing classes, Keelboats, Olympic sailboat classes;
             matched keywords: classes)
  \end{itemize}
\end{itemize}
\end{small}

\ \\

For the Russian version of Wikipedia, the majority of namespaces are also mapped correctly. 
For example: 

\begin{small}
\begin{itemize}

\item АСТРОНОМИЯ (ASTRONOMY)
    \begin{itemize}
      \item Звезды (Stars)
           (wiki: Физические науки (Physical sciences), Астрофизика (Astrophysics), Астрономия (Astronomy))
    \end{itemize}

\item ГЕОФИЗИКА (GEOPHYSICS)
    \begin{itemize}
      \item Океанология (Oceanology) (wiki: Океанология (Oceanology), Физическая география (Physical geography), Гидрология (Hydrology))
    \end{itemize}

\item КИБЕРНЕТИКА (CYBERNETICS)
    \begin{itemize}
      \item Теория информации (Information theory) (wiki: Цифровые системы (Digital systems), Теория информации (Information theory), Теория кодирования (Coding theory))
      \item Теория конечных автоматов и формальных языков (Finite state automata and formal languages) (wiki: Теория алгоритмов (Algorithmic theory), Теория автоматов (Automata theory), Визуализация данных (Data visualization))
    \end{itemize}

\item МАТЕМАТИКА (MATHEMATICS)
    \begin{itemize}
      \item Математический анализ (Mathematical analysis) (wiki: Математический анализ (Mathematical analysis), Разделы математики (Parts of mathematics), Функциональный анализ (Functional analysis))
      \item Теория вероятностей и математическая статистика (Probability and statistics) (wiki: Теория вероятностей (Probability), Теория вероятностей и математическая статистика (Probability and statistics), Теория меры (Measure theory))
      \item Основания математики и математическая логика (Foundation of mathematics and mathematica logics) (wiki: Логика (Logic), Разделы математики (Parts of mathematics), Математика (Mathematics))
      \item Алгебра (Algebra) (wiki: Теория колец (Rings Theory), Теория полей (Fields Theory), Теория групп (Groups Theory))
    \end{itemize}

\item МЕТРОЛОГИЯ (METROLOGY)
    \begin{itemize}
      \item Измерения отдельных величин и характеристик (Measurements of individual values and characteristics) (wiki: Единицы измерения (Units of measure), Системы мер (Measure systems), Макс Планк (Max Plank))
    \end{itemize}

\item ФИЗИКА (PHYSICS)
    \begin{itemize}
      \item Физика элементарных частиц. Теория полей. Физика высоких энергий (Particle physics. Field theory. High-energy physics) (wiki: Гравитация (Gravity), Классическая механика (Classical mechanics), Классическая физика (Classical physics))
      \item Физика твердых тел (Physics of solids) (wiki: Физика конденсированного состояния (Condensed matter physics), Кристаллография (Crystallography), Физика твёрдого тела (Physics of solids))
      \item Оптика (Optics) (wiki: Оптика (Optics), Физические науки (Physical sciences), Методы экспериментальной физики (Methods of experimental physics))
    \end{itemize}

\end{itemize}
\end{small}



\subsection{Evaluation Summary} \label{sec:evaluation-summary}


The best definition embedding technique is soft association.
The best clustering algorithm is $K$-Means with $K=9500$
on the semantic space produced by rank-reduced SVD with $k = 250$
with TF-IDF weight where TF is sublinear.

To visualize the discovered namespaces, we first map them to a hierarchy
as previously described in section~\ref{sec:hierarchy}, and then find the
most frequent categories according to this hierarchy. Then, for each
category, we rank all discovered identifier-definitions pairs, and show
only the most highly ranked ones
(see table~\ref{tab:top-namespaces}). Additionally, we show the most
frequent Wikipedia categories that the documents inside the namespaces
have, and also the most influential documents: the ones that contain
more identifiers than others. We also repeat the same for Russian Wikipedia
(see table~\ref{tab:top-namespaces-rus}).

We can note that the top discovered namespaces are quite different
across the two datasets, and there is no single namespace among the top namespaces
that both datasets share. However, ``Group theory and generalizations''
and ``Алгебра'' (``Algebra'') look a little similar and share a few identifiers.
It is probably due to the fact that the datasets may be different in the content
and have different distribution of categories. Also, the hierarchies matter
as well: in case of Russian, the hierarchy is more general, and therefore
the matched categories tend to be more general as well.




\begin{table}
\makebox[\textwidth][c]{\begin{tabular}{|c|x{3cm}|x{4cm}|x{3.5cm}|x{4cm}|}
		\hline							
Freq.	&	Namespaces	&	Definitions	&	Categories	&	Top Articles	\\
		\hline							
10	&	Fluid mechanics	&	$\rho$: density,
$p$: pressure,
$g$: acceleration,
$k$: wavenumber,
$u$: velocity,
$v$: velocity,
$\eta$: free surface,
$\omega$: angular frequency,
$z$: free surface,
$\nu$: kinematic viscosity	&	Fluid dynamics;
Fluid mechanics;
Dynamics;
Aerodynamics;
Partial differential equations	&	Navier–Stokes equations;
Stokes wave;
Airy wave theory;
Mild-slope equation;
Classic energy problem in open-channel flow	\\
		\hline							
9	&	Differential and difference algebra	&	$R$: ring,
$k$: field,
$D$: city,
$K$: field,
$x$: polynomials,
$\Gamma$: value group,
$M$: submodule,
$n$: matrix,
$S$: ring,
$v$: valuation	&	Abstract algebra;
Algebra;
Functions and mappings;
Polynomials;
Analytic functions	&	Recurrence relation;
Spectral radius;
Levi-Civita symbol;
Perron–Frobenius theorem;
Leibniz formula for determinants	\\
		\hline							
9	&	Partial differential equations	&	$\Omega$: domain,
$t$: time,
$L$: space,
$p$: space,
$\omega$: angular frequency,
$V$: hilbert space,
$D$: domain,
$u$: horizontal velocity,
$x$: time,
$U$: velocity profiles	&	Partial differential equations;
Differential equations;
Multivariable calculus;
Mathematical analysis;
Fluid dynamics	&	Orr–Sommerfeld equation;
Helmholtz equation;
Fictitious domain method;
Green's function for the three-variable Laplace equation;
Eikonal equation	\\
		\hline							
8	&	Mathematical economics	&	$K$: strike,
$P$: price level,
$p$: probability,
$u$: utility,
$V$: money,
$M$: money,
$\alpha$: confidence level,
$g$: distortion function,
$\sigma$: volatility,
$R_f$: return	&	Economics;
Financial economics;
Microeconomics;
Mathematical finance;
Economic theories	&	Modern portfolio theory;
Lookback option;
Binary option;
Equation of exchange;
Slutsky equation	\\
		\hline							
8	&	Foundations of probability theory	&	$X$: process,
$t$: time,
$X_t$: process,
$P$: probability measure,
$S$: state space,
$s$: stochastic processe,
$f$: measurable function,
$M$: local martingale,
$M_t$: local martingale,
$W_t$: process	&	Stochastic processes;
Probability theory;
Statistics;
Statistical data types;
Measure theory	&	Wiener process;
It\={o} calculus;
Local martingale;
Stratonovich integral;
Glivenko–Cantelli theorem	\\
		\hline							
7	&	Group theory and generalizations	&	$G$: group,
$H$: subgroup,
$Z$: group,
$K$: subgroup,
$N$: group,
$p$: power,
$n$: root,
$F$: free group,
$Q$: extension,
$T$: homomorphism	&	Group theory;
Abstract algebra;
Metric geometry;
Algebraic structures;
Theorems in group theory	&	Free abelian group;
Center (group theory);
Holomorph (mathematics);
$P$-group;
Powerful $p$-group	\\
		\hline							
7	&	General logic	&	$w$: world,
$R$: binary relation,
$P$: predicate,
$Q$: statement,
$W$: nodes,
$\phi$: modal formula,
$n$: natural number,
$v$: world,
$T$: relation,
$k$: degree	&	Mathematical logic;
Logic;
Proof theory;
Syntax (logic);
Formal systems	&	Sequent calculus;
First-order logic;
Original proof of G\"odel's completeness theorem;
Kripke semantics;
Szpilrajn extension theorem	\\
		\hline							
\end{tabular}}
\caption{Most frequent definitions in most frequent namespaces extracted from
English Wikipedia.}
\label{tab:top-namespaces}
\end{table}



\newgeometry{bottom=2cm}
\begin{table}
\makebox[\textwidth][c]{\begin{tabular}{|c|x{3cm}|x{5.2cm}|x{4.2cm}|x{4cm}|}
		\hline							
Freq.	&	Namespaces	&	Definitions	&	Categories	&	Top Articles	\\
		\hline							
10	&	Алгебра (Algebra)	&	$G$: группа (group),
$V$: пространство (space),
$p$: простой число (prime number),
$K$: подпространство (subspace),
$R$: кольцо (ring),
$C$: категория (category),
$n$: ``o ( n )'',
$F$: поль (field),
$L$: линейный пространство (linear space),
$D$: категория (category)	&	Действие группы (Group action),
Линейная алгебра (Linear algebra),
Векторное пространство (Vector space),
Алгебра (Algebra),
Нормированное пространств (Normed vector space)	&	Общая алгебра (General algebra),
Алгебра (algebra),
Теория групп (Group theory),
Линейная алгебра (Linear algebra),
Теория колец (Ring theory)	\\
\hline									
7	&	Топология (Topology)	&	$M$: многообразие (manifold),
$X$: топологический пространство (topologic space),
$n$: многообразие (manifold),
$U$: окрестность (neighborhood),
$C$: класс (class),
$S$: пучок (sheaf),
$x$: пространство (space),
$k$: ранг (rank),
$V$: слой (layer),
$f$: степень отображение (degree of mapping)	&	Топология (Topology);
Дифференциальная геометрия и топология (Differential geometry and topology);
Геометрия (Geometry);
Общая топология (General topology);
Общая алгебра (General algebra)	&	Параллельное поле (Parallel field);
Алгебраическая топология (Algebraic topology);
Векторное расслоение (Vector bundle);
Когомологии де Рама (De Rham cohomologies);
Структура (дифференциальная геометрия) (Structure -- differential geometry)	\\
\hline									
6	&	Теория вероятностей и математическая статистика (Probability and statistics)	&	$\Omega$: элементарный событие (event),
$P$: вероятность (probability),
$F$: алгебра событие (event algebra),
$X$: случайный величина (random variable),
$\omega$: множество элемент (element of set),
$g$: интегрировать функция (integrable function),
$n$: стремление (convergence),
$N$: счётный мера (countable measure),
$\sigma$: событие (event),
$p$: момент (moment)	&	Теория вероятностей (Probability);
Теория вероятностей и математическая статистика (Probability and statistics);
Теория меры (Measure theory);
Математические теоремы (Mathematical theorems);
Теоремы теории вероятностей и математической статистики (Theorems of probability and statistics)	&	 Случайная величина (Random variable);
Аксиоматика Колмогорова (Kolmogorov axioms);
Пространство элементарных событий (Sample space);
Теорема Лебега о мажорируемой сходимости (Lebesgue's dominated convergence theorem);
$t$-Критерий Стьюдента (Student's $t$-test)	\\
\hline									
5	&	Физика элементарных частиц. Теория полей. Физика высоких энергий (Particle physics. Field theory. High energy physics)	&	$G$: гравитационный постоянный (gravitation constant),
$M$: масса (mass),
$c$: скорость свет (speed of light),
$m_2$: масса (mass),
$m$: масса (mass),
$\Psi$: волновой функция (wave function),
$m_1$: материальный точка масса (mass of material point),
$t$: время (time),
$r$: расстояние (distance),
$R$: масса (mass)	&	Гравитация (Gravitation);
Астрономия (Astronomy);
Общая теория относительности (General relativity);
Теория относительности (Relativity theory);
Физическая космология (Physical cosmology)	&	Чёрная дыра (Black hole);
Гравитационное красное смещение (Gravitational redshift);
Квантовый компьютер (Quantum computer);
Метрика Шварцшильда (Schwarzschild metric);
Гравитация (Gravitation)	\\
\hline									
5	&	Оптика (Optics)	&	$\Phi_0$: поток излучение (Radiant flux),
$\lambda$: длина (length),
$\Phi$: поток излучение (Radiant flux),
$l$: расстояние (distance),
$\varepsilon$: приёмник (``receiver''),
$n_1$: показатель преломление (refractive index),
$K_m$: световой эффективность излучение (luminous efficacy),
$n$: показатель преломление (refractive index),
$n_2$: преломление среда (``refractive environment''),
$r$: рассеяние (dispersion)	&	Оптика (Optics);
Физические науки (Physical sciences);
Волновая физика (Wave mechanics);
Фотометрия (Photometry);
Методы экспериментальной физики (Methods of experimental physics)	&	Энергия излучения (оптика) (Radiant energy);
Облучённость (фотометрия) (Irradiance);
Фотонный кристалл (Photonic crystal);
Свет (Light);
Сила излучения (фотометрия) (Radiant intensity) \\
\hline									
\end{tabular}}
\caption{Most frequent definitions in most frequent namespaces extracted from Russian Wikipedia.}
\label{tab:top-namespaces-rus}
\end{table}
\restoregeometry



\subsection{Java Language Processing} \label{sec:jlp}

Previously we have illustrated the idea of identifier namespaces by comparing
it with namespaces in Computer Science, and it allowed us to develop an intuition
behind the namespaces in mathematics and also propose a method to discover them:
we motivated the assumption that there exist ``namespace defining''
groups of documents by arguing that these groups also exist in
programming languages.
In this section we will try to do the reserve: apply the methods
developed for identifier namespace discovery to the source code.

If a programming language is statically typed, like Java or Pascal,
usually it is possible to know the type of a variable from the declaration
of this variable. Therefore we can see variable names as ``identifiers''
and variable types as ``definitions''. Clearly, there is a difference
between variable types and identifier definitions, but we believe
that this comparison is valid because the type carries additional semantic
information about the variable and in what context it can be used --
like the definition of an identifier.

The information about variables and their types can be extracted from a
source code repository, and each source file can be processed to
obtain its Abstract Syntax Tree (AST). By processing the ASTs,
we can extract the variable declaration information. Thus, each
source file can be seen as a document, which is represented
by all its variable declarations.

In this work we process Java source code, and for parsing it
and building ASTs we use a library JavaParser \cite{javaparser}.
The Java programming language was chosen because it requires the programmer
to always specify the type information when declaring a variable.
It is different for other languages when the type information is
usually inferred by the compilers at compilation time.


In Java a variable can be declared in three places:
as an inner class variable (or a ``field''), as a method (constructor)
parameter or as a local variable inside a method or a constructor.
We need to process all three types of variable declarations
and then apply additional preprocessing, such as converting the name
of the type from short to fully qualified using the information from the
import statements. For example, \verb|String| is converted to
\verb|java.lang.String| and \verb|List<Integer>| to \verb|java.util.List<Integer>|,
but primitive types like \verb|byte| or \verb|int| are left unchanged.


Consider an example in the listing~\ref{code:javaclass}. There is a
class variable \texttt{threshold}, a method parameter \texttt{in} and
two local variables \texttt{word} and \texttt{posTag}. The following
relations will be extracted from this class: (``threshold'', \verb|double|),
(``in'', \verb|domain.Word|), (``word'', \verb|java.lang.String|),
(``posTag'', \verb|java.lang.String|).
Since all primitives and classes from packages that start with
\verb|java| are discarded, at the end the class \verb|WordProcesser|
is represented with only one relation (``in'', \verb|domain.Word|).


\begin{lstlisting}[language=Java,caption={A Java class},label={code:javaclass}]
package process;

import domain.Word;

public class WordProcesser  {

    private double threshold;

    public boolean isGood(Word in) {
        String word = in.getWord();
        String posTag = in.getPosTag();
        return isWordGood(word) && isPosTagGood(posTag);
    }

    // ...

}
\end{lstlisting}


In the experiments we applied this source code analysis to
the source code of Apache Mahout 0.10 \cite{mahout}, which is an open-source
library for scalable Machine Learning and Data Mining.
As on \today, this dataset consists of 1\,560 java classes with 45\,878
variable declarations. After discarding declarations from the standard Java API,
primitives and types with generic parameters, only 15\,869 declarations were
retained.

The following is top-15 variable/type declarations extracted from the Mahout
source code:

\begin{itemize}
\item ``conf'', \verb|org.apache.hadoop.conf.Configuration| (491 times)
\item ``v'', \verb|org.apache.mahout.math.Vector| (224 times)
\item ``dataModel'', \verb|org.apache.mahout.cf.taste.model.DataModel| (207 times)
\item ``fs'', \verb|org.apache.hadoop.fs.FileSystem| (207 times)
\item ``log'', \verb|org.slf4j.Logger| (171 times)
\item ``output'', \verb|org.apache.hadoop.fs.Path| (152 times)
\item ``vector'', \verb|org.apache.mahout.math.Vector| (145 times)
\item ``x'', \verb|org.apache.mahout.math.Vector| (120 times)
\item ``path'', \verb|org.apache.hadoop.fs.Path| (113 times)
\item ``measure'', \verb|org.apache.mahout.common.distance.DistanceMeasure| (102 times)
\item ``input'', \verb|org.apache.hadoop.fs.Path| (101 times)
\item ``y'', \verb|org.apache.mahout.math.Vector| (87 times)
\item ``comp'', \verb|org.apache.mahout.math.function.IntComparator| (74 times)
\item ``job'', \verb|org.apache.hadoop.mapreduce.Job| (71 times)
\item ``m'', \verb|org.apache.mahout.math.Matrix| (70 times)
\end{itemize}

We used the ``soft'' association method to incorporate ``definition''
(i.e. types), and considering each source code file as a document,
we build an identifier-document matrix of dimensionality
$1436 \times 1560$. Only identifiers that occur at least twice are used
to build the matrix.

Previously the best performance was achieved by using LSA and MiniBatch $K$-Means,
and therefore we apply the same algorithms here. However there are a lot of instances
of the types \texttt{Vector} and \texttt{Matrix}, and to mediate its influence, we use two variants of TF-IDF:
with usual TF component and with sublinear TF.
The best performance is achieved with the rank $k=200$ of SVD and
number of clusters $K=200$ using sublinear weighting (see fig.~\ref{fig:jlp-perf}).
% each experiment was repeated 10 times and we record the average


\begin{figure}[h!]
\centering
\hfill
\begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{jlp-lens.pdf}
  \caption{Usual TF $\times$ IDF weighting}
  \label{fig:jlp-perf-tf}
\end{subfigure}
~
\begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{jlp-lens-sublin.pdf}
  \caption{$(\log \text{TF}) \times \text{IDF}$ weighting}
  \label{fig:jlp-perf-subtf}
\end{subfigure}
\caption{The performance MiniBatch $K$-Means on the Mahout dataset.}
\label{fig:jlp-perf}
\end{figure}


With these parameters the best result is 33 clusters.
One of such clusters is a cluster about SVD: there are 5 classes
from the \verb|recommender.svd|\footnote{full name: \texttt{org.apache.mahout.cf.taste.impl.recommender.svd}} package
(\texttt{Factorization}, \texttt{FilePersistenceStrategy}, \texttt{NoPersistenceStrategy}, \texttt{PersistenceStrategy}, \texttt{FilePersistenceStrategyTest})
and one from \\ \verb|kddcup.track1.svd|\footnote{full name: \texttt{org.apache.mahout.cf.taste.example.kddcup.track1.svd}} package
(\texttt{Track1SVDRunner}).
Although this cluster is not 100\% pure, in the sense that not all of
these classes belong to the same package, these classes are
clearly related: they are all about SVD.
The top dimensions with the most influence in this cluster are
\verb|svd.Factorization|\footnote{full name: \texttt{org.apache.mahout.cf.taste.impl.recommender.svd.Factorization}}
and \texttt{factorization}.

Another interesting namespace is the distance namespace, with classes mostly 
from \verb|common.distance|\footnote{full name: \texttt{org.apache.mahout.common.distance}}
(see table~\ref{tab:mahout-dist-classes}).
What is interesting, there are mostly tests in this cluster, but because they use 
classes that they test, they roughly can be seen as documents that refer to
some concepts defined outside of the cluster (see table~\ref{tab:mahout-dist-ids}). 


\begin{table}[h!]
\centering
\begin{subtable}{\textwidth}
\centering
\begin{tabular}{|c|}
  \hline
Class name \\
\hline
\verb|org.apache.mahout.math.neighborhood.UpdatableSearcher| \\
\verb|org.apache.mahout.common.distance.CosineDistanceMeasureTest| \\
\verb|org.apache.mahout.common.distance.DefaultDistanceMeasureTest| \\
\verb|org.apache.mahout.common.distance.DefaultWeightedDistanceMeasureTest| \\
\verb|org.apache.mahout.common.distance.TestChebyshevMeasure| \\
\verb|org.apache.mahout.common.distance.TestMinkowskiMeasure| \\
\hline
\end{tabular}
\caption{A namespace-defining cluster about Distances.}
\label{tab:mahout-dist-classes}
\end{subtable}

\begin{subtable}{\textwidth}
\centering
\begin{tabular}{|c|c|}
  \hline
  ID & Class  \\
  \hline
\verb|chebyshevDistanceMeasure| &  \verb|org.apache.mahout.common.distance.DistanceMeasure| \\
\verb|distanceMeasure| &  \verb|org.apache.mahout.common.distance.DistanceMeasure|  \\
\verb|euclideanDistanceMeasure| &  \verb|org.apache.mahout.common.distance.DistanceMeasure|  \\
\verb|manhattanDistanceMeasure| &  \verb|org.apache.mahout.common.distance.DistanceMeasure|  \\
\verb|minkowskiDistanceMeasure| &  \verb|org.apache.mahout.common.distance.DistanceMeasure|  \\
\verb|v| &  \verb|org.apache.mahout.math.Vector|  \\
\verb|vector| &  \verb|org.apache.mahout.math.Vector| \\
\hline
\end{tabular}
\caption{Definitions in the distance namespace.}
\label{tab:mahout-dist-ids}
\end{subtable}

\caption{Namespace ``\texttt{distance}'' extracted from Apache Mahout.}
\label{tab:mahout-namespaces}
\end{table}

Note that there is a difference between mathematical namespaces and
namespaces discovered in the source code. The found ``namespaces'' do not necessarily
correspond to the real packages in the source code. The reason for this is
the document-centric view on the namespaces: for documents we assume that the
namespaces are not directly observed, and instead we can see only the documents where
these namespaces are used. It is not the case for real namespaces
in software: we do observe namespaces directly, and documents (that is, classes)
are also the elements of the namespaces.
