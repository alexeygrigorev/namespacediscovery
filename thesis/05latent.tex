\section{Discovering Latent Semantics} \label{sec:latent-semantics}
In chapter~\ref{sec:namespaces-top} we have discussed the
lexical variability and ambiguity problems in natural language: synonymy
and polysemy. We can treat these problems as ``statistical noise'' and
apply dimensionality reduction techniques to find the optimal dimensionality
for the data and thus reduce the amount of noise there.
In this chapter we discuss two approaches for doing this:
Latent Semantic Analysis in section~\ref{sec:lsa}
and Non-Negative Matrix Factorization in section~\ref{sec:nmf}.



% \textbf{How to deal with these problems?} Term Extraction techniques:
% these techniques create "artificial" terms that aren't really terms - they are generated, and not the ones that actually occurred in the text
% The original terms don't have the optimal dimensionality for document content representation
% because of the problems of polysemy, homonymy and synonymy
% so we want to find better representation that doesn't suffer from these issues


\subsection{Latent Semantic Analysis} \label{sec:lsa}

The Vector Space Model discussed in section~\ref{sec:vsm}.
The solution to this problem is to assume that there exists
some optimal document vector space where the document vectors
do not suffer from the ...
This vector space can be found by finding the best $k$-rank approximation
to the Term-Document matrix using Singular Value Decomposition (SVD).
This technique is called Latent Semantic Analysis \cite{landauer1998introduction}
or Latent Semantic Indexing in the context of Information Retrieval
\cite{deerwester1990indexing}.
It is also a popular Text Mining technique for reducing the dimensionality
of text data and it is often used for
document clustering \cite{aggarwal2012survey} \cite{osinski2004lingo}.


There are three major steps in Latent Semantic Analysis  \cite{evangelopoulos2012latent}:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Preprocess documents
\item
  Construct a Term-Document matrix $D$ using the Vector Space Model
\item
  Reduce dimensionality of $D$ by using SVD
\end{itemize}


The first two steps are the same as for traditional Vector Space Models:
consider a set of document $\mathcal D = \{ d_1, d_2, \ ... \ , d_n \}$
with the vocabulary $\mathcal V = \{t_1, t_2, \ ... \ , t_m \}$, then
$D$ is a $m \times n$ Term-Document Matrix. If the matrix $D$ has
rank $r$, then the SVD of $D$ is $D = U  \Sigma V^T$, where:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item $U$ is an $m \times r$ orthogonal matrix, i.e. $U U^T = I$;
\item $\Sigma$ is a diagonal $r \times r$ matrix with singular values ordered by their magnitude;
\item $V$ is an $n \times r$ orthogonal matrix, $V V^T = I$.
\end{itemize}

The dimensionality reduction is done by finding the best $k$-rank approximation
of $D$, which is obtained by keeping only the first $k$ singular values of $\Sigma$
and setting the rest to 0.
Typically, not only $\Sigma$ is truncated, but also $U$ and $V$,
and therefore, the $k$-rank approximation of $D$ using SVD is written as
$D \approx D_k = U_k \Sigma_k V_k^T$ where $U_k$ is an $m \times k$
matrix with first $k$ columns of $U$, $\Sigma_k$ is an $k \times k$
diagonal matrix with singular values, and $V_k$ is an $n \times k$
matrix with first $k$ columns of $V$.  This decomposition
is called \emph{rank-reduced} SVD and when applied to text data
it reveals the ``true'' latent semantic space. The parameter $k$ corresponds
to the number of ``latent concepts'' in the data. The idea 
of LSA is very nicely illustrated by examples  in 
\cite{deerwester1990indexing} and \cite{landauer1998introduction}.

LSA can be used for clustering as well, and this is usually done
by first transforming the document space to the LSA space
and then doing applying transitional cluster analysis techniques
there \cite{schutze1997projections}.

However these is not need to reconstruct the rank-reduced matrix
to apply clustering, and in many cases it is not possible:
the original input space is very sparse, but the rank-reduced
reconstructed matrix becomes very dense. Therefore we do not
reconstruct the entire matrix, but instead keep only the low
dimensional representation $V_k \Sigma_k$, which is enough
for many clustering algorithms.

For example, consider the inner product. Document-document similarity
in the original space is calculated as $D D^T$ (the columns of $D$
are the document $\mathbf d_1, \ ... \ , \mathbf d_n$), and by applying
SVD we have $D^T D = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T =
(V \Sigma) (V \Sigma)^T$. Thus, to compute the similarity between
document $\mathbf d_i$ and $\mathbf d_j$ we compute the inner product
between $i$th and $j$th rows of $V \Sigma$. Likewise, to compute
the similarity between documents $i$ and $j$ in the reduced representation,
we compute the inner product between the respective rows of $V_k \Sigma_k$.

Additionally, the Euclidean distance in the reduced space can also be computed
directly on the rows of $V_k \Sigma_k$. Recall that the Euclidean distance can
be expressed as an inner product
$\| \mathbf d_i - \mathbf d_j \|^2 = \mathbf d_i^T \mathbf d_i - 2 \, \mathbf d_i^T \mathbf d_j + \mathbf d_j^T \mathbf d_j$, and since we know how to compute the inner product
in the semantic space, we can compute the distance in this space as well
by using the rows of $V_k \Sigma_k$.

This means that we can apply any clustering algorithm,
including $K$-means, on the rows of $V_k \Sigma_k$ and without having
to reconstruct the entire term-document matrix.

A generic LSA-based clustering algorithm therefore consists of the following steps:

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
  \item Build a term-document matrix $D$ from the document collection
  \item Select number of latent concepts $k$ and apply rank-reduced SVD on $D$ to get $V_k \Sigma_k$
  \item Apply the cluster algorithm on the rows of $V_k \Sigma_k$
\end{itemize}


LSA has some drawbacks. Because SVD looks for orthogonal basis
for the new document space, there are negative values that are harder
to interpret. Additionally, with negative values in the reconstructed
space can cause the cosine to take negative values as well.
However, it does not affect significantly the properties of the
cosine distance: it still will always give the results larger than 0.
This problem can be solved by using a different matrix decomposition
technique instead of SVD, and we discuss one of them in the next
section.


\ \\

Apart from SVD there are many other different matrix decomposition
techniques that can be applied for document clustering and for discovering
the latent structure of the term-document matrix \cite{osinski2006improving},
and one of them in Non-Negative Matrix Factorization (NMF) \cite{lee1999nnmf}.


NMF is a matrix decomposition technique. When it is applied to non-negative
data, NMF produces non-negative rank-reduced approximations.
Since term-document matrices do not have negative values, it makes
NMF a good candidate to replace SVD in LSA. The main conceptual difference
between SVD and NMF is that SVD looks for orthogonal directions to
represent document space, while NMF does not require orthogonality.
As the result, SVD often produces semantic spaces with negative values,
but NMF does not \cite{xu2003document} (see fig.~\ref{fig:nmf-svd}).


\begin{figure}[h]
\centering\includegraphics[width=0.75\textwidth]{nmf-svd.png}
\caption{\textbf{TODO redraw} Directions found by  SVD (on the left) vs directions by NMF (on the right)}
\label{fig:nmf-svd}
\end{figure}


The NMF of an $m \times n$ term-document matrix $D$ is $D \approx D_k = U  V^T$
where $U$ is an $m \times k$ matrix, $V$ is an $n \times k$ matrix and
$k$ is the number of semantic concepts in $D$.
Non-negativity of elements in $D_k$ is very good for interpretability: it
ensures that documents can be seen as a non-negative combination of
the key concepts.


Additionally, NMF is useful for clustering: the results of NMF can
be directly interpreted as cluster assignment and there is no need
to use separate clustering algorithms \cite{xu2003document}.

What is more, NMF is a co-clustering algorithms: it clusters both
rows of $D$ and columns of $D$ at the same time. For a term-document
matrix $D \approx U V^T$, where $U$ defines the reduced vector space for terms
and $V$ defines the reduced vector space for documents. Since all elements
are non-negative, it can have the following interpretation:
elements $(U)_{ij}$ of $U$ represent the degree to which terms $i$ belongs to cluster $j$,
and elements $(V)_{ij}$ represent the degree to which document $i$ belongs to cluster $j$.

\ \\

The document clustering using NMF consists of the following
steps \cite{xu2003document}:

\begin{itemize}
  \item Construct the term-document matrix $D$;
  \item Perform NMF on $D$ to get $U$ and $V$;
  \item Normalize rows $\mathbf v_i$ of $V$ by using $\mathbf v_i' = \mathbf v_i \cdot \| \mathbf u_i \|$ and rows $\mathbf u_i$ of $U$ with $\mathbf u_i' = \mathbf u_i / \| \mathbf u_i \|$;
  \item To determine the cluster assignment for document $\mathbf d_i$, examine $\mathbf v_i'$ (the $i$th row of $V$) and find the largest component of this vector. That is,
      $i$th document belongs to cluster $x$ if $x = \operatorname{arg \, max}_j v_{ij}$ where $v_{ij}$ are components of $\mathbf v_i$;
  \item If the desired number of clusters $K$ is larger than the rank $k$ of the reduced matrix $D_k$, the clustering can be performed directly on the rows of $V$, for example,
      by using $K$-Means.
\end{itemize}

% The computational complexity on NMF is $O(kn)$ 