\section{Implementation}


In section~\ref{sec:dataset} we describe the data set that we use, 
then we describe how we extract identifiers from this dataset 
(section~\ref{sec:defextraction-impl}) and how this dataset is cleaned
(section~\ref{sec:datacleaning}). Next, the implementation of clustering
algorithms is described in the section~\ref{sec:datacleaning}. After
the clusters are found, we combine them into a hierarchy in 
the section~\ref{sec:hierarchy}. 

Finally, in the section~\ref{sec:jlp} we explore how the same set of techniques
can be applied to source code in Java.


\subsection{Data set} \label{sec:dataset}

In this work we apply the discussed techniques to the English version of
Wikipedia \cite{enwikidump}.
It's a big web encyclopedia where articles are written and edited by 
the community. For our work wikipedia is interesting because 
there are many math pages. 


At present (\today) English wikipedia contains about 4.9 mln
articles\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Statistics}}
and it is 1.5 Gb in the compressed form. However, only a small portion
of these articles are math related: only about 30.000 pages contain at least 
one \verb|<math>| tag. 

The math information is enriched with semantic information by MediaWiki 
and we use this augmented data representation. 30.000 math pages with 
augmented math tags occupy around 1.5 Gb in uncompressed form. 

Apart from the text data and formulas wikipedia articles have information
about categories, which can also be exploited. It is hard to 
extract category information from the raw wikipedia mark up, but this 
information is available in a structured form in DBPedia \cite{bizer2009dbpedia}.


\subsection{Definition Extraction} \label{sec:defextraction-impl}

Before we can proceed to discovering identifier namespaces, we  
need to extract identifier-definition relations. For this we use the probabilistic 
approach, discussed in the section~\ref{sec:mlp}. 
The extraction process in implemented using Apache Flink \cite{flink}.

But before the original dataset can be preprocessed, it is enriched with 
augmented MathML (see section~\ref{sec:mathml}), 
and then the dataset is filtered such that only articles with
the math tag are retained. 

In the wikipedia dataset each document is represented using wiki XML. It 
makes it easy to extract title and content, and then, the all the formulas 
are extracted from the content. The formulas are extracted by 
looking for \verb|<math>| tags. However some formulas are typed without 
the tag, but only with unicode characters. Such formulas are not easy to 
detect and therefore in this word we choose not to process them.
Once all \verb|<math>| tags are found, they (along with the content) 
are replaced with a special placeholder \verb|FORMULA_%HASH%|, where 
\verb|%HASH%| is MD5 hash \cite{rivest1992md5} of the tag's content represented as 
a hexadecimal string. After that 
the content of the tags is kept separately from the document content. 

Once formulas are retrieved, we extract the definitions from them. 
We are not interested in the semantics of a formula, only in the identifiers 
it contains. Hence we need only to look for all \verb|<ci>| tags. 
There are two types of identifiers: simple identifiers such as 
$t$, $C$, $\mu$; and complex identifiers with subscripts such as
$x_1$, $\xi_i$ or even $\beta_{\text{slope}}$. We do not process 
superscripts because they are usually powers (for example, $x^2$), 
and therefore they are not interesting for this work. 
There are exceptions to this, for example, $\sigma^2$ is an identifier,
but these cases are rare and can be ignored. 

Since MathML is XML, the identifiers are extracted with XPath queries:

\begin{itemize}
  \item \verb|//m:mi[not(ancestor::m:msub)]/text()| for all \verb|<ci>| tags
that are not subscript identifers
  \item \verb|//m:msub| for subscript identifiers
\end{itemize}

Once the identifiers are extracted, the rest of the formula is discarded. 
As the result, we have a ``Bag of Formulas''.

The content of a wiki document is structured and authored with a special 
markup language for specifying document layout elements such as
headers, lists, text formatting and tables: Wiki markup. 
Thus the next step is to process the Wiki markup and extract the textual 
content of an article, and this is done using a Java library 
``Mylyn Wikitext'' \cite{mylynwikitext}.
Almost all annotations are discarded at this stage,
and only inner-wiki links are kept: they can be useful as candidate definitions.

Once the markup annotations are removed and the text content of an article is 
extracted, we then apply Natural Language Processing (NLP) techniques.
Thus, the next step is the NLP step, and for NLP we use StanfordNLP
\cite{manning2014stanford}. 
The first part at this stage is to tokenize the text and also split it by sentences. 
Once it is done, we then apply Math-aware POS tagging
(see section~\ref{sec:postagging}).
For identifiers and math formulas we introduce two new POS classes: 
``\verb|ID|'' and ``\verb|MATH|'', respectively. 
These classes are not a part of the 
standard Penn Treebank POS Scheme \cite{santorini1990part} used by 
StanfordNLP, therefore we need to label all the instances of these tags ourselves 
during the additional post-processing step. If a token starts
with ``\verb|FORMULA_|'', then we recognize that it is a placeholder 
for a math formula, and therefore we annotate it with the ``\verb|MATH|''
tag. Additionally, if this formula contains only one identifier, this 
placeholder token is replaced by the identifier and it is tagged with 
``\verb|ID|''. Additionally, we keep track of all identifiers found in 
the document and then for each token we check if this token is in the list. 
If it is, then it is re-annotated with ``\verb|ID|'' as well.

At the Wikipedia markup processing step we discard almost all markup 
annotations, but keep only inter-wiki links, because these links
are good definition candidates. To use them, we introduce 
another POS Tag: ``\verb|LINK|''. To detect all inner-wiki links, 
we first find all token subsequences that start with \verb|[[| 
and end with \verb|]]|. 
Then these subsequences are concatenated and tagged as ``\verb|LINK|''.

Also we are interested in all sequences of successive nouns (both singular 
and plural) possibly modified by an adjective. We concatenate all
such sequences into one token tagged with ``\verb|NOUN_PHRASE|''.


Next we select the most probably identifier-definition pairs. 
At this stage we are interested only in tokens annotated with  ``\verb|LINK|''
and ``\verb|NOUN_PHRASE|'': these tokens are definition candidates, and 
we rank each token by a score that depends how far it is from the identifer
of interest and how far is the closest formula that contains this 
identifier (see section~\ref{sec:mlp}). 
The output of this step is a list of identifier-definition pairs along 
with the score. Only pairs with scores above the user specified  
threshold are retained. 

The following is the list of the most common identifier-definition pairs:

\begin{itemize}
\item $t$: ``time'' (1086)
\item $m$: ``mass'' (424)
\item $\theta$: ``angle'' (421)
\item $T$: ``temperature'' (400)
\item $r$: ``radius'' (395)
\item $v$: ``velocity'' (292)
\item $\rho$: ``density'' (290)
\item $G$: ``group'' (287)
\item $V$: ``volume'' (284)
\item $\lambda$: ``wavelength'' (263)
\item $R$: ``radius'' (257)
\item $n$: ``degree'' (233)
\item $r$: ``distance'' (220)
\item $c$: ``speed of light'' (219)
\item $L$: ``length'' (216)
\item $n$: ``length'' (189)
\item $n$: ``order'' (188)
\item $n$: ``dimension'' (185)
\item $n$: ``size'' (178)
\item $M$: ``mass'' (171)
\end{itemize}



\subsection{Data Cleaning} \label{sec:datacleaning}

The Natural Language data is famous for being noisy and hard to 
clean \cite{sonntag2004assessing}. The same is true for 
mathematical identifiers and  scientific texts with formulas. 
In this section we describe how the data was preprocessed and 
cleaned at different stages of Definition Extraction 
(section~\ref{sec:defextraction-impl}).


Often identifiers contain additional semantic information visually conveyed 
by special THINGIES like hats ($\hat w$), bars ($\bar X$), arrows ($\overrightarrow x$) 
and others or special fort features. For example boldness is often used to 
denote vectors $\mathbf w$ or matrices $\mathbf X$, calligraphic fonts are 
used for sets $\mathcal H$, double-struck fonts often denote spaces ($\mathbb R$),
etc. Unfortunately there is no common notation established across all fields of 
mathematics and there is a lot of variance. For example, 
a vector can be denoted by $\overrightarrow x$, $\boldsymbol x$ or $\mathbf x$, 
and a real filed by $\mathbb R$, $\mathbf R$ or $\mathfrak R$. 
Therefore we discard all this additional information, such that
$\bar X$ becomes $X$, $\mathbf w$ becomes $w$ and $\mathfrak R$ becomes $R$.


The additional THINGIES can easily be discarded, but the visual features are 
encoded directly on the character level: the identifiers use special unicode 
characters to convey font features such as boldness or Fraktur,
so it has to be normalized by converting characters
from special ``Mathematical Alphanumeric Symbols'' unicode block
back to standard ASCII positions (``Basic Latin'' block).

Additionally, the is a lot of noise on the annotation level in MathML formulas: 
many non-identifiers are captures as identifiers inside \verb|<ci>| tags. Among 
them there are many mathematic-related symbols
like ``\textasciicircum'', ``\#'',``$\forall$'', ``$\int$''; 
miscellaneous symbols like ``$\diamond$'' or
``$\circ$'', arrows like ``$\to$'' and ``$\Rightarrow$'', and special characters like
``$\lceil$''.

To filter out these one-symbol false identifiers we fully exclude all characters from 
the following unicode blocks: ``Spacing Modifier Letters'', ``Miscellaneous Symbols'', 
``Geometric Shapes'', ``Arrows'', ``Miscellaneous Technical'', ``Box Drawing'', 
``Mathematical Operators'' (except ``$\nabla$'' which is sometimes used as an identifier)
and ``Supplemental Mathematical Operators''.
Some symbols (like ``='', ``+'', ``\verb|~|'', ``\%'', ``?'', ``!'')
belong to commonly used unicode blocks which we cannot exclude altogether. 
For these symbols we manually prepare a stop list for filtering them.

It also captures multiple-symbol false positives: operators and functions
like ``\texttt{sin}'', ``\texttt{cos}'', ``\texttt{exp}'', ``\texttt{max}'', ``\texttt{trace}''; 
words commonly used in formulas like ``\texttt{const}'', ``\texttt{true}'', ``\texttt{false}'',
``\texttt{vs}'', ``\texttt{iff}''; English stop words like ``\texttt{where}'', ``\texttt{else}'',
``\texttt{on}'', ``\texttt{of}'', ``\texttt{as}'', ``\texttt{is}''; 
units like ``\texttt{mol}'', ``\texttt{dB}'', ``\texttt{mm}''.
These false identifiers are excluded by a stop list as well: if a
candidate identifier is in the list, it is filtered out.


Then, at the next stage, the definitions are extracted. However many 
shortlisted definitions are either not valid definitions or too general. 
For example, some identifiers become associated with ``if and only if'', 
``alpha'', ``beta'', ``gamma'', which are not valid definitions. Other definitions
like ``element'', ``number'' or ``variable'' are valid, but 
they are too general and not descriptive. We maintain a stop list of such 
false definitions and filter them out from the result. 



\subsection{Document Clustering} \label{sec:clustering-impl}

First step is discarding all definitions (from each id-pair definitions)
that occur only once and therefore do not have much discriminating power. 

Then we need to build an Identifier Space (see section~\ref{sec:ism}). 
There are three ways of doing it.

\begin{itemize}
  \item keeping identifiers alone
  \item ``weak identifier-definition association'': putting identifiers along with definitions
  \item identifier-definition pairs 
\end{itemize}

In the first case we use only identifier information - and discard the 
definitions altogether.

In the second and third cases we keep the definitions. 
But there is some variability in the definitions. For example, 
the same identifier can be assigned to ``Cauchy stress tensor'' and
``stress tensor'' - which are almost the same thing. To reduce 
this variability we tokenize the definition and use individual tokens
as dimensions of the space. For example, if we have two pairs
($\sigma$, ``Cauchy stress tensor'') and ($\sigma$, ``stress tensor''),
for second case we have dimensions $(\sigma, \text{Cauchy}, \text{stress}, \text{tensor})$,
while for the third case we have 
$(\sigma\text{\_Cauchy}, \sigma\text{\_stress}, \sigma\text{\_tensor})$.


Additionally, to decrease the effect of variability further, 
a stemming technique is applied to each definition token. 
We use Snowball stemmer for English \cite{porter2001snowball} 
implemented in NLTK \cite{bird2006nltk}: 
a python library for Natural Language Processing. 

Each document is vectorized (converted to a vector form) by using
\verb|TfidfVectorizer| from  scikit-learn \cite{scikit-learn}. 
We use the following settings: 

- \verb|use_idf=True, min_df=2|
- \verb|use_idf=False, min_df=2|
- \verb|use_idf=False, sublinear_tf=True, min_df=2|

The output is a document-identifier matrix (analogous to ``document-term''):
documents are rows and identifiers/definitions are columns. 

The output of \verb|TfidfVectorizer| is row-normalized, i.e. 
all rows has unit length. 


K-Means and Mini-Batch K-Means implementation from scikit-learn \cite{scikit-learn}.
Classes \verb|KMeans|, \verb|MiniBatchKMeans|

If rows are unit-normalized, then running k-means with Euclidean distance
is equivalent to cosine distance (see section~\ref{sec:kmeans}).

Bisecting K-Means (see section~\ref{sec:kmeans-ext}) was implemented on top of scikit-learn: each time a data set is clustered
with k-means with $k = 2$. if the subset being clustered is big, 
($N > 2000$) then mini-batch k-means is used, otherwise usual K-means

Scatter/Gather (see section~\ref{sec:kmeans-ext}) was implemented manually 
using scipy \cite{scipy} and numpy \cite{walt2011numpy} because 
scikit-learn's implementation of K-Means does not allow using user-defined distances. 


DBScan (section~\ref{sec:dbscan}) and SNN Clustering (section~\ref{sec:dbscan-ext})
algorithms were also implemented manually:
available DBScan implementations usually take distance measure rather than 
a similarity measure, but we want to use a similarity matrix, and not 
a distance matrix because converting would make the matrix dense, 
and we want to avoid that. Additionally, available implementation
do not use the structure of data: in text-like data clustering algorithms 
can be sped up by using an inverted index (section~\ref{sec:index})


LSA (section~\ref{sec:lsa}) is implemented via randomized SVD \cite{tropp2009finding},
The implementation is taken from scikit-learn \cite{scikit-learn} - 
method \verb|randomized_svd|.

Non-negative Matrix Factorization (section~\ref{sec:nmf}) is also taken from 
cikit-learn \cite{scikit-learn}, class \verb|NMF|.


To assess the quality of clustering we used wikipedia categories. 
It is difficult to extract category information from raw wikipedia text, therefore
we used DBPedia \cite{bizer2009dbpedia} for that. Additionally 
DBPedia provides a SKOS ontology about categories. 


\subsection{Building Hierarchy} \label{sec:hierarchy}

AMS Mathematics Subject Classification (2010) \cite{ams2010msc}
Excluded all subcategories those code end with '99': they are 
usually 'Miscellaneous topics' or 'None of the above, but in this section'. 
top level categories 'General', 'History and biography', and 'Mathematics education'
were also excluded. 
Additionally we exclude the following: 

\begin{itemize}
\item Quantum theory $\to$ Axiomatics, foundations, philosophy
\item Quantum theory $\to$ Applications to specific physical systems
\item Quantum theory $\to$ Groups and algebras in quantum theory
\item Partial differential equations $\to$ Equations of mathematical physics and other areas of application
\item Statistics $\to$ Sufficiency and information
\item Functional analysis $\to$ Other (nonclassical) types of functional analysis
\item Functional analysis $\to$ Miscellaneous applications of functional analysis
\end{itemize}

So these categories do not interfere with PACS. 


APS Physics and Astronomy Classification Scheme (2010) \cite{aps2010pacs}

We remove the ``GENERAL'' top-level category. 
In PACS there are 3 levels of categories, but we merge all 3-rd level categories into 
2nd level.

ACM Classification Scheme \cite{rous2012acm} available as a SKOS \cite{miles2005skos} ontology 
at their website \cite{amc2012ccs}. The SKOS ontology graph was processed with RDFLib \cite{rdflib}

We keep the following top level categories:
``Hardware'', ``Computer systems organization'', ``Networks'', 
``Software and its engineering'', ``Theory of computation'', 
``Information systems'', ``Security and privacy'',
``Human-centered computing'', ``Computing methodologies''.

After obtaining the data and parsing, all categories, the hierarchies 
are merged into one and then we try to match the found namespaces 
with second-level category in the hierarchy. 

This is done by keywords matching: we extract all words from the 
category (this includes top level category name, subcategory name
and all sub-sub categories concatenated). From the cluster 
we also extract the category information.
Then we try to do keyword matching using cosine similarity 
between the cluster and each category. The cluster is assigned 
to the category with the best cosine. 

If the cosine score is low (below $0.2$) or there is only one 
keyword matched, then the cluster is assigned to the ``OTHERS'' 
category.


\subsection{Java Language Processing} \label{sec:jlp}

The same set of techniques can be applied to source code.
(todo: why!)
If a language 
is statically typed, like Java or Pascal, usually it is possible to know the
type of a variable from its declaration. Therefore we can see 
variables as identifiers and types as "definitions" (TODO: clearly state 
the difference between types and definitions).

To extract this information from some source code repository each file source
file can be processed to obtain its Abstract Syntax Tree, and then 
declaration information can be extracted from it. 


In this word we process Java source code using JavaParser \cite{javaparser} - 
a library for parsing java source code. Java was chosen because the variable 
types always have to be declared, unlike other languages where the type 
can be inferred by compilers. 

The following declarations are processed: fields of a class, method and 
constructor parameters, inner variable declarations inside 
methods and constructors. It processes both usual classes and inner classes. 

\textbf{Add example and the results}


In the experiments we process Apache Mahout source code \cite{mahout}. 

Describe the dataset
