\section{Implementation} \label{sec:implementation}


In section~\ref{sec:dataset} we describe the data set that we use,
then we describe how we extract identifiers from this dataset
(section~\ref{sec:defextraction-impl}) and how this dataset is cleaned
(section~\ref{sec:datacleaning}). Next, the implementation of clustering
algorithms is described in the section~\ref{sec:datacleaning}. After
the clusters are found, we combine them into a hierarchy in
the section~\ref{sec:hierarchy}.

Finally, in the section~\ref{sec:jlp} we explore how the same set of techniques
can be applied to source code in Java.


\subsection{Data set} \label{sec:dataset}

Wikipedia is a big online encyclopedia where the content
are written and edited by the community. It contains a large
amount of articles on a variety of topics, including articles about
Mathematics and Mathematics-related fields such as Physics. It
is multilingual and available in several languages, including
English, German, French, Russian and others.
The content of wikipedia pages are authored in a special
markup language and the content of the entire encyclopedia
is freely available for download.


% why wiki?

The techniques discussed in this work are mainly applied
to the English version of Wikipedia. At the moment of writing
(\today) English Wikipedia contains about 4.9 million
articles\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Statistics}}.
However, just a small portion of these articles are math related:
there are only 30.000 pages that contain at least one \verb|<math>| tag.

Apart from the text data and formulas Wikipedia articles have information
about categories, and we can exploit this information as well.
The category information is encoded directly into each Wikipedia page
with a special markup tag. For example, the article
``Linear Regression''\footnote{\url{https://en.wikipedia.org/wiki/Linear_regression}}
belongs to the category ``Regression analysis'' and \verb|[[Category:Regression analysis]]|
tag encodes this information.
However there are other indirect ways to associate a page with some
category, for example, by using Wiki templates. A template is a user-defined macro
that is executed by the Wikipedia engine, and the content of a template
can include category association tags. It is hard
to extract the content information from the template tag and therefore
we use category information available in a structured
machine-processable form in DBPedia \cite{bizer2009dbpedia}.
Additionally, DBPedia provides extra information  such as parent
categories (categories of categories) that is very easy to process
and incorporate into analysis.


Wikipedia is available in other languages, not only English.
While the most of the analysis is performed on the English Wikipedia,
we also apply some of the techniques to the Russian version \cite{ruwikidump}
to compare it with the results obtained on the English Wikipedia.
Russian Wikipedia is smaller that the English Wikipedia and contains
1.9 million
articles\footnote{\url{https://en.wikipedia.org/wiki/Russian_Wikipedia}},
among which only 15\,000 pages are math-related (i.e. contain at
least one \verb|<math>| tag).


\subsection{Definition Extraction} \label{sec:defextraction-impl}

Before we can proceed to discovering identifier namespaces, we
need to extract identifier-definition relations. For this we use the probabilistic
approach, discussed in the section~\ref{sec:mlp}.
The extraction process is implemented using Apache Flink \cite{flink}
and it is based on the open source implementation provided by Pagael and
Schubotz in \cite{pagael2014mlp}\footnote{\url{https://github.com/rbzn/project-mlp}}.


The first step is to keep only mathematical articles and discard the rest.
This is done by retaining only those articles that contain
at least one \verb|<math>| tag. Once the data set is filtered, then
all the \LaTeX\ formulas form the \verb|<math>| tags are converted
to MathML, an XML-based representation of mathematical
formulae \cite{mathml}.

The dataset is stored in a big XML file in the Wiki XML
format. It makes it easy to extract the title and the content
of each document, and then process the documents separately.
The formulas are extracted by looking for the \verb|<math>| tags.
However some formulas for some reasons are typed without the tags
using the unicode symbols, and such formulas are very hard to
detect and therefore we choose not to process them.
Once all \verb|<math>| tags are found, they (along with the content)
are replaced with a special placeholder \verb|FORMULA_%HASH%|, where
\verb|%HASH%| is MD5 hash \cite{rivest1992md5} of the tag's content represented as
a hexadecimal string. After that
the content of the tags is kept separately from the document content.

The next step is to find the definitions for identifiers in formulas.
We are not interested in the semantics of a formula, only in the identifiers
it contains. In MathML \verb|<ci>| corresponds to identifiers, and
hence extracting identifiers from MathML formulas amounts to finding
all \verb|<ci>| tags and retrieving their content. It is enough to
extract simple identifiers such as ``$t$'', ``$C$'', ``$\mu$'', but
there also are complex identifiers with subscripts, such as
``$x_1$'', ``$\xi_i$'' or even ``$\beta_{\text{slope}}$''.
To extract them we need to look for tags \verb|<msub>|. We do not
process superscripts because they are usually powers (for example, ``$x^2$''),
and therefore they are not interesting for this work.
There are exceptions to this, for example, ``$\sigma^2$'' is an identifier,
but these cases are rare and can be ignored.

Since MathML is XML, the identifiers are extracted with XPath
queries \cite{moller2006introduction}:

\begin{itemize}
  \item \verb|//m:mi[not(ancestor::m:msub)]/text()| for all \verb|<ci>| tags
that are not subscript identifers;
  \item \verb|//m:msub| for subscript identifiers.
\end{itemize}

Once the identifiers are extracted, the rest of the formula is discarded.
As the result, we have a ``Bag of Formulae'': analogously to the Bag of Words
approach (see section~\ref{sec:vsm}) we keep only the counts of occurrences
of different identifiers and we do not preserve any other structure.


The content of Wikipedia document is authored with Wiki markup --
a special markup language for specifying document layout elements
such as headers, lists, text formatting and tables.
Thus the next step is to process the Wiki markup and extract the textual
content of an article, and this is done using a Java library
``Mylyn Wikitext'' \cite{mylynwikitext}.
Almost all annotations are discarded at this stage,
and only inner-wiki links are kept: they can be useful as candidate definitions.
The implementation of this step is taken entirely from \cite{pagael2014mlp}
with only a few minor changes.


Once the markup annotations are removed and the text content of an article is
extracted, we then apply Natural Language Processing (NLP) techniques.
Thus, the next step is the NLP step, and for NLP we use the
Stanford Core NLP library (StanfordNLP) \cite{manning2014stanford}.
The first part of this stage is to tokenize the text and also split it by sentences.
Once it is done, we then apply Math-aware POS tagging
(see section~\ref{sec:postagging}). For English documents from the English Wikipedia
we use StanfordNLP's Maximal Entropy POS Tagger \cite{toutanova2003feature}.
Unfortunately, there are no trained models available for POS tagging the Russian
language for the StanfordNLP library and we were not able to find a
suitable implementation of any other POS taggers in Java. Therefore we
implemented a simple rule-based POS tagger ourselves. The implementation is based on
a PHP function from \cite{habr2012postag}: it is translated into Java
and seamlessly integrated into the StanfordNLP pipeline.
The English tagger uses the Penn Treebank POS Scheme \cite{santorini1990part},
and hence we follow the same convention for the Russian tagger.


For handling mathematics we introduce two new POS classes:
``\verb|ID|'' for identifiers and ``\verb|MATH|'' for formulas.
These classes are not a part of the Penn Treebank POS Scheme,
and therefore we need to label all the instances of these tags ourselves
during the additional post-processing step. If a token starts
with ``\verb|FORMULA_|'', then we recognize that it is a placeholder
for a math formula, and therefore we annotate it with the ``\verb|MATH|''
tag. Additionally, if this formula contains only one identifier, this
placeholder token is replaced by the identifier and it is tagged with
``\verb|ID|''. We also keep track of all identifiers found in
the document and then for each token we check if this token is in the list.
If it is, then it is re-annotated with the ``\verb|ID|'' tag.

At the Wikipedia markup processing step we discard almost all markup
annotations, but we do keep inner Wikipedia links, because these links
are good definition candidates. To use them, we introduce
another POS Tag: ``\verb|LINK|''. To detect all inner-wiki links,
we first find all token subsequences that start with \verb|[[|
and end with \verb|]]|, and then these subsequences are
concatenated and tagged as ``\verb|LINK|''.

Successive nouns (both singular and plurals), possible modified
by an adjective, are also candidates for definitions. Therefore
we find all such sequences on the text and then concatenate each
into one single token tagged with ``\verb|NOUN_PHRASE|''.

The next stage is selecting the most probable identifier-definition
pairs, and this is done by ranking definition candidates.
The definition candidates are tokens annotated with ``\verb|NN|'' (noun singular),
``\verb|NNS|'' (noun plural), ``\verb|LINK|'' and ``\verb|NOUN_PHRASE|''.
We rank these tokens by a score that depends how far it is from the identifer
of interest and how far is the closest formula that contains this
identifier (see section~\ref{sec:mlp}).
The output of this step is a list of identifier-definition pairs along
with the score, and only the pairs with scores above
the user specified threshold are retained. The implementation of
this step is also taken entirely from \cite{pagael2014mlp} with very minor
modifications.

\subsubsection{Data Cleaning} \label{sec:datacleaning} \ \\

The Natural Language data is famous for being noisy and hard to
clean \cite{sonntag2004assessing}. The same is true for
mathematical identifiers and  scientific texts with formulas.
In this section we describe how the data was preprocessed and
cleaned at different stages of Definition Extraction.

Often identifiers contain additional semantic information visually conveyed
by special diacritical marks or font features. For example, the diacritics can be
hats to denote ``estimates'' (e.g. ``$\hat w$''), bars to denote the expected value
(e.g. ``$\bar X$''), arrows to denote vectors (e.g. ``$\vec x\, $'') and others.
As for the font features, boldness is often used to
denote vectors (e.g. ``$\mathbf w$'') or matrices (e.g. ``$\mathbf X$''), calligraphic fonts are
used for sets (e.g. ``$\mathcal H$''), double-struck fonts often denote spaces
(e.g. ``$\mathbb R$''), and so on.

Unfortunately there is no common notation established across all fields of
mathematics and there is a lot of variance. For example,
a vector can be denoted by ``$\vec x\, $'', ``$\boldsymbol x$'' or ``$\mathbf x$'',
and a real line by ``$\mathbb R$'', ``$\mathbf R$'' or ``$\mathfrak R$''.
In natural languages there are related problems of lexical ambiguity such as 
synonymy, when different words refer to the same concept, and it can be solved
by replacing the ambiguous words with some token, representative of the concept. 
Therefore this problem with identifiers can be solved similarly 
by reducing identifiers to their ``root'' form. This can be done 
by discarding all additional visual information, such that 
``$\bar X$'' becomes ``$X$'', ``$\mathbf w$'' becomes ``$w$'' and ``$\mathfrak R$''
becomes ``$R$''. The disadvantage of this approach is that we lose 
the additional semantic information about the identifier that overwise 
could be useful. 


The diacritic marks can easily be discarded because they are represented
by special MathML instructions that can be ignored when 
the identifiers are retrieved. But, on the other hand,
the visual features are encoded directly on the character level:
the identifiers use special unicode symbols to convey font features such
as boldness or Fraktur, so it needs to be normalized by converting characters
from special ``Mathematical Alphanumeric Symbols'' unicode block \cite{allen2007unicode}
back to the standard ASCII positions (``Basic Latin'' block).
Some identifiers (such as ``$\hbar$'' or ``$\ell$'') are expressed using the  
characters from a special ``Letterlike Symbols'' table, and these characters 
are normalized as well.

Additionally, the is a lot of noise on the annotation level in MathML formulas:
many non-identifiers are captures as identifiers inside \verb|<ci>| tags. Among
them there are many mathematic-related symbols
like ``\textasciicircum'', ``\#'',``$\forall$'', ``$\int$'';
miscellaneous symbols like ``$\diamond$'' or
``$\circ$'', arrows like ``$\to$'' and ``$\Rightarrow$'', and special characters like
``$\lceil$''.

To filter out these one-symbol false identifiers we fully exclude all characters from
the following unicode blocks: ``Spacing Modifier Letters'', ``Miscellaneous Symbols'',
``Geometric Shapes'', ``Arrows'', ``Miscellaneous Technical'', ``Box Drawing'',
``Mathematical Operators'' (except ``$\nabla$'' which is sometimes used as an identifier)
and ``Supplemental Mathematical Operators'' \cite{allen2007unicode}.
Some symbols (like ``='', ``+'', ``\verb|~|'', ``\%'', ``?'', ``!'')
belong to commonly used unicode blocks which we cannot exclude altogether.
For these symbols we manually prepare a stop list for filtering them.

It also captures multiple-symbol false positives: operators and functions
like ``\texttt{sin}'', ``\texttt{cos}'', ``\texttt{exp}'', ``\texttt{max}'', ``\texttt{trace}'';
words commonly used in formulas like ``\texttt{const}'', ``\texttt{true}'', ``\texttt{false}'',
``\texttt{vs}'', ``\texttt{iff}''; auxiliary words like ``\texttt{where}'', 
``\texttt{else}'', ``\texttt{on}'', ``\texttt{of}'', ``\texttt{as}'', ``\texttt{is}'';
units like ``\texttt{mol}'', ``\texttt{dB}'', ``\texttt{mm}''.
These false identifiers are excluded by a stop list as well: if a
candidate identifier is in the list, it is filtered out. 
The stop list of false positives is quite similar for 
both English and Russian: for the Russian wikipedia we only need 
to handle the auxiliary words such as ``\texttt{где}'' (``\texttt{where}''), 
``\texttt{иначе}'' (``\texttt{else}'') and so on. The names for operators and functions 
are more or less consistent across both data sources. 


Then, at the next stage, the definitions are extracted. However many
shortlisted definitions are either not valid definitions or too general.
For example, some identifiers become associated with ``\texttt{if and only if}'',
``\texttt{alpha}'', ``\texttt{beta}'', ``\texttt{gamma}'', which are not valid definitions.
Other definitions like ``\texttt{element}'' (``\texttt{элемент}''), 
``\texttt{number}'' (``\texttt{число}'') or ``\texttt{variable}'' (``\texttt{переменная}'' ) 
are valid, but they are too general and not descriptive. We maintain a stop list of such
false definitions and filter them out from the result. The elements 
of the stop list are also consistent across both data data sets, 
in the sense that the false definition candidates are same but expressed 
in different languages.

The Russian language is highly inflected, and because of this extracted 
definitions have many different forms, depending on grammatical gender, 
form (singular or plural) and declensions. This highly increases the 
variability of the definitions, and to reduce it lemmatize the definitions: 
they are reduced to the same common form (nominative, singular and masculinum). 
This is done using Pymorphy2: a python library for Russian and 
Ukrainian morphology \cite{korobov2015morphological}.

At the next stage the retrieved identifier/definition pairs 
are used for document clustering. Some definitions are used only 
once and we can note that they are not very useful because 
they do not have any discriminative power. Therefore all such 
definitions are excluded.


\subsubsection{Dataset Statistics}  \ \\


During the data cleaning at the identifier extraction step 
some false identifiers are discarded, and after that some documents
become empty: they contain no identifiers
at all, and these documents are no longer considered for the analysis.
Additionally, we discard all the documents that have only one identifier.
This leaves only 22\,515 documents out of 30\,000, and they
contain 12\,771 distinct identifiers, which occur about 2 million times.

The most frequent identifiers are $x$ (125\,500 times), $p$ (110\,000),
$m$ (105\,000 times) and $n$ (83\,000 times), but about 3\,700 identifiers occur
only once and 1\,950 just twice. Clearly, the distribution of
identifiers follows some power law distribution (see fig.~\ref{fig:ed-wiki-ids}).

\begin{figure}[h]
\centering
\hfill
\begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{en-wiki-ids-1.pdf}
  \caption{Frequencies of the first 50 identifiers}
  \label{fig:en-wiki-ids-1}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{en-wiki-ids-2-log.pdf}
  \caption{Frequencies, log scale}
  \label{fig:en-wiki-ids-2-log.pdf}
\end{subfigure}
\hfill
\caption{Distribution of frequencies of identifiers}
\label{fig:ed-wiki-ids}
\end{figure}



The distribution of counts for identifiers inside the documents also
appears to follow a long tail power law distribution: there are few articles
that contain many identifiers, while most of the articles do not
(see fig.~\ref{fig:en-wiki-doc-ids-1.pdf}).
The biggest article (``Euclidean algorithm'') has 22\,766 identifiers, a
and the second largest (``Lambda lifting'') has only 6\,500 identifiers.
The mean number of identifiers per document is 33.
The distribution for number of distinct identifiers per document
is less skewed (see fig.~\ref{fig:en-wiki-doc-ids-2.pdf}).
The largest number of distinct identifiers is 287 (in the article
``Hooke's law''), and it is followed by 194 (in ``Dimensionless quantity'').
The median number of identifiers per document is 10.


\begin{figure}[h]
\centering

\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{en-wiki-doc-ids-1.pdf}
  \caption{Identifier frequencies per document for first 80 most largest documents}
  \label{fig:en-wiki-doc-ids-1.pdf}
\end{subfigure}

\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{en-wiki-doc-ids-2.pdf}
  \caption{No. of distinct identifiers per document for first 80 most largest documents}
  \label{fig:en-wiki-doc-ids-2.pdf}
\end{subfigure}

\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{en-wiki-doc-def.pdf}
  \caption{Definitions per document}
  \label{fig:en-wiki-doc-def.pdf}
\end{subfigure}

\caption{Frequencies per documents}
\label{fig:freqs-doc}

\end{figure}


For 12\,771 identifiers the algorithm extracted 115\,300 definitions, and the
number of found definitions follows a long tail distribution as well
(see fig.~\ref{fig:en-wiki-doc-def.pdf}), with the median number of
definitions per page being 4.

The following is the list of the most common identifier-definition pairs
extracted from the English Wikipedia:

\begin{itemize}
\item $t$: ``time'' (1086)
\item $m$: ``mass'' (424)
\item $\theta$: ``angle'' (421)
\item $T$: ``temperature'' (400)
\item $r$: ``radius'' (395)
\item $v$: ``velocity'' (292)
\item $\rho$: ``density'' (290)
\item $G$: ``group'' (287)
\item $V$: ``volume'' (284)
\item $\lambda$: ``wavelength'' (263)
\item $R$: ``radius'' (257)
\item $n$: ``degree'' (233)
\item $r$: ``distance'' (220)
\item $c$: ``speed of light'' (219)
\item $L$: ``length'' (216)
\item $n$: ``length'' (189)
\item $n$: ``order'' (188)
\item $n$: ``dimension'' (185)
\item $n$: ``size'' (178)
\item $M$: ``mass'' (171)
\item $d$: ``distance'' (163)
\item $X$: ``topological space'' (159)
\end{itemize}

In Russian Wikipedia only 5\,300 articles contain enough identifiers,
and the remaining 9\,500 are discarded.

The identifiers and definitions extracted from the Russain Wikipedia
exhibit the similar properties. The most frequently
occurring identifier is $x$ with 13\,248 occurrences, 
but the median frequency of an identifer is only 3 times. 
The article with the largest number of identifiers is ``Уравнения Максвелла''
(``Maxwell's equations'') which contains 1\,831 identifiers, while
the median number of identifiers is just 3;
the article with the largest number of distinct identifiers 
is also  ``Уравнения Максвелла'' with 112 unique identifiers, and 
the median number of distinct identifiers in the data set is 5. 
Finally, the largest number of extracted definitions is 44 
(again, for ``Уравнения Максвелла'') with 2 being the median number of 
definitions per page.

The following is the list most frequent relations extracted 
from the Russian wikipedia:

\begin{itemize}
\item $t$: ``функция'' (``function'') (215)
\item $t$: ``время'' (``time'') (130)
\item $X$: ``множество'' (``set'') (113)
\item $m$: ``масса'' (``mass'') (103)
\item $c$: ``скорость свет'' (``speed of light'') (89)
\item $G$: ``группа'' (``group'') (87)
\item $T$: ``температура'' (``temperature'') (69)
\item $h$: ``постоянный планка'' (``Plank constant'') (68)
\item $\rho$: ``плотность'' (``density'') (57)
\item $M$: ``многообразие'' (``manifold'') (53)
\item $K$: ``поль'' (``field'') (53)
\item $X$: ``пространство'' (``space'') (50)
\item $v$: ``скорость'' (``speed'') (50)
\item $X$: ``топологический пространство'' (``topological space'') (46)
\item $G$: ``граф'' (``graph'') (44)
\item $R$: ``радиус'' (``radius'') (38)
\item $R$: ``кольцо'' (``ring'') (36)
\item $G$: ``гравитационный постоянный'' (``gravitational constant'') (34)
\item $E$: ``энергия'' (``energy'') (34)
\item $m$: ``модуль'' (``modulo'') (33)
\item $S$: ``площадь'' (``area'') (32)
\item $k$: ``постоянный больцмана'' (``Boltzmann constant'') (30)
\end{itemize}



\subsection{Document Clustering} \label{sec:clustering-impl}

At the Document Clustering stage we want to find cluster of documents
that are good namespace candidates.

Before we can do this, we need to vectorize our dataset: i.e. build the
Identifier Space (see section~\ref{sec:vsm}) and represent each document
in this space.

There are three choices for dimensions of the Identifier space:

\begin{itemize}
  \item identifiers alone,
  \item ``weak'' identifier-definition association,
  \item ``strong'' association: use identifier-definition pairs.
\end{itemize}

In the first case we are only interested in identifier information and
discard the definitions altogether.

In the second and third cases we keep the definitions and use them to
index the dimensions of the Identifier Space. Bur there is some
variability in the definitions: for example, the same identifier
``$\sigma$'' in one document can be assigned to ``Cauchy stress tensor'' and
in other it can be assigned to ``stress tensor'', which are almost the same thing.
To reduce this variability we perform some preprocessing: we tokenize
the definitions and use individual tokens to index dimensions of the space.
For example, suppose we have two pairs ($\sigma$, ``Cauchy stress tensor'')
and ($\sigma$, ``stress tensor''). In the ``weak'' association case
we have will dimensions $(\sigma, \text{Cauchy}, \text{stress}, \text{tensor})$,
while for the ``strong'' association case we will have
$(\sigma\text{\_Cauchy}, \sigma\text{\_stress}, \sigma\text{\_tensor})$.

Additionally, the effect of variability can be decreased further
by applying a stemming technique for each definition token.
In this work we use Snowball stemmer for English \cite{porter2001snowball}
implemented in NLTK \cite{bird2006nltk}: a python library for
Natural Language Processing.

Using \verb|TfidfVectorizer|  from  scikit-learn \cite{scikit-learn} we vectorize 
each document. 

We use the following settings:

\begin{enumerate}[label=\Alph*]
  \item \verb|use_idf=True, min_df=2|
  \item \verb|use_idf=False, min_df=2|
  \item \verb|use_idf=False, sublinear_tf=True, min_df=2|
\end{enumerate}


In the first case we use inverse document frequency (IDF) to assign additional
collection weight for "terms"
(see section~\ref{sec:vsm}), while in second and in third we use only
term frequency (TF).
In the second case we apply a sublinear transformation to the TF component
to reduce  the influence of frequently occurring words.
In all three cases we keep
only "terms" that are used in at least two documents.

The output is a document-identifier matrix (analogous to ``document-term''):
documents are rows and identifiers/definitions are columns.
The output of \verb|TfidfVectorizer| is row-normalized, i.e.
all rows has unit length.


Once we the documents are vectorized, we can apply clustering techniques
to them. We use $K$-Means (class \verb|KMeans| in scikit-learn) and
Mini-Batch $K$-Means (class \verb|MiniBatchKMeans|) \cite{scikit-learn}.
Note that if rows are unit-normalized, then running $K$-Means with
Euclidean distance is equivalent to cosine distance
(see section~\ref{sec:kmeans}).

Bisecting $K$-Means (see section~\ref{sec:kmeans}) was implemented on top of
scikit-learn: at each step we take a subset of the dataset and apply
$K$-Means with $K = 2$ to this subset. If the subset is big (with number of
documents $n > 2000$), then we use Mini-Batch $K$-means with $K=2$
because it converges much faster.

Scatter/Gather, an extension to $K$-means (see section~\ref{sec:kmeans}), was
implemented manually  using scipy \cite{scipy} and numpy \cite{walt2011numpy} because
scikit-learn's implementation of $K$-Means does not allow using user-defined distances.

DBScan (section~\ref{sec:dbscan}) and SNN Clustering (also section~\ref{sec:dbscan})
algorithms were also implemented manually:
available DBScan implementations usually take distance measure rather than
a similarity measure. The similarity matrix cleated by similarity measures
are typically very sparse, because usually only a small fraction of the documents
are similar to some given document. Similarity measures
can be converted to distance measures, but in this case
the matrix will no longer be sparse, and we would like to avoid that.
Additionally, available implementations are usually general purpose
implementations and do not take advantage of the structure of the data:
in text-like data clustering algorithms can be sped up significantly
by using an inverted index.


Dimensionality reduction techniques are also important: they
not only reduce the dimensionality, but also help reveal the latent
structure of data. In this work we use Latent Semantic Analysis (LSA) (section~\ref{sec:lsa})
which is implemented using randomized Singular Value Decomposition (SVD)
\cite{tropp2009finding}, The implementation of randomized SVD is taken from scikit-learn
\cite{scikit-learn} - method \verb|randomized_svd|. Non-negative Matrix Factorization
is an alternative technique for dimensionality reduction (section~\ref{sec:lsa}).
Its implementation is also taken from scikit-learn \cite{scikit-learn},
class \verb|NMF|.

To assess the quality of produced clusters we use wikipedia categories. It is
quite difficult to extract category information from raw wikipedia text,
therefore we use DBPedia \cite{bizer2009dbpedia} for that: it provides
machine-readable information about categories for each wikipedia article.
Additionally, categories in wikipedia form a hierarchy, and this hierarchy
is available as a SKOS ontology.

A cluster is said to be ``pure'' if all documents have the same category.
Using categories information we can find the most frequent category of the
cluster, and then we can define purity as
$$\operatorname{purity}(C) = \cfrac{\max_i \operatorname{count}(c_i)}{|C|},$$
where $C$ is a cluster, and $c_i$ is some category.

Then we can calculate the overall purity of a cluster assignment and use
this to compare results of different clustering algorithms. However it is not
enough just to find the most pure cluster assignment: because as the number
of clusters increases the overall purity also grows.
Thus we can also optimize for the number of clusters with purity $p$ of
size at least $n$.

When the number of clusters increase, the purity always grows
(see fig.~\ref{fig:k-vs-purity}), but at some point the number of pure clusters
will start decreasing (see fig.~\ref{fig:k-vs-pureclusters}).


\begin{figure}[h]
\centering\includegraphics[width=0.9\textwidth]{purity.pdf}
\caption{$K$ in $K$-Means vs overall purity of clustering: the purity increases 
linearly with $K$ ($R^2 = 0.99$).}
\label{fig:k-vs-purity}
\end{figure}


\begin{figure}[h]
\centering\includegraphics[width=0.9\textwidth]{pure-clusters.pdf}
\caption{$K$ in $K$-Means vs the number of pure clusters: it grows initially, but after $K\approx 8\,000$ starts to decrease}
\label{fig:k-vs-pureclusters}
\end{figure}



\subsection{Parameter Tuning} \label{sec:param-tuning} \ \\

There are many different clustering algorithms, each with its own set
of parameter. In this section we describe how we find the settings that
find the best namespaces. 

The following things can be changed:

\begin{itemize}
  \item Ways to incorporate definition information (no definitions, soft association, hard association);
  \item Weighting schemes for the identifier-document matrix $D$: TF, sublinear TF, TF-IDF;
  \item There are different clustering algorithms: agglomerative clustering, DBSCAN, SNN clustering, $K$-Means, Bisecting $K$-Means, Scatter/Gather, each algorithm has its own set of parameters;
  \item Dimensionality of $D$ can be reduced via SVD or NMF, parameter $k$ controls the rank of output.
\end{itemize}

% The approach for finding 
% Distance and similarity measures used
% Euclidean distance, cosine similarity, jaccard similarity, SNN Similarity


To find the best parameters set we use the grid search approach: we try 
different combinations of parameters and keep track on the number of 
pure clusters and the purity. 


\subsubsection{Only identifiers}

The first way of building the identifier space is to use only identifiers 
and do not use definitions at all. 

If we do this, the identifier-document matrix is 6075x22512 (we keep only 
identifiers that occur at least twice), and it contains  302\, 541
records, so the density of this matrix is just 0.002. 


Agglomerative clustering 
too long, we exclude them from further analysis


DBSCAN on SSN Similarity

using Jaccard and using Cosine 


$K$-means:
The algorithm we run first is $K$-Means on non-reduced data. 


For $K=600$ the algorithm found only 16 pure clusters. (also add 
$K$ vs no. pure clusters) 

However increasing $K$ leads to linear increase in time (see fig.~\ref{fig:k-vs-time}),
which means that for bigger values of $K$, it takes longer, so it is not 
feasible to run: for example, $K = 10\, 000$ we estimate the runtime to be 
about 4.5 hours. 


\begin{figure}[h]
\centering\includegraphics[width=0.9\textwidth]{k-vs-time.pdf}
\caption{$K$ in $K$-Means vs time in minutes ($R^2 = 0.99$).}
\label{fig:k-vs-time}
\end{figure}

Although the run time of MiniBatch $K$-Means also increases linearly with $K$
(see fig.~\ref{fig:k-vs-time-minibatch}), it runs considerably faster.
For example, MiniBatch $K$-Means takes 15 seconds with $K=700$ while 
usual $K$-Means takes about 15 minutes (see fig.~\ref{fig:k-vs-time} 
and fig.~\ref{fig:k-vs-time-minibatch}).



\begin{figure}[h]
\centering\includegraphics[width=0.9\textwidth]{k-vs-time-minibatch.pdf}
\caption{$K$ in MiniBatch $K$-Means vs time in seconds ($R^2 = 0.97$).}
\label{fig:k-vs-time-minibatch}
\end{figure}



(54 documents in total)

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
Article & Identifiers \\
\hline
APL (programming language) & $n, O, R$ \\
Binary search tree & $O, n$ \\
Boolean satisfiability problem & $O, n$ \\
Complexity & $O, n$ \\
Earley parser  & $O, n$ \\
Heapsort   & $O, n, \Omega$ \\
Lisp (programming language)& $O, n$ \\
Priority queue & $O, n$ \\
Sieve of Eratosthenes  & $O, n$ \\
Smoothsort & $O, n$ \\
Comb sort  & $\Omega, n, p, O$ \\
Divide and conquer algorithm   & $O, n, \Omega$ \\
Stack (abstract data type) & $O, n, t$ \\
Skip list  & $n, p, O$ \\
... & ... \\
\hline 
\end{tabular}
\caption{Computer Science cluster with $O$-notation, 55 documents in total} 
\label{tab:kmeans-cs}
\end{table}





$K$-Means takes a lot of time so we use Minibatch K means 
with random initialization


% Agglomerative: Wald linkage: takes forever never finished


\textbf{Only identifiers }





\underline{DBSCAN SNN}

$k = 10$
dist = jaccard

$\varepsilon$=7 points
MinPts=5 points


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
Article & Identifiers \\
\hline
Epsilon Eridani in fiction & $M_\odot$ \\
Solar mass & $M_\odot$ \\
Orders of magnitude (mass) & $M_\odot$ \\
Carbon-burning process & $M_\odot$ \\
Baryonic dark matter & $M_\odot$ \\
PSR J1614–2230 & $M_\odot$ \\
Kennicutt–Schmidt law & $M_\odot$ \\
Portal:Star/Selected article/19 & $M_\odot$ \\
NGC 6166 & $M_\odot$ \\
Celestial Snow Angel & $M_\odot$ \\
Huge-LQG & $M_\odot$ \\
High-velocity cloud & $M_\odot$ \\
NGC 4845 & $M_\odot$ \\
Pulsating white dwarf & $M_\odot$ \\
Robust associations of massive baryonic objects & $M_\odot$ \\
Black Widow Pulsar & $M_\odot$ \\
Betelgeuse & $M_\odot$ \\
Andromeda Galaxy & $M_\odot$ \\
\hline
\end{tabular}
\caption{Galaxies cluster discovered by DBSCAN}
\label{tab:ssn-galaxies}
\end{table}


In general doesn't give clusters

TODO add some numbers and graphs

\subsubsection{Weak Association} \ \\


K-Means weak association

\underline{DBSCAN}
k=15, eps=8, min\_pts=5

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
Article & Identifiers \\
\hline
Codex Ephraemi Rescriptus  & $P, \text{papyrus}$ \\
Categories of New Testament manuscripts    & $P, \text{papyrus}$ \\
Papyrus 4  & $P, \text{papyrus}$ \\
Uncial 0308    & $P, M, \text{papyrus}, \text{47}$ \\
Codex Athous Lavrensis & $P, \text{papyrus}$ \\
Papyrus 92 & $P, \text{papyrus}$ \\
Papyrus 90 & $P, \text{papyrus}$ \\
Papyrus 111    & $P, \text{papyrus}$ \\
Uncial 0243    & $P, \text{papyrus}$ \\
Minuscule 1739 & $P, \text{papyrus}$ \\
Minuscule 88   & $P, \text{papyrus}$ \\
Authorship of the Epistle to the Hebrews   & $P, \text{papyrus}$ \\
Egerton Gospel & $P, \text{papyrus}$ \\
Rylands Library Papyrus P52    & $P, \text{papyrus}$ \\
Codex Vaticanus    & $P, \text{papyrus}$ \\
... & ... \\
\hline
\end{tabular}
\caption{Papyrus cluster by DBSCAN}
\label{tab:ssn-papyrus}
\end{table}


\underline{K-Means}

\textbf{TODO}: think of different ways to represent it - maybe truncate some definitions?

Some articles are not from this namespace: e.g. Pi Josephson junction,
or Almost integer

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
Article & Identifiers \\
\hline
Direct shear test & $\text{angle}, \varphi, \text{friction}$ \\
Truncated dodecadodecahedron & $\text{golden}, \text{ratio}, \phi$ \\
Golden triangle (mathematics) & $\text{golden}, \text{section}, \theta, \phi$ \\
Petrophysics & $percentage, \varphi, symbol, S_w$ \\
Greedy algorithm for Egyptian fractions & $\text{golden}, \text{terms}, d, \phi, \ ...$ \\
Pi Josephson junction & $\pi, \text{junction}, \varphi$ \\
Snub dodecahedron & $\text{golden}, \text{ratio}, \tau, V, \xi$ \\
Lucas number & $\text{golden}, \phi, m, L, \text{ratio}, L_k, \ ...$ \\
54 (number) & $\text{golden},\varphi,\text{ratio}$ \\
Special right triangles & $\pi, c, b, \text{ratio}, \phi, m, \text{golden}, \ ...$ \\
Universal code (data compression) & $\text{golden}, \text{code}, \text{ratio}, \phi, \ ...$ \\
Existential instantiation & $c, \text{symbol}, \varphi, \text{variable}$ \\
Perles configuration & $\text{golden}, \text{ratio}, \phi$ \\
Wythoff array & $\text{golden}, \text{ratio}, \phi, \varphi, \text{fibonacci}, \ ...$ \\
Golomb sequence & $a_n, n, \text{golden}, \text{ratio}, \phi$\\
Almost integer & $\text{constant}, \pi, \text{gelfonds},\varphi,\ ...$ \\
16:10 & $\text{golden}, \text{ratio}, \phi$ \\
Leonardo number & $\text{golden}, \text{ratio}, \phi, \ ...$ \\
Rogers–Ramanujan continued fraction & $\text{golden}, \text{functions}, \phi, \ ...$ \\
Great rhombic triacontahedron & $\text{golden}, \text{ratio}, \phi$ \\
... & ... \\
\hline
\end{tabular}
\caption{Golden Ratio by DBSCAN}
\label{tab:ssn-papyrus}
\end{table}

Found some interesting clusters but in general doesn't show good results.
Need to use semantic menthols

Batch, k = 2500 .. 10000 with step 50
SVD with n=600



HEre results



\subsubsection{Strong Association}





\subsection{Building Hierarchy} \label{sec:hierarchy}


After the namespaces are found, we need to organize them into a hierarchical 
structure. It is hard to do automatically, and we choose to use 
existing hierarchies for mathematical knowledge, and then map the 
found namespaces to these hierarchies.

The first hierarchy that we use is ``Mathematics Subject Classification'' (MSC)
hierarchy \cite{ams2010msc} by the American Mathematical Society, and it 
is used for categorizing mathematical articles. In this scheme there are 
64 top-level categories such as ``Mathematical logic'', ``Number theory'',
or ``Fourier analysis''. It also includes some physics categories such 
as ``Fluid mechanics'' or ``Quantum Theory''. The following top level 
categories are excluded: ``General'', ``History and biography'' and 
``Mathematics education''.

Each top-level category contains second-level categories and third-level
categories. In this work we exclude all subcategories those code 
ends with 99: they are usually ``Miscellaneous topics'' or 
``None of the above, but in this section''. 

Additionally, we excluded the following second level categories because 
they interfere with PACS, a hierarchy for Physics: 

\begin{itemize}
\item Quantum theory $\to$ Axiomatics, foundations, philosophy
\item Quantum theory $\to$ Applications to specific physical systems
\item Quantum theory $\to$ Groups and algebras in quantum theory
\item Partial differential equations $\to$ Equations of mathematical physics and other areas of application
\end{itemize}


% \begin{itemize}
% \item Statistics $\to$ Sufficiency and information
% \item Functional analysis $\to$ Other (nonclassical) types of functional analysis
% \item Functional analysis $\to$ Miscellaneous applications of functional analysis
%\end{itemize}

The second hierarchy is ``Physics and Astronomy Classification Scheme'' (PACS) 
\cite{aps2010pacs}, which is a scheme for categorizing articles about Physics. 
Like in MSC, we remove the top-level category  ``GENERAL''. 

Finally, we also use the ACM Classification Scheme \cite{rous2012acm} 
available as a SKOS \cite{miles2005skos} ontology at their website \cite{amc2012ccs}. 
The SKOS ontology graph was processed with RDFLib \cite{rdflib}. 
We use the following top level categories: 
``Hardware'', ``Computer systems organization'', ``Networks'',
``Software and its engineering'', ``Theory of computation'',
``Information systems'', ``Security and privacy'',
``Human-centered computing'', ``Computing methodologies''.

After obtaining and processing the data, the three hierarchies 
are merged into one.


However these categories are only good for English articles and 
a different hierarchy is needed for Russian. One of such hierarchies is 
``Госу\-дар\-ствен\-ный руб\-ри\-ка\-тор научно-тех\-ни\-чес\-кой инфор\-ма\-ции'' 
(ГРНТИ)~-- ``State categorizator of scientific and technical information'', which 
is a state-recommended scheme for categorizing scientific articles published 
in Russian  \cite{feodosimov2000grnti}. The hierarchy  is extracted from the 
official website\footnote{\url{http://grnti.ru/}} \cite{grntiweb}. It provides 
a very general categorization and therefore we keep only the following math-related 
categories: ``Астрономия'' (``Astronomy''), ``Биология'' (``Biology''), 
``Информатика'' (``Informatics''), ``Математика'' (``Mathematics''), 
``Механика'' (``Mechanics''), ``Ста\-тис\-тика'' (``Statistics''), 
``Физика'' (``Physics''), ``Химия'' (``Chemistry''), 
``Экономика. Экономические Науки'' (``Economics'') and others.


One we a hierarchy is established, each found namespace is mapped to
the most suitable second-level category. This is done by keywords matching.
First, we extract all key words from the category, which includes 
top level category name, subcategory name and all third level categories. 
Then we also extract the category information from the namespace, but 
we also use the names of the articles that form the namespace. 
Finally, the keyword matching is done by using the cosine similarity 
between the cluster and each category. The namespace is assigned to the 
category with the best (largest) cosine score. 

If the cosine score is low (below $0.2$) or there is only one
keyword matched, then the cluster is assigned to the ``OTHERS''
category.

For example, consider a namespace  derived from the cluster consisting of 
``Tautology (logic)'', ``List of logic systems'', ``Regular modal logic''
``Combinational logic'' documents. Among others, these articles belong to categories 
``Mathematical logic'' and ``Logic''. Then the following is the list of keywords 
extracted from the cluster:
``tautology'', ``logic'', ``list'', ``systems'', ``regular'', ``modal'', ``combinational'',
``logical'', ``expressions'', ``formal'', ``propositional'', ``calculus'' and so on.
Apparently, this namespace is about mathematical logic. 

Then consider a list of keywords for ``'General logic'', a subcategory of 
``Mathematical logic and foundations'' from MSC: 
``mathematical'', ``logic'', ``foundations'', ``general'', ``classical'', ``propositional'', ``type'', ``subsystems'' and others. 

These keywords are represented as vectors in some Vector Space and the cosine score 
between these vectors is calculated. For this example, the cosine is 
approximately 0.75, and this is the largest similarity, and therefore this namepsace 
is mapped to the ``General logic'' subcategory.

Unfortunately the mapping is not always correct (TODO: add examples)


\subsection{Java Language Processing} \label{sec:jlp}

\textbf{TODO:} also refer back to the introduction

 we have compared the identifier namespaces
with namespaces in programming languages and with packages in the Java
programming language in particular. We motivated the assumption that there
exist ``namespace defining'' groups of documents by arguing that these
groups also exist in programming languages. Thus, the same set of techniques
for namespace discovery should be applicable to source code as well.

If a programming language is statically typed, like Java or Pascal,
usually it is possible to know the type of a variable from the declaration
of this variable. Therefore we can see variable names as ``identifiers''
and variable types as ``definitions''. Clearly, there is a difference
between variable types and identifier definitions, but we believe
that this comparison is valid because the type carries additional semantic
information about the variable and in what context it can be used --
like the definition of an identifier.

The information about variables and their types can be extracted from a
source code repository, and each source file can be processed to
obtain its Abstract Syntax Tree (AST). By processing the ASTs,
we can extract the variable declaration information. Thus, each
source file can be seen as a document, which is represented
by all its variable declarations.

In this work we process Java source code, and for parsing it
and building ASTs we use a library JavaParser \cite{javaparser}.
The Java programming language was chosen because it requires the programmer
to always specify the type information when declaring a variable.
It is different for other languages when the type information is
usually inferred by the compilers at compilation time.


In Java a variable can be declared in three places:
as an inner class variable (or a ``field''), as a method (constructor)
parameter or as a local variable inside a method or a constructor.
We need to process all three types of variable declarations
and then apply additional preprocessing, such as converting the name
of the type from short to fully qualified using the information from the
import statements. For example, \verb|String| is converted to
\verb|java.lang.String| and \verb|List<Integer>| to \verb|java.util.List<Integer>|,
but primitive types like \verb|byte| or \verb|int| are left unchanged.
Secondly,


Consider an example in the listing~\ref{code:javaclass}. There is a
class variable \texttt{threshold}, a method parameter \texttt{in} and
two local variables \texttt{word} and \texttt{posTag}. The following
relations will be extracted from this class: (``threshold'', \verb|double|),
(``in'', \verb|domain.Word|), (``word'', \verb|java.lang.String|),
(``posTag'', \verb|java.lang.String|).
Since all primitives and classes from packages that start with
\verb|java| are discarded, at the end the class \verb|WordProcesser|
is represented with only one relation (``in'', \verb|domain.Word|).


\begin{lstlisting}[language=Java,caption={A java class},label={code:javaclass}]
package process;

import domain.Word;

public class WordProcesser  {

    private double threshold;

    public boolean isGood(Word in) {
        String word = in.getWord();
        String posTag = in.getPosTag();
        return isWordGood(word) && isPosTagGood(posTag);
    }

    // ...

}
\end{lstlisting}


In the experiments we applied this source code analysis to
the source code of Apache Mahout 0.10 \cite{mahout}, which is an open-source
library for scalable Machine Learning and Data Mining.

As on 2015-07-15, this dataset consists of 1560 java classes with 45878
variable declarations. After discarding declarations from the standard Java API,
primitives and types with generic parameters, only 15869 declarations were
retained.

The following is top-15 variable/type declarations:

\begin{itemize}
\item (``conf'', \verb|org.apache.hadoop.conf.Configuration|), 491 times
\item (``v'', \verb|org.apache.mahout.math.Vector|), 224 times
\item (``dataModel'', \verb|org.apache.mahout.cf.taste.model.DataModel|), 207 times
\item (``fs'', \verb|org.apache.hadoop.fs.FileSystem|), 207 times
\item (``log'', \verb|org.slf4j.Logger|), 171 times
\item (``output'', \verb|org.apache.hadoop.fs.Path|), 152 times
\item (``vector'', \verb|org.apache.mahout.math.Vector|), 145 times
\item (``x'', \verb|org.apache.mahout.math.Vector|), 120 times
\item (``path'', \verb|org.apache.hadoop.fs.Path|), 113 times
\item (``measure'', \verb|org.apache.mahout.common.distance.DistanceMeasure|), 102 times
\item (``input'', \verb|org.apache.hadoop.fs.Path|), 101 times
\item (``y'', \verb|org.apache.mahout.math.Vector|), 87 times
\item (``comp'', \verb|org.apache.mahout.math.function.IntComparator|), 74 times
\item (``job'', \verb|org.apache.hadoop.mapreduce.Job|), 71 times
\item (``m'', \verb|org.apache.mahout.math.Matrix|), 70 times
\end{itemize}
