\section{Conclusions} \label{sec:conclusions}

The goal of this work was to discover namespaces in mathematical notation
given a collection of documents with mathematical formulae. To achieve that,
we used document clustering techniques on documents, where each document is 
represented by identifiers, used in its formulae. 

We expected to discover namespaces, that are homogenies and corresponded
to the same area of knowledge. The clusters that we discovered are homogenous, 
but not all corresponded to the same category. This is why we 
additionally used the category information to recognize the namespaces-defining
clusters amount all clusters, and then we built namespaces from them.

We also initially expected that there would be more namespace-defining clusters, 
but in our results the majority of clusters are not ``pure'': documents inside these 
clusters do not belong to the same category. These clusters are only homogenous in 
the cluster analysis sense: the within-cluster distances is minimal. 


Nevertheless, the obtained results indicate that there are namespaces in mathematical 
notation and it is possible to discover them automatically from a collection of 
documents. We also verified that clustering document is better than random guessing 
by the factor of ten. We observed that dimensionality reduction techniques 
are very helpful, and clustering algorithms work better on the reduce space. 
MiniBatch $K$-Means algorithms shows the best results for discovering 
namespace-defining clusters.

Finally, to verify our intuition we also applied the developed method to extract 
namespaces from Java source code, but the conceptual differences between documents 
and classes make it harder to use Java code for validating the namespace discovery 
on documents with formulae. 

There are many ways in which the present approach can be improved further.
In the next section we discuss possible directions.


\section{Outlook and Future Work} \label{sec:future-work}

\subsection{Implementation and Other Algorithms}  % \ \\

We use the Probabilistic approach to extracting definitions for identifiers, and
it is good because it requires almost no parameter tuning.
While this approach works well most of the time, sometimes we observe
some false positives, most likely due to the fact that the dataset is
quite noisy. It potentially can be improved by using some Machine Learning
method, which, however, may require creating a hand-labeled dataset
with identifier-definition relations. To facilitate the creation of such a
dataset it is possible to pre-generate some data using using the current approach
for further labeling.

In the experiments section we have observed that cluster algorithms that produce
many clusters tend to have good performance. However, they also tend to create related
clusters from the same category and with same or similar identifiers and
definitions. Therefore such results can be refined further and merged.
This can be done, for example, by using the join operation from
the Scatter/Gather algorithm \cite{cutting1992scatter}, which finds the most
similar clusters and merges them.

We were not able to apply hierarchical agglomerative clustering algorithms because
their time complexity is prohibitive, but they may produce good clusters.
For these algorithms we are usually interested in the nearest neighbors of a given
data point, and therefore we can use approximation algorithms for computing nearest
neighbors such as Locality-Sensitive Hashing (LSH) \cite{leskovec2014mining}.
The LSH algorithms can be used for text clustering \cite{ravichandran2005randomized},
and therefore they should work well for identifiers. Additionally, LSH is also a
dimensionality reduction technique, and we have observed that generally
reducing dimensionality helps to obtain better clusters.

In this work we use hard assignment clustering algorithms, which means, that a document
can import only from one namespace. This assumption does not necessarily always hold true
and we may model the fact that documents may import from several namespaces by
using Fuzzy Clustering (or Soft Clustering) algorithms \cite{baraldi1999survey}.

In Latent Semantic Analysis other dimensionality reduction techniques
can be used, for example, Local Non-Negative Matrix Factorization \cite{li2001learning}.
There is also a randomized Non-Negative Matrix Factorization algorithm that uses
random projections \cite{wang2010efficient} \cite{damle2014random},
which potentially can give a speed up while not significantly losing
in performance. Another dimensionality reduction technique useful for
discovering semantics is Dynamic Auto-Encoders \cite{mirowski2010dynamic}.

Additionally, we can try different approaches to clustering such as
Spectral Clustering \cite{ng2002spectral} or Micro-Clustering \cite{uno2015micro}.

Finally, topic modeling techniques such as Latent Dirichlet Allocation
\cite{blei2003latent} can be quite useful for modeling namespaces. It can be
seen as a ``soft clustering'' technique and it can naturally model the fact that
a document may import from several namespaces.


\subsection{Other Concepts} %  \ \\
% Other concepts

In this work we assume that document can import only from one namespace,
but in reality is should be able to import from several namespaces. As discussed,
it can be modeled by Fuzzy Clustering. But it also can be achieved by
dividing the document in parts (for example, by paragraphs)
and then treating each part as an independent document.

For document clustering we only use identifiers, extracted definitions
and categories. It is possible to take advantage of additional information from
Wikipedia articles. For example, extract some keywords from the articles
and use them to get a better cluster assignment.

The Wikipedia data set can be seen as a graph, where two articles have
an edge if there is an interwiki link between them. Pages that describe
certain namespaces may be quite interconnected, and using this idea it is possible
to apply link-based clustering methods (such as ones described in
\cite{botafogo1991identifying} and \cite{johnson1996adaptive}) to find namespace
candidates. There are also hybrid approaches that can use both textual representation
and links \cite{oikonomakou2005review}.

Vector Space Model is not the only possible model to represent textual
information as vectors. There are other ways to embed textual information
into vector spaces like word2vec \cite{mikolov2013efficient} or
GloVe \cite{pennington2014glove}, and these methods may be useful
for representing identifers and definitions as well.

Tensors may be a better way of representing
identifier-definition pairs. For example, we can represent the data set
as a 3-dimensional tensor indexed by documents, identifiers and definition.
Tensor Factorization methods for revealing semantic information
are an active area of research in NLP and linguistics \cite{anisimov2014semantic},
so it is also possible to apply these methods to the namespace discovery problem.

Finally, while running experiments, we observed that sometimes results of
clustering algorithms with the same parameters produce quite different
results, or some algorithms produce a small amount of good quality namespaces,
while others produce many namespaces which may be less coherent.
Therefore it can be interesting to investigate how to combine the results
of different cluster assignments such that the combined result is better
in terms of the number of namespace-defining clusters. One way of
achieving this can be building ensembles of clustering algorithms \cite{strehl2003cluster}.
Alternatively, a special approach for optimizing for the number of pure clusters
can be proposed, for example, partially based on the ideas from
Boosting \cite{freund1996experiments}: apply a clustering algorithm,
remove the discovered pure clusters, and run the algorithm again on the remaining
documents until no new clusters are discovered.


\subsection{Other Datasets} % \ \\
% Other Datasets

In this work we use Wikipedia as the data source and extract namespaces from the
English part of Wikipedia. Additionally, we also apply the methods to the Russian
part, and therefore it shows that it is possible to extract namespaces from
Wikipedia in any other available language.

But we can also apply to some other larger dataset, such as arXiv\footnote{\url{http://arxiv.org/}}, a repository of over one million
of scientific papers in many different areas. The source code of these
articles are available in \LaTeX, and it can be processed automatically.

There are many scientific Q\&A websites on the Internet. The stack
exchange\footnote{\url{http://stackexchange.com/}} is one of the largest Q\&A networks,
and there are many sites on this network that contain mathematical formulae, such as
``mathematics'', ``mathoverflow'', ``cross validated'', ``data science'',
``theoretical computer science'', ``physics'', ``astronomy'', ``economics'' and many others.
This network makes their data available for download and it also can be a good
data source for namespace discovery. In addition to content, the questions contain
a lot of potentially useful metadata such as related questions and tags.


\subsection{Unsolved Questions} % \ \\

The most important question is how to extend this method to situations when
no additional information about document category is known. To solve
it, we need to replace the notion of purity with some other objective
for discovering namespace-defining clusters.

Also, a metric for evaluating the quality of a namespace is needed.
Now we assume that pure clusters are namespace-defining clusters. But the namespace
candidates should adhere to the namespace definition as much as possible,
and therefore a good criteria is needed to quantify to what extent the definition
is satisfied. This will help to define whether a cluster defines a good namespace
or not.

After namespaces are discovered we organize them into hierarchies.
To do that we use existing hierarchies, but they are not always complete
and there are mismatches. What is more, when this technique is applied to some
other language, a different hierarchy is needed for this language, and we experienced
it when processing the Russian part of Wikipedia: for that we needed to obtain
a special hierarchy. There should be a way of building these hierarchies
automatically, without the need of external dataset.
Potentially it should be possible to use hierarchical clustering
algorithms, but it may result in very deep and unnatural hierarchies, and
therefore some additional investigation in this direction may be needed.
